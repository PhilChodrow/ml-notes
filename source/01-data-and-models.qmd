---
code-fold: true
code-summary: "Show code"
---

# Data and Models

Broadly, machine learning is the science and practice of building algorithms that learn patterns from data. Here are two synthetic examples of data in which there is some kind visually clear pattern in the data. In the first example (@fig-examples-1), we can see that there is a relationship between two quantitative variables, $x$ and $y$. If someone told is the value of $x$, then we would likely be able to make a reasonable prediction about the value of the number $y$. This is an example of a **regression** task. In the second example (@fig-examples-2), we can observe a relation between the location of each data point in 2d space and a color, which could represent a category or class. If someone told us the location of the point in 2d space, we would likely be able to say which of the two classes it belonged to. 

```{python}
#| label: fig-examples
#| fig-cap: "Examples of data with visualizable patterns."
#| fig-subcap: 
#|   - "Example data in which we can seek a pattern in the value of $y$ based on the value of $x$. This task is called  **regression**."
#|   - "Example data in which we can seek a pattern in the **category** of a data point (represented by color) based on the value of two variables, $x_1$ and $x_2$. This task is called **classification.**"
#| layout-ncol: 2

import torch as tt
from matplotlib import pyplot as plt

plt.style.use("seaborn-v0_8-whitegrid")

def plot_fig(pattern = False):

    # panel 1: regression

    noise = 0.3

    x = tt.arange(0, tt.pi, 0.01)
    y = 2*tt.sin(x) + 0.5
    y_noise = y + tt.normal(0.0, noise, size = (len(x),))

    plt.scatter(x, y_noise, s = 20,  facecolors='none', edgecolors = "darkgrey", label = "data")
    if pattern: 
        plt.plot(x, y, label = "pattern", linestyle = "--", color = "black", zorder = 10)
    plt.gca().set(xlabel = r"$x$", ylabel = r"$y$")
    plt.gca().set_xlim(-0, tt.pi)
    plt.gca().set_ylim(-0, tt.pi)
    plt.gca().set_aspect('equal')
    plt.legend()
    plt.show()

    # panel 2: classification

    noise = 0.4
    n_points = 300
    x = tt.Tensor([[0, 0], [1, 1]]).repeat(int(n_points/2),1) 
    x = x + tt.normal(0.0, noise, size = x.size())
    y = tt.Tensor([0, 1]).repeat(int(n_points/2))


    plt.scatter(x[:,0], x[:,1], s = 20, alpha = 0.5, c = y, facecolors = "none", cmap = "Paired", label = "data")

    if pattern: 
        plt.plot([-0.5, 1.5], [1.5, -0.5], label = "pattern", linestyle = "--", color = "black", zorder = 10)
    plt.legend()
    plt.gca().set(xlabel = r"$x_1$", ylabel = r"$x_2$")
    plt.gca().set_xlim(-1.0, 2.0)
    plt.gca().set_ylim(-1.0, 2.0)
    plt.gca().set_aspect('equal')
    plt.show()

plot_fig()
```

In examples like these, it's often possible for us to just use our eyes and spatial reasoning in order to make a reasonable prediction. 

How do we encode this knowledge mathematically, in a way that a computer can understand and use? In each of the two cases, we can express the pattern in the data **as a mathematical function**. 

- In the regression example (@fig-examples-1), we can encode the pattern in the data as a function $f:\mathbb{R} \rightarrow \mathbb{R}$ that accepts a value of $x$ and returns a modeled or predicted value of $y$. Such a function is shown in @fig-examples-with-patterns-1. The function $f$ is our *predictive model*. We often write $y \approx f(x)$ to express the qualitative idea that $f$ models the relationship between $x$ and $y$ in a given data set. We'll learn how to make this idea much more precise when we study empirical risk minimization later in the notes. 
- In the classification example (@fig-examples-2), we can again encode the pattern in the data as a function. This time, our model is going to describe an estimated **boundary* between the two classes of points. In this example, a linear boundary is pretty good. The plot shows the plot of the linear equation  $x_2 = g(x_1) = 1 - x_1$. Later in the course, it will be convenient to instead write this equation in the equivalent form $\langle \mathbf{x}, \mathbf{w} \rangle = 0$, where $\mathbf{x} = (x_1, x_2, 1)^T$ and $\mathbf{w} = (1, 1, -1)^T$.  
 
```{python}
#| label: fig-examples-with-patterns
#| fig-cap: "Examples of patterns in data."
#| fig-subcap: 
#|   - "As above, with a plot of the function $f(x) = 2\\sin{x} + \\frac{1}{2}$."
#|   - "As above, with a plot of the function $g(x_1) = 1 - x_1$. This plot is the same as the affine subspace defined by the equation $x_1 + x_2 = 1$."
#| layout-ncol: 2
plot_fig(pattern = True)
```

