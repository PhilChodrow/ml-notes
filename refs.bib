@book{bishopPatternRecognitionMachine2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {Springer},
  address = {New York},
  url = {https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf},
  isbn = {978-0-387-31073-2},
  lccn = {Q327 .B52 2006},
  keywords = {Machine learning,Pattern perception}
}

@article{bak-colemanStewardshipGlobalCollective2021,
  title = {Stewardship of Global Collective Behavior},
  author = {{Bak-Coleman}, Joseph B. and Alfano, Mark and Barfuss, Wolfram and Bergstrom, Carl T. and Centeno, Miguel A. and Couzin, Iain D. and Donges, Jonathan F. and Galesic, Mirta and Gersick, Andrew S. and Jacquet, Jennifer and Kao, Albert B. and Moran, Rachel E. and Romanczuk, Pawel and Rubenstein, Daniel I. and Tombak, Kaia J. and Van Bavel, Jay J. and Weber, Elke U.},
  year = {2021},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {27},
  pages = {e2025764118},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2025764118},
  url = {https://pnas.org/doi/full/10.1073/pnas.2025764118},
  urldate = {2022-05-16},
  abstract = {Collective behavior provides a framework for understanding how the actions and properties of groups emerge from the way individuals generate and share information. In humans, information flows were initially shaped by natural selection yet are increasingly structured by emerging communication technologies. Our larger, more complex social networks now transfer high-fidelity information over vast distances at low cost. The digital age and the rise of social media have accelerated changes to our social systems, with poorly understood functional consequences. This gap in our knowledge represents a principal challenge to scientific progress, democracy, and actions to address global crises. We argue that the study of collective behavior must rise to a ``crisis discipline'' just as medicine, conservation, and climate science have, with a focus on providing actionable insight to policymakers and regulators for the stewardship of social systems.},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/9B9AC8BX/Bak-Coleman et al. - 2021 - Stewardship of global collective behavior.pdf}
}

@book{hardtPatternsPredictionsActions2022,
  title = {Patterns, {{Predictions}}, and {{Actions}}},
  author = {Hardt, Moritz and Recht, Benjamin},
  year = {2022},
  publisher = {Princeton University Press},
  url = {https://mlstory.org/pdf/patterns.pdf},
  isbn = {978-0-691-23372-7},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/44JG3327/Hardt and Recht - Patterns, Predictions, and Actions.pdf}
}

@article{mehrabiSurveyBiasFairness2021,
  title = {A {{Survey}} on {{Bias}} and {{Fairness}} in {{Machine Learning}}},
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  year = {2021},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {6},
  pages = {1--35},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3457607},
  url = {https://dl.acm.org/doi/10.1145/3457607},
  urldate = {2022-11-11},
  abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/APRDZTJ3/Mehrabi et al. - 2021 - A Survey on Bias and Fairness in Machine Learning.pdf}
}

@article{stiglerEpicStoryMaximum2007,
  title = {The {{Epic Story}} of {{Maximum Likelihood}}},
  author = {Stigler, Stephen M.},
  year = {2007},
  month = nov,
  journal = {Statistical Science},
  volume = {22},
  number = {4},
  eprint = {0804.2996},
  primaryclass = {stat},
  issn = {0883-4237},
  doi = {10.1214/07-STS249},
  url = {http://arxiv.org/abs/0804.2996},
  urldate = {2023-01-31},
  abstract = {At a superficial level, the idea of maximum likelihood must be prehistoric: early hunters and gatherers may not have used the words ``method of maximum likelihood'' to describe their choice of where and how to hunt and gather, but it is hard to believe they would have been surprised if their method had been described in those terms. It seems a simple, even unassailable idea: Who would rise to argue in favor of a method of minimum likelihood, or even mediocre likelihood? And yet the mathematical history of the topic shows this ``simple idea'' is really anything but simple. Joseph Louis Lagrange, Daniel Bernoulli, Leonard Euler, Pierre Simon Laplace and Carl Friedrich Gauss are only some of those who explored the topic, not always in ways we would sanction today. In this article, that history is reviewed from back well before Fisher to the time of Lucien Le Cam's dissertation. In the process Fisher's unpublished 1930 characterization of conditions for the consistency and efficiency of maximum likelihood estimates is presented, and the mathematical basis of his three proofs discussed. In particular, Fisher's derivation of the information inequality is seen to be derived from his work on the analysis of variance, and his later approach via estimating functions was derived from Euler's Relation for homogeneous functions. The reaction to Fisher's work is reviewed, and some lessons drawn.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/philchodrow/Zotero/storage/97IZKFI9/Stigler - 2007 - The Epic Story of Maximum Likelihood.pdf;/Users/philchodrow/Zotero/storage/9FDAVVQ3/0804.html}
}

@book{abu-mostafaLearningDataShort2012,
  title = {Learning from Data: A Short Course},
  shorttitle = {Learning from Data},
  author = {{Abu-Mostafa}, Yaser S. and {Magdon-Ismail}, Malik and Lin, Hsuan-Tien},
  year = {2012},
  address = {S.l.},
  url = {https://amlbook.com/},
  isbn = {978-1-60049-006-4},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/LF6LK5RU/Abu-Mostafa Yaser S., Malik Magdon-Ismail, Hsuan-Tien Lin (2012) -- Learning From Data_ A short course.pdf}
}

@book{murphyProbabilisticMachineLearning2022a,
  title = {Probabilistic Machine Learning: An Introduction},
  shorttitle = {Probabilistic Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2022},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  url = {https://probml.github.io/pml-book/book1.html},
  abstract = {"This book provides a detailed and up-to-date coverage of machine learning. It is unique in that it unifies approaches based on deep learning with approaches based on probabilistic modeling and inference. It provides mathematical background (e.g. linear algebra, optimization), basic topics (e.g., linear and logistic regression, deep neural networks), as well as more advanced topics (e.g., Gaussian processes). It provides a perfect introduction for people who want to understand cutting edge work in top machine learning conferences such as NeurIPS, ICML and ICLR"--},
  isbn = {978-0-262-04682-4},
  lccn = {Q325.5 .M872 2022},
  keywords = {Machine learning,Probabilities},
  file = {/Users/philchodrow/Zotero/storage/SFJ9XXZT/book1.pdf}
}

@book{barocasFairnessMachineLearning2023,
  title = {Fairness and Machine Learning: Limitations and Opportunities},
  shorttitle = {Fairness and Machine Learning},
  author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  year = {2023},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  url = {https://fairmlbook.org/pdf/fairmlbook.pdf},
  abstract = {"This book offers a critical view on the current practice of machine learning, as well as proposed technical fixes for achieving fairness in automated decisionmaking"--},
  isbn = {978-0-262-04861-3},
  lccn = {Q325.5 .B36 2023},
  keywords = {Automation,Decision making,Discrimination,Human factors,Law and legislation,Machine learning,Moral and ethical aspects,United States},
  file = {/Users/philchodrow/Zotero/storage/A2S2E6RA/fairmlbook.pdf}
}

@book{deisenrothMathematicsMachineLearning2020,
  title = {Mathematics for Machine Learning},
  author = {Deisenroth, Marc Peter and Faisal, A. Aldo and Ong, Cheng Soon},
  year = {2020},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK New York, NY},
  url = {https://mml-book.github.io/book/mml-book.pdf},
  abstract = {"The fundamental mathematical tools needed to understand machine learning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability, and statistics. These topics are traditionally taught in disparate courses, making it hard for data science or computer science students, or professionals, to efficiently learn the mathematics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models, and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts. For those learning the mathematics for the first time, the methods help build intuition and practical experience with applying mathematical concepts"},
  isbn = {978-1-108-47004-9 978-1-108-45514-5},
  langid = {english},
  lccn = {006.31},
  file = {/Users/philchodrow/Zotero/storage/BY2YT3SE/mml-book.pdf}
}

@book{kroeseDataScienceMachine2020,
  title = {Data Science and Machine Learning: Mathematical and Statistical Methods},
  shorttitle = {Data Science and Machine Learning},
  author = {Kroese, Dirk P. and Botev, Zdravko I. and Taimre, Thomas and Vaisman, Radislav},
  year = {2020},
  series = {Chapman \& {{Hall}}/{{CRC}} Machine Learning \& Pattern Recognition Series},
  publisher = {CRC Press, Taylor \& Francis Group},
  address = {Boca Raton London New York},
  isbn = {978-1-138-49253-0},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/PRWRZWR7/DSML.pdf}
}

@book{zhangDiveDeepLearning2023,
  title = {Dive into Deep Learning},
  author = {Zhang, Aston and Lipton, Zachary and Li, Mu},
  year = {2023},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK},
  isbn = {978-1-00-938943-3},
  langid = {english},
  annotation = {OCLC: 1403378564},
  file = {/Users/philchodrow/Zotero/storage/S5S23QCB/d2l-en.pdf}
}

@article{gormanEcologicalSexualDimorphism2014,
  title = {Ecological {{Sexual Dimorphism}} and {{Environmental Variability}} within a {{Community}} of {{Antarctic Penguins}} ({{Genus Pygoscelis}})},
  author = {Gorman, Kristen B. and Williams, Tony D. and Fraser, William R.},
  editor = {Chiaradia, Andr{\'e}},
  year = {2014},
  month = mar,
  journal = {PLoS ONE},
  volume = {9},
  number = {3},
  pages = {e90081},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0090081},
  url = {https://dx.plos.org/10.1371/journal.pone.0090081},
  urldate = {2024-01-17},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/J7LBCFE6/Gorman et al. - 2014 - Ecological Sexual Dimorphism and Environmental Var.pdf}
}

@misc{horstAllisonhorstPalmerpenguinsV02020,
  title = {Allisonhorst/Palmerpenguins: V0.1.0},
  shorttitle = {Allisonhorst/Palmerpenguins},
  author = {Horst, Allison M and Hill, Alison Presmanes and Gorman, Kristen B},
  year = {2020},
  month = jul,
  doi = {10.5281/ZENODO.3960218},
  url = {https://zenodo.org/record/3960218},
  urldate = {2024-01-17},
  abstract = {CRAN release of palmerpenguins v0.1.0 R package by Horst, Hill and Gorman (July 2020).},
  copyright = {Open Access},
  howpublished = {Zenodo}
}

@book{hastieElementsStatisticalLearning2017,
  title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  shorttitle = {The Elements of Statistical Learning},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
  year = {2017},
  series = {Springer Series in Statistics},
  edition = {Second edition, corrected at 12th printing 2017},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/b94608},
  url = {https://hastie.su.domains/Papers/ESLII.pdf},
  isbn = {978-0-387-84857-0},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/FW5KENHU/Hastie et al. - 2017 - The elements of statistical learning data mining,.pdf}
}

@book{vanderplasPythonDataScience2016,
  title = {Python Data Science Handbook: Essential Tools for Working with Data},
  shorttitle = {Python Data Science Handbook},
  author = {Vanderplas, Jacob T.},
  year = {2016},
  edition = {First edition},
  publisher = {O'Reilly Media, Inc},
  address = {Sebastopol, CA},
  isbn = {978-1-4919-1205-8},
  lccn = {QA76.73.P98 V365 2016},
  keywords = {Data mining,Data Mining,Datenanalyse,Datenmanagement,Python,Python (Computer program language)},
  annotation = {OCLC: ocn915498936}
}

@article{wickhamSplitApplyCombineStrategyData2011,
  title = {The {{Split-Apply-Combine Strategy}} for {{Data Analysis}}},
  author = {Wickham, Hadley},
  year = {2011},
  journal = {Journal of Statistical Software},
  volume = {40},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v040.i01},
  url = {http://www.jstatsoft.org/v40/i01/},
  urldate = {2024-02-01},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/GWGH5IMN/Wickham - 2011 - The Split-Apply-Combine Strategy for Data Analysis.pdf}
}

@misc{narayanan2022limits,
  title = {The Limits of the Quantitative Approach to Discrimination},
  author = {Narayanan, Arvind},
  year = {2022},
  howpublished = {Speech}
}

@article{obermeyerDissectingRacialBias2019b,
  title = {Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations},
  author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
  year = {2019},
  month = oct,
  journal = {Science},
  volume = {366},
  number = {6464},
  pages = {447--453},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aax2342},
  url = {https://www.science.org/doi/10.1126/science.aax2342},
  urldate = {2024-02-05},
  abstract = {Racial bias in health algorithms                            The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer               et al.               find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care.                                         Science               , this issue p.               447               ; see also p.               421                        ,              A health algorithm that uses health costs as a proxy for health needs leads to racial bias against Black patients.           ,              Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/I4PDP3YW/Obermeyer et al. - 2019 - Dissecting racial bias in an algorithm used to man.pdf}
}

@incollection{angwin2022machine,
  title = {Machine Bias},
  booktitle = {Ethics of Data and Analytics},
  author = {Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
  year = {2022},
  pages = {254--264},
  publisher = {Auerbach Publications}
}

@inproceedings{kingma2015adam,
  title = {Adam: {{A}} Method for Stochastic Gradient Descent},
  booktitle = {{{ICLR}}: International Conference on Learning Representations},
  author = {Kingma, Diederik P and Ba, Jimmy Lei},
  year = {2015},
  pages = {1--15},
  publisher = {ICLR US.}
}

@inproceedings{corbett-daviesAlgorithmicDecisionMaking2017,
  title = {Algorithmic {{Decision Making}} and the {{Cost}} of {{Fairness}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {{Corbett-Davies}, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
  year = {2017},
  month = aug,
  pages = {797--806},
  publisher = {ACM},
  address = {Halifax NS Canada},
  doi = {10.1145/3097983.3098095},
  url = {https://dl.acm.org/doi/10.1145/3097983.3098095},
  urldate = {2024-02-14},
  isbn = {978-1-4503-4887-4},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/8FRZQU2A/Corbett-Davies et al. - 2017 - Algorithmic Decision Making and the Cost of Fairne.pdf}
}

@article{chouldechovaFairPredictionDisparate2017a,
  title = {Fair {{Prediction}} with {{Disparate Impact}}: {{A Study}} of {{Bias}} in {{Recidivism Prediction Instruments}}},
  shorttitle = {Fair {{Prediction}} with {{Disparate Impact}}},
  author = {Chouldechova, Alexandra},
  year = {2017},
  month = jun,
  journal = {Big Data},
  volume = {5},
  number = {2},
  pages = {153--163},
  issn = {2167-6461, 2167-647X},
  doi = {10.1089/big.2016.0047},
  url = {http://www.liebertpub.com/doi/10.1089/big.2016.0047},
  urldate = {2024-02-14},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/4MK5U5XG/Chouldechova - 2017 - Fair Prediction with Disparate Impact A Study of .pdf}
}

@inproceedings{kleinbergInherentTradeOffsAlgorithmic2018,
  title = {Inherent {{Trade-Offs}} in {{Algorithmic Fairness}}},
  booktitle = {Abstracts of the 2018 {{ACM International Conference}} on {{Measurement}} and {{Modeling}} of {{Computer Systems}}},
  author = {Kleinberg, Jon},
  year = {2018},
  month = jun,
  pages = {40--40},
  publisher = {ACM},
  address = {Irvine CA USA},
  doi = {10.1145/3219617.3219634},
  url = {https://dl.acm.org/doi/10.1145/3219617.3219634},
  urldate = {2024-02-14},
  isbn = {978-1-4503-5846-0},
  langid = {english}
}

@article{flores2016false,
  title = {False Positives, False Negatives, and False Analyses: {{A}} Rejoinder to Machine Bias: {{There}}'s Software Used across the Country to Predict Future Criminals. and It's Biased against Blacks},
  author = {Flores, Anthony W and Bechtel, Kristin and Lowenkamp, Christopher T},
  year = {2016},
  journal = {Federal Probation},
  volume = {80},
  pages = {38},
  publisher = {HeinOnline}
}

@inproceedings{fogliatoValidityArrestProxy2021,
  title = {On the {{Validity}} of {{Arrest}} as a {{Proxy}} for {{Offense}}: {{Race}} and the {{Likelihood}} of {{Arrest}} for {{Violent Crimes}}},
  shorttitle = {On the {{Validity}} of {{Arrest}} as a {{Proxy}} for {{Offense}}},
  booktitle = {Proceedings of the 2021 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Fogliato, Riccardo and Xiang, Alice and Lipton, Zachary and Nagin, Daniel and Chouldechova, Alexandra},
  year = {2021},
  month = jul,
  pages = {100--111},
  publisher = {ACM},
  address = {Virtual Event USA},
  doi = {10.1145/3461702.3462538},
  url = {https://dl.acm.org/doi/10.1145/3461702.3462538},
  urldate = {2024-02-25},
  isbn = {978-1-4503-8473-5},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/NAPL39YY/Fogliato et al. - 2021 - On the Validity of Arrest as a Proxy for Offense .pdf}
}

@incollection{yusefCriminalizingRaceRacializing2017,
  title = {Criminalizing {{Race}}, {{Racializing Crime}}: {{Assessing}} the {{Discipline}} of {{Criminology}} through a {{Historical Lens}}},
  shorttitle = {Criminalizing {{Race}}, {{Racializing Crime}}},
  booktitle = {The {{Handbook}} of the {{History}} and {{Philosophy}} of {{Criminology}}},
  author = {Yusef, Kideste Wilder and Yusef, Tseleq},
  editor = {Triplett, Ruth Ann},
  year = {2017},
  month = dec,
  edition = {1},
  pages = {272--288},
  publisher = {Wiley},
  doi = {10.1002/9781119011385.ch16},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/9781119011385.ch16},
  urldate = {2024-02-25},
  isbn = {978-1-119-01135-4 978-1-119-01138-5},
  langid = {english}
}

@book{bonilla-silvaRacismRacistsColorblind2018,
  title = {Racism without Racists: Color-Blind Racism and the Persistence of Racial Inequality in {{America}}},
  shorttitle = {Racism without Racists},
  author = {{Bonilla-Silva}, Eduardo},
  year = {2018},
  edition = {Fifth edition},
  publisher = {Rowman \& Littlefield},
  address = {Lanham},
  isbn = {978-1-4422-7622-2 978-1-4422-7623-9},
  lccn = {E184.A1 B597 2018},
  keywords = {Economic conditions,Minorities,Race relations,Racism,Social conditions,United States}
}

@book{corbettSolvingEquationVariables2015,
  title = {Solving the Equation: The Variables for Women's Success in Engineering and Computing},
  shorttitle = {Solving the Equation},
  author = {Corbett, Christianne and Hill, Catherine},
  year = {2015},
  publisher = {AAUW},
  address = {Washington, DC},
  isbn = {978-1-879922-45-7},
  lccn = {MLCM 2018/41686 (T)},
  keywords = {Women in engineering}
}

@inproceedings{kearnsEfficientAgnosticLearning1992,
  title = {Toward Efficient Agnostic Learning},
  booktitle = {Proceedings of the Fifth Annual Workshop on {{Computational}} Learning Theory},
  author = {Kearns, Michael J. and Schapire, Robert E. and Sellie, Linda M.},
  year = {1992},
  month = jul,
  pages = {341--352},
  publisher = {ACM},
  address = {Pittsburgh Pennsylvania USA},
  doi = {10.1145/130385.130424},
  url = {https://dl.acm.org/doi/10.1145/130385.130424},
  urldate = {2024-02-28},
  isbn = {978-0-89791-497-0},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/36KHK7MZ/Kearns et al. - 1992 - Toward efficient agnostic learning.pdf}
}

@book{dignazioDataFeminism2023,
  title = {Data Feminism},
  author = {D'Ignazio, Catherine and Klein, Lauren F.},
  year = {2023},
  edition = {First MIT Press paperback edition},
  publisher = {The Mit Press},
  address = {Cambridge, Massachusetts},
  abstract = {Today, data science is a form of power. It has been used to expose injustice, improve health outcomes, and topple governments. But it has also been used to discriminate, police, and surveil. This potential for good, on the one hand, and harm, on the other, makes it essential to ask: Data science by whom? Data science for whom? Data science with whose interests in mind? The narratives around big data and data science are overwhelmingly white, male, and techno-heroic. The authors present a new way of thinking about data science and data ethics--one that is informed by intersectional feminist thought.--back cover},
  isbn = {978-0-262-54718-5},
  langid = {english},
  annotation = {OCLC: 1354647893}
}

@inproceedings{barabasStudyingReorientingStudy2020,
  title = {Studying up: Reorienting the Study of Algorithmic Fairness around Issues of Power},
  shorttitle = {Studying Up},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Barabas, Chelsea and Doyle, Colin and Rubinovitz, Jb and Dinakar, Karthik},
  year = {2020},
  month = jan,
  pages = {167--176},
  publisher = {ACM},
  address = {Barcelona Spain},
  doi = {10.1145/3351095.3372859},
  url = {https://dl.acm.org/doi/10.1145/3351095.3372859},
  urldate = {2024-03-05},
  abstract = {Research within the social sciences and humanities has long characterized the work of data science as a sociotechnical process, comprised of a set of logics and techniques that are inseparable from specific social norms, expectations and contexts of development and use. Yet all too often the assumptions and premises underlying data analysis remain unexamined, even in contemporary debates about the fairness of algorithmic systems. This blindspot exists in part because the methodological toolkit used to evaluate the fairness of algorithmic systems remains limited to a narrow set of computational and legal modes of analysis. In this paper, we expand on Elish and Boyd's [17] call for data scientists to develop more robust frameworks for understanding their work as situated practice by examining a specific methodological debate within the field of anthropology, frequently referred to as the practice of "studying up". We reflect on the contributions that the call to "study up" has made in the field of anthropology before making the case that the field of algorithmic fairness would similarly benefit from a reorientation "upward". A case study from our own work illustrates what it looks like to reorient one's research questions "up" in a high-profile debate regarding the fairness of an algorithmic system -- namely, pretrial risk assessment in American criminal law. We discuss the limitations of contemporary fairness discourse with regard to pretrial risk assessment before highlighting the insights gained when we reframe our research questions to focus on those who inhabit positions of power and authority within the U.S. court system. Finally, we reflect on the challenges we have encountered in implementing data science projects that "study up". In the process, we surface new insights and questions about what it means to ethically engage in data science work that directly confronts issues of power and authority.},
  isbn = {978-1-4503-6936-7},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/P87MY9QK/Barabas et al. - 2020 - Studying up reorienting the study of algorithmic .3372859}
}

@article{schaefferAreEmergentAbilities,
  title = {Are {{Emergent Abilities}} of {{Large Language Models}} a {{Mirage}}?},
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous, predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a metaanalysis of emergent abilities on BIG-Bench; and (3) show how to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/ZJMXKYQT/Schaeffer et al. - Are Emergent Abilities of Large Language Models a .pdf}
}

@inproceedings{hannaCriticalRaceMethodology2020,
  title = {Towards a Critical Race Methodology in Algorithmic Fairness},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Hanna, Alex and Denton, Emily and Smart, Andrew and {Smith-Loud}, Jamila},
  year = {2020},
  month = jan,
  pages = {501--512},
  publisher = {ACM},
  address = {Barcelona Spain},
  doi = {10.1145/3351095.3372826},
  url = {https://dl.acm.org/doi/10.1145/3351095.3372826},
  urldate = {2024-03-05},
  abstract = {We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.},
  isbn = {978-1-4503-6936-7},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/K8WZATVY/Hanna et al. - 2020 - Towards a critical race methodology in algorithmic.3372826}
}

@article{gebruDatasheetsDatasets2021,
  title = {Datasheets for Datasets},
  author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daum{\'e} and Crawford, Kate},
  year = {2021},
  month = dec,
  journal = {Communications of the ACM},
  volume = {64},
  number = {12},
  pages = {86--92},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3458723},
  url = {https://dl.acm.org/doi/10.1145/3458723},
  urldate = {2024-03-07},
  abstract = {Documentation to facilitate communication between dataset creators and consumers.},
  langid = {english},
  file = {/Users/philchodrow/Zotero/storage/YSIPAPY7/Gebru et al. - 2021 - Datasheets for datasets.pdf}
}


@article{rocks2022memorizing,
  title={Memorizing without overfitting: Bias, variance, and interpolation in overparameterized models},
  author={Rocks, Jason W and Mehta, Pankaj},
  journal={Physical review research},
  volume={4},
  number={1},
  pages={013201},
  year={2022},
  publisher={APS}
}