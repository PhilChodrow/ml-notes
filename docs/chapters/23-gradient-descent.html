<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Machine Learning - 9&nbsp; Optimization with Gradient Descent</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/30-features-regularization.html" rel="next">
<link href="../chapters/22-convex-erm.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/20-perceptron.html">Machine Learning Models</a></li><li class="breadcrumb-item"><a href="../chapters/23-gradient-descent.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Optimization with Gradient Descent</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Introducing Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-data-and-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Data, Patterns, and Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-black-box-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Classification as a Black Box</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Fundamentals of Prediction</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-score-based-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Score-Based Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-decision-theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Decision Theory in Classification</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Discrimination, Disparity, Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/10-compas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction to Algorithmic Disparity: COMPAS</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/12-statistical-fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Statistical Definitions of Fairness in Decision-Making</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Machine Learning Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/20-perceptron.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to Classification: The Perceptron</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/22-convex-erm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Convex Empirical Risk Minimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/23-gradient-descent.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Optimization with Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/30-features-regularization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Feature Maps, Regularization, and Generalization</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#linear-approximations-of-single-variable-functions" id="toc-linear-approximations-of-single-variable-functions" class="nav-link active" data-scroll-target="#linear-approximations-of-single-variable-functions">Linear Approximations of Single-Variable Functions</a></li>
  <li><a href="#gradient-descent-in-1-dimension" id="toc-gradient-descent-in-1-dimension" class="nav-link" data-scroll-target="#gradient-descent-in-1-dimension">Gradient Descent in 1 dimension</a></li>
  <li><a href="#gradient-descent-in-multiple-dimensions" id="toc-gradient-descent-in-multiple-dimensions" class="nav-link" data-scroll-target="#gradient-descent-in-multiple-dimensions">Gradient Descent in Multiple Dimensions</a></li>
  <li><a href="#gradients" id="toc-gradients" class="nav-link" data-scroll-target="#gradients">Gradients</a>
  <ul class="collapse">
  <li><a href="#multivariate-gradient-descent" id="toc-multivariate-gradient-descent" class="nav-link" data-scroll-target="#multivariate-gradient-descent">Multivariate Gradient Descent</a></li>
  </ul></li>
  <li><a href="#gradient-of-the-empirical-risk" id="toc-gradient-of-the-empirical-risk" class="nav-link" data-scroll-target="#gradient-of-the-empirical-risk">Gradient of the Empirical Risk</a></li>
  <li><a href="#example-logistic-regression" id="toc-example-logistic-regression" class="nav-link" data-scroll-target="#example-logistic-regression">Example: Logistic Regression</a></li>
  <li><a href="#recap" id="toc-recap" class="nav-link" data-scroll-target="#recap">Recap</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/20-perceptron.html">Machine Learning Models</a></li><li class="breadcrumb-item"><a href="../chapters/23-gradient-descent.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Optimization with Gradient Descent</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Optimization with Gradient Descent</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a href="../chapters/22-convex-erm.html">Last time</a>, we studied the <em>empirical risk minimization</em> (ERM). ERM casts the machine learning problem as an optimization problem: we want to find a vector of weights <span class="math inline">\(\mathbf{w}\)</span> such that</p>
<p><span id="eq-erm"><span class="math display">\[
\DeclareMathOperator*{\argmin}{argmin}
\begin{aligned}
\hat{\mathbf{w}} &amp;= \argmin_{\mathbf{w}} L(\mathbf{w}) \\
&amp;= \argmin_{\mathbf{w}} \frac{1}{n}\sum_{i = 1}^n \ell(\langle \mathbf{w}, \mathbf{x}_i\rangle, y_i)\;.
\end{aligned}
\tag{9.1}\]</span></span></p>
<p>As usual, <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n\times p}\)</span> is the feature matrix</p>
<p><span class="math display">\[
\mathbf{X} = \left[\begin{matrix} &amp; - &amp; \mathbf{x}_1 &amp; - \\
&amp; - &amp; \mathbf{x}_2 &amp; - \\
&amp; \vdots &amp; \vdots &amp; \vdots \\
&amp; - &amp; \mathbf{x}_{n} &amp; - \end{matrix}\right]
\]</span></p>
<p>and <span class="math inline">\(\mathbf{y}\)</span> is the vector of targets, which we usually assume to be binary in the context of classification. The per-observation <em>loss function</em> <span class="math inline">\(\ell: \mathbb{R}\times \mathbb{R}\rightarrow \mathbb{R}\)</span> measures the quality of the score <span class="math inline">\(s_i = \langle \mathbf{w}, \mathbf{x}_i \rangle\)</span> assigned to data point <span class="math inline">\(i\)</span> by comparing it to <span class="math inline">\(y_i\)</span> and outputing a real number <span class="math inline">\(\ell(s_i, y_i)\)</span>.</p>
<p>So, our mathematical task is to find a vector of weights <span class="math inline">\(\mathbf{w}\)</span> that solves <a href="#eq-erm" class="quarto-xref">Equation&nbsp;<span>9.1</span></a>. How do we do it? The modern answer is <em>gradient descent</em>, and this set of lecture notes is about what that means and why it works.</p>
<section id="linear-approximations-of-single-variable-functions" class="level2">
<h2 class="anchored" data-anchor-id="linear-approximations-of-single-variable-functions">Linear Approximations of Single-Variable Functions</h2>
<p>Recall the limit definition of a derivative of a single-variable function. Let <span class="math inline">\(g:\mathbb{R} \rightarrow \mathbb{R}\)</span>. The derivative of <span class="math inline">\(g\)</span> at point <span class="math inline">\(w_0\)</span>, if it exists, is</p>
<p><span class="math display">\[
\begin{aligned}
\frac{dg(w_0)}{dw} = \lim_{\delta w\rightarrow 0}\frac{g(w_0 + \delta w) - g(w_0)}{\delta w}\;.
\end{aligned}
\]</span></p>
<p>If we imagine that <span class="math inline">\(\delta w\)</span> is very small but nonzero, we can interpret this equation a bit loosely as the statement that</p>
<p><span class="math display">\[
\begin{aligned}
\frac{dg(w_0)}{dw} \approx \frac{g(w_0 + \delta w) - g(w_0)}{\delta w}\;,
\end{aligned}
\]</span></p>
<p>which upon some algebraic rearrangement says that</p>
<p><span class="math display">\[
g(w_0 + \delta w) \approx g(w_0) + \frac{dg(w_0)}{dw} \delta w\;.
\]</span></p>
<p>Taylor’s theorem makes this statement precise:</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-taylor" class="theorem">
<p><span class="theorem-title"><strong>Theorem 9.1 (Taylor’s Theorem: Univariate Functions)</strong></span> Let <span class="math inline">\(g:\mathbb{R}\rightarrow \mathbb{R}\)</span> be differentiable at point <span class="math inline">\(w_0\)</span>. Then, there exists <span class="math inline">\(a &gt; 0\)</span> such that, if <span class="math inline">\(\lvert\delta w\rvert &lt; a\)</span>, then</p>
<p><span class="math display">\[
g(w_0 + \delta w) = g(w_0) + \frac{dg(w_0)}{dw} \delta w + o(\vert\delta w\rvert)\;.
\]</span></p>
</div>
</div>
</div>
</div>
<p>Here, <span class="math inline">\(o(\lvert\delta w\rvert)\)</span> means “terms that grow small in comparison to <span class="math inline">\(\delta w\)</span> when <span class="math inline">\(\lvert\delta w\rvert\)</span> itself grows small.”</p>
<p>Another common way to write Taylor’s theorem is</p>
<p><span class="math display">\[
g(w) = g(w_0) + \frac{dg(w_0)}{dw} (w - w_0) + o(|w - w_0|)\;,
\]</span></p>
<p>which comes from substituting <span class="math inline">\(\delta w = w - w_0\)</span>.</p>
<p>Taylor’s theorem says that, in a neighborhood of <span class="math inline">\(w_0\)</span>, we can approximate <span class="math inline">\(g(w)\)</span> with a linear function. Here’s an example of how that looks:</p>
<div id="cell-1" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'seaborn-v0_8-whitegrid'</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">101</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> <span class="kw">lambda</span> w: w<span class="op">**</span><span class="dv">2</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>plt.plot(w, g(w), label <span class="op">=</span> <span class="vs">r"$g(w)$"</span>, color <span class="op">=</span> <span class="st">"black"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="vs">r"$w$"</span>, ylim <span class="op">=</span> (<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> taylor(w, w0):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> g(w0) <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>w0<span class="op">*</span>(w<span class="op">-</span>w0)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>w0 <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>plt.plot(w, taylor(w, w0), label <span class="op">=</span> <span class="vs">r"1st-order Taylor approximation"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="23-gradient-descent_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This doesn’t really look like a very good approximation, but it looks better if you zoom in!</p>
<div id="cell-3" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>plt.plot(w, g(w), label <span class="op">=</span> <span class="vs">r"$g(w)$"</span>, color <span class="op">=</span> <span class="st">"black"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="vs">r"$w$"</span>, ylim <span class="op">=</span> (<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>plt.plot(w, taylor(w, w0), label <span class="op">=</span> <span class="vs">r"1st-order Taylor approximation"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlim <span class="op">=</span> (w0 <span class="op">-</span> <span class="fl">0.1</span>, w0 <span class="op">+</span> <span class="fl">0.1</span>), ylim <span class="op">=</span> (<span class="op">-</span><span class="fl">0.00</span>, <span class="fl">0.1</span>))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="23-gradient-descent_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="gradient-descent-in-1-dimension" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-in-1-dimension">Gradient Descent in 1 dimension</h2>
<p>Suppose that we have a function <span class="math inline">\(g\)</span> and we would like to solve the the optimization problem</p>
<p><span class="math display">\[
\begin{aligned}
\hat{w} = \argmin _w g(w) \;.
\end{aligned}
\]</span></p>
<p>How do we go about doing this? You might remember from calculus that one way starts with solving the equation</p>
<p><span class="math display">\[
\begin{aligned}
\frac{dg(\hat{w})}{dw} = 0\;,
\end{aligned}
\]</span></p>
<p>to find <em>critical points</em> – under certain conditions, it is guaranteed that a minimum, if it exists, will be one of these critical points. However, it is not always feasible to solve this equation exactly in practice.</p>
<p>In <em>iterative</em> approaches, we instead imagine that we have a current guess <span class="math inline">\(\hat{w}\)</span> which we would like to improve by adding some <span class="math inline">\(\delta \hat{w}\)</span> to it. To this end, consider the casual Taylor approximation <span class="alert">In the rest of these notes, we will assume that term <span class="math inline">\(o(\lvert \delta w\rvert)\)</span> is small enough to be negligible.</span></p>
<p><span class="math display">\[
\begin{aligned}
g(\hat{w} + \delta \hat{w}) \approx g(\hat{w}) + \frac{dg(\hat{w})}{dw} \delta \hat{w}\;.
\end{aligned}
\]</span></p>
<p>We’d like to update our estimate of <span class="math inline">\(\hat{w}\)</span>. Suppose we make a strategic choice: <span class="math inline">\(\delta \hat{w} = -\alpha \frac{dg(\hat{w})}{dw}\)</span> for some small <span class="math inline">\(\alpha &gt; 0\)</span>. We therefore decide that we will do the update</p>
<p><span id="eq-w-update"><span class="math display">\[
\begin{aligned}
    \hat{w} \gets \hat{w} - \alpha \frac{dg(\hat{w})}{dw}\;.
\end{aligned}
\tag{9.2}\]</span></span></p>
<p>What does this update do to the value of <span class="math inline">\(g\)</span>? Let’s check:</p>
<p><span class="math display">\[
\begin{aligned}
    g(\hat{w} + \delta \hat{w}) &amp;\approx g(\hat{w}) + \frac{dg(\hat{w})}{dw} \delta \hat{w} \\
    &amp;= g(\hat{w}) - \frac{dg(\hat{w})}{dw} \alpha \frac{dg(\hat{w})}{dw}\\
    &amp;= g(\hat{w}) - \alpha\left(\frac{dg(\hat{w})}{dw}\right)^2\;.   
\end{aligned}
\]</span></p>
<p>This is the big punchline. Let’s look at the second term. If <span class="math inline">\(\left(\frac{dg(\hat{w})}{dw}\right)^2 = 0\)</span> then that must mean that <span class="math inline">\(\frac{dg(\hat{w})}{dw}\)</span> and that we are at a critical point, which we could check for being a local minimum. On the other hand, if <span class="math inline">\(\frac{dg(\hat{w})}{dw} \neq 0\)</span>, then <span class="math inline">\(\left(\frac{dg(\hat{w})}{dw}\right)^2 &gt; 0\)</span>. This means that</p>
<p><span class="math display">\[
\begin{aligned}
g(\hat{w} + \delta \hat{w}) &amp;\approx  g(\hat{w}) - \alpha\left(\frac{dg(\hat{w})}{dw}\right)^2 \\
                            &amp;&lt;  g(\hat{w})\;,
\end{aligned}
\]</span></p>
<p>provided that <span class="math inline">\(\alpha\)</span> is small enough for the error terms in Taylor’s Theorem to be small. We have informally derived the following fact:</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Single-Variable Gradient-Descent Works
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(g:\mathbb{R}\rightarrow \mathbb{R}\)</span> be differentiable and assume that <span class="math inline">\(\frac{dg(\hat{w})}{dw} \neq 0\)</span>. Then, if <span class="math inline">\(\alpha\)</span> is sufficiently small, <a href="#eq-w-update" class="quarto-xref">Equation&nbsp;<span>9.2</span></a> is guaranteed to reduce the value of <span class="math inline">\(g\)</span>.</p>
</div>
</div>
<p>Let’s see an example of single-variable gradient descent in action:</p>
<div id="cell-5" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>w_     <span class="op">=</span> <span class="op">-</span><span class="fl">0.7</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>grad  <span class="op">=</span> <span class="kw">lambda</span> w: <span class="dv">2</span><span class="op">*</span>w</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>w_vec <span class="op">=</span> [w_]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        w_ <span class="op">=</span> w_ <span class="op">-</span> alpha<span class="op">*</span>grad(w_) </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        w_vec.append(w_)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>w_vec <span class="op">=</span> torch.tensor(w_vec)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>plt.plot(w, g(w), label <span class="op">=</span> <span class="vs">r"$g(w)$"</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plt.scatter(w_vec, g(w_vec), color <span class="op">=</span> <span class="st">"black"</span>, label <span class="op">=</span> <span class="vs">r"Gradient descent updates"</span>, s <span class="op">=</span> <span class="dv">10</span>, zorder <span class="op">=</span> <span class="dv">10</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="vs">r"$w$"</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="23-gradient-descent_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can see the updates from gradient descent eventually converging to the point <span class="math inline">\(w = 0\)</span>, which is the global minimum of this function.</p>
</section>
<section id="gradient-descent-in-multiple-dimensions" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-in-multiple-dimensions">Gradient Descent in Multiple Dimensions</h2>
<p>Our empirical risk function <span class="math inline">\(L\)</span> is not a single-variable function; indeed, <span class="math inline">\(L: \mathbb{R}^p \rightarrow \mathbb{R}\)</span>. So, we can’t directly apply the results above. Fortunately, these results extend in a smooth way to this setting. The main thing we need is the definition of the <em>gradient</em> of a multivariate function.</p>
</section>
<section id="gradients" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="gradients">Gradients</h2>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="margin-aside">We’re not going to talk much about what it means for a function to be multivariate differentiable. You can assume that all the functions we will deal with in this class are unless I highlight otherwise. For a more rigorous definition, you should check out a multivariable calculus class.</span></div></div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-gradient" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.1 (Gradient of a Multivariate Function)</strong></span> Let <span class="math inline">\(g:\mathbb{R}^p \rightarrow \mathbb{R}\)</span> be a <em>multivariate differentiable</em> function. The <em>gradient</em> of <span class="math inline">\(g\)</span> evaluated at point <span class="math inline">\(\mathbf{w}\in \mathbb{R}^p\)</span> is written <span class="math inline">\(\nabla g(\mathbf{w})\)</span>, and has value</p>
<p><span class="math display">\[
\nabla g(\mathbf{w}) \triangleq
\left(\begin{matrix}
    \frac{\partial g(\mathbf{w})}{\partial w_1} \\
    \frac{\partial g(\mathbf{w})}{\partial w_2} \\
    \cdots \\
    \frac{\partial g(\mathbf{w})}{\partial w_p} \\
\end{matrix}\right) \in \mathbb{R}^p\;.
\]</span></p>
<p>Here, <span class="math inline">\(\frac{\partial g(\mathbf{w})}{\partial w_1}\)</span> is the <em>partial derivative of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(z_1\)</span>, evaluated at <span class="math inline">\(\mathbf{w}\)</span></em>. To compute it:</p>
<blockquote class="blockquote">
<p>Take the derivative of <span class="math inline">\(f\)</span> *with respect to variable <span class="math inline">\(z_1\)</span>, holding all other variables constant, and then evaluate the result at <span class="math inline">\(\mathbf{w}\)</span>.</p>
</blockquote>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(p = 3\)</span>. Let <span class="math inline">\(g(\mathbf{w}) = w_2\sin w_1  + w_1e^{2w_3}\)</span>. The partial derivatives we need are</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial g(\mathbf{w})}{\partial w_1} &amp;= w_2 \cos w_1 + e^{2w_3}\\
\frac{\partial g(\mathbf{w})}{\partial w_2} &amp;= \sin w_1\\
\frac{\partial g(\mathbf{w})}{\partial w_3} &amp;= 2w_1 e^{2w_3}\;.
\end{align}
\]</span></p>
<p>So, the gradient of <span class="math inline">\(g\)</span> evaluated at a point <span class="math inline">\(\mathbf{w}\)</span> is</p>
<p><span class="math display">\[
\nabla g(\mathbf{w}) =
\left(\begin{matrix}
    \frac{\partial g(\mathbf{w})}{\partial w_1} \\
    \frac{\partial g(\mathbf{w})}{\partial w_2} \\
    \frac{\partial g(\mathbf{w})}{\partial w_3} \\
\end{matrix}\right) =
\left(\begin{matrix}
    w_2 \cos w_1 + e^{2w_3}\\
    \sin w_1\\
    2w_1 e^{2w_3}
\end{matrix}\right)
\]</span></p>
</div>
</div>
<p>Taylor’s Theorem extends smoothly to this setting.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-taylor" class="theorem">
<p><span class="theorem-title"><strong>Theorem 9.2 (Taylor’s Theorem: Multivariate Functions)</strong></span> Let <span class="math inline">\(g:\mathbb{R}^p\rightarrow \mathbb{R}\)</span> be differentiable at point <span class="math inline">\(\mathbf{w}_0 \in \mathbb{R}^p\)</span>. Then, there exists <span class="math inline">\(a &gt; 0\)</span> such that, if <span class="math inline">\(\lVert \delta \mathbf{w} \rVert &lt; a\)</span>, then </p>
<p><span class="math display">\[
g(\mathbf{w}_0 + \delta \mathbf{w}) = g(\mathbf{w}_0) + \langle \nabla g(\mathbf{w}_0), \delta \mathbf{w} \rangle + o(\lVert \delta \mathbf{w}\rVert)\;.
\]</span></p>
</div>
</div>
</div>
</div>
<div class="no-row-height column-margin column-container"><span class="margin-aside callout-margin-content callout-margin-content-simple"><span class="math inline">\(\lVert \mathbf{\delta} \mathbf{w}\rVert \triangleq \sqrt{\sum_{i = 1}^p (\delta w_i)^2}\)</span></span></div><p>The vector <span class="math inline">\(\nabla g(\mathbf{w}_0)\)</span> plays the role of the single-variable derivative <span class="math inline">\(\frac{d g(w_0)}{dw}\)</span>.</p>
<section id="multivariate-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="multivariate-gradient-descent">Multivariate Gradient Descent</h3>
<p>In multiple dimensions, the gradient descent update is:</p>
<p><span id="eq-gradient-descent-multi"><span class="math display">\[
\begin{aligned}
    \hat{\mathbf{w}} \gets \hat{\mathbf{w}} - \alpha \nabla g(\hat{\mathbf{w}})\;.
\end{aligned}
\tag{9.3}\]</span></span></p>
<p>Let’s check that a single update of gradient descent will reduce the value of <span class="math inline">\(g\)</span> provided that <span class="math inline">\(\alpha\)</span> is small enough. Here, <span class="math inline">\(\delta \hat{\mathbf{w}} = -\alpha \nabla g(\hat{\mathbf{w}})\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
    g(\hat{\mathbf{w}} - \delta \hat{\mathbf{w}}) &amp;\approx g(\hat{\mathbf{w}}) + \langle \nabla g(\mathbf{w}_0), \delta \mathbf{w} \rangle \\
    &amp;= g(\hat{\mathbf{w}}) + \langle \nabla g(\hat{\mathbf{w}}), -\alpha \nabla g(\hat{\mathbf{w}}) \rangle \\
    &amp;= g(\hat{\mathbf{w}}) - \alpha \langle \nabla g(\hat{\mathbf{w}}),  \nabla g(\hat{\mathbf{w}}) \rangle \\
    &amp;= g(\hat{\mathbf{w}}) - \alpha \lVert\nabla g(\hat{\mathbf{w}}) \rVert^2\;.
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\lVert\nabla g(\hat{\mathbf{w}}) \rVert^2 &gt; 0\)</span> whenever <span class="math inline">\(\nabla g(\hat{\mathbf{w}}) \neq\mathbf{0}\)</span>, we conclude that, unless <span class="math inline">\(\hat{w}\)</span> is a critical point (where the gradient is zero), then</p>
<p><span class="math display">\[
\begin{aligned}
    g(\hat{\mathbf{w}} - \alpha \nabla g(\hat{\mathbf{w}})) &lt; g(\hat{\mathbf{w}})\;.
\end{aligned}
\]</span></p>
<p>In other words, provided that <span class="math inline">\(\alpha\)</span> is small enough for the Taylor approximation to be a good one, multivariate gradient descent also always reduces the value of the objective function.</p>
</section>
</section>
<section id="gradient-of-the-empirical-risk" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="gradient-of-the-empirical-risk">Gradient of the Empirical Risk</h2>
<p>Remember that our big objective here was to solve <a href="#eq-erm" class="quarto-xref">Equation&nbsp;<span>9.1</span></a> using gradient descent. To do this, we need to be able to calculate <span class="math inline">\(\nabla L(\mathbf{w})\)</span>, where the gradient is with respect to the entries of <span class="math inline">\(\mathbf{w}\)</span>. Fortunately, the specific <em>linear</em> structure of the score function <span class="math inline">\(s_i = \langle \mathbf{w}, \mathbf{x}_i \rangle\)</span> makes this relatively simple: indeed, we actually only need to worry about the <em>single</em> variable derivatives of the per-observation loss <span class="math inline">\(\ell\)</span>. To see this, we can compute</p>
<p><span class="math display">\[
\begin{align}
\nabla L(\mathbf{w}) &amp;= \nabla \left(\frac{1}{n} \sum_{i = 1}^n \ell(\langle \mathbf{w}, \mathbf{x}_i\rangle , y_i)\right) \\
              &amp;= \frac{1}{n} \sum_{i = 1}^n \nabla \ell(\langle \mathbf{w}, \mathbf{x}_i\rangle , y_i) \\
              &amp;= \frac{1}{n} \sum_{i = 1}^n  \frac{d\ell(s_i, y_i)}{ds} \nabla \langle \mathbf{w}, \mathbf{x}_i\rangle  &amp;\quad \text{(multivariate chain rule)} \\
              &amp;= \frac{1}{n} \sum_{i = 1}^n  \frac{d\ell(s_i, y_i)}{ds}  \mathbf{x}_i &amp;\quad \text{(gradient of a linear function)} \\
              &amp;= \frac{1}{n} \sum_{i = 1}^n  \frac{d\ell(s_i, y_i)}{ds} \mathbf{x}_i &amp;\quad \text{($s_i = \langle \mathbf{w}, \mathbf{x}_i\rangle$)} \\
\end{align}
\]</span></p>
<p>The good news here is that for linear models, we don’t actually need to be able to compute more gradients: we just need to be able to compute derivatives of the form <span class="math inline">\(\frac{d\ell(s_i, y_i)}{ds}\)</span> and then plug in <span class="math inline">\(s_i = \langle \mathbf{w}, \mathbf{x}_i\rangle\)</span>.</p>
<p>Let’s do an example with the logistic loss:</p>
<p><span class="math display">\[\ell(s, y) = -y \log \sigma(s) - (1-y)\log (1-\sigma(s))\;.\]</span></p>
<p>A useful fact to know about the logistic sigmoid function <span class="math inline">\(\sigma\)</span> is that <span class="math inline">\(\frac{d\sigma(s) }{ds} = \sigma(s) (1 - \sigma(s))\)</span>. So, using that and the chain rule, the derivative we need is</p>
<p><span class="math display">\[
\begin{align}
\frac{d\ell(s, y)}{ds} &amp;= -y \frac{1}{\sigma(s)}\frac{d\sigma(s) }{ds} - (1-y)\frac{1}{1-\sigma(s)}\left(- \frac{d\sigma(s) }{ds}\right) \\
&amp;= -y \frac{1}{\sigma(s)}\sigma(s) (1 - \sigma(s)) - (1-y)\frac{1}{1-\sigma(s)}\left(- \sigma(s) (1 - \sigma(s))\right) \\
&amp;= -y (1 - \sigma(s)) + (1-y)\sigma(s) \\
&amp;= \sigma(s) - y\;.
\end{align}
\]</span></p>
<p>Finally, we need to plug this back in to our empirical risk, obtaining the gradient of the empirical risk for logistic regression:</p>
<div class="page-columns page-full"><p> <span id="eq-logistic-gradient"><span class="math display">\[
\begin{align}
\nabla L(\mathbf{w}) &amp;= \frac{1}{n} \sum_{i = 1}^n (\sigma(s_i) - y_i)\mathbf{x}_i \\
              &amp;=\frac{1}{n} \sum_{i = 1}^n (\sigma(\langle \mathbf{w}, \mathbf{x}_i\rangle) - y_i)\mathbf{x}_i\;.
\end{align}
\tag{9.4}\]</span></span></p><div class="no-row-height column-margin column-container"><span class="margin-aside">An important note about this formula that can easily trip one up: this looks a bit like a matrix multiplication or dot product, but it isn’t!</span></div></div>
<p>This gives us all the math that we need in order to learn logistic regression by choosing a learning rate and iterating the update <span class="math inline">\(\mathbf{w}^{(t+1)} \gets \mathbf{w}^{(t)} - \alpha \nabla L(\mathbf{w}^{(t)})\)</span> until convergence.</p>
</section>
<section id="example-logistic-regression" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="example-logistic-regression">Example: Logistic Regression</h2>
<p>Let’s see gradient-descent in action for logistic regression. Our computational approach is based on the <code>LinearModel</code> class that you previously started implementing. The training loop is also <em>very</em> similar to our training loop for the perceptron. The main difference is that the loss is calculated using the <code>binary_cross_entropy</code> function above, and the <code>step</code> function of the <code>GradientDescentOptimizer</code> works differently in a way that we will discuss below.</p>
<p><em>Starting with the code block below, you won’t be able to follow along in coding these notes unless you have sneakily implemented logistic regression in a <code>hidden</code> module.</em></p>
<div id="cell-7" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> classification_data(n_points <span class="op">=</span> <span class="dv">300</span>, noise <span class="op">=</span> <span class="fl">0.2</span>, p_dims <span class="op">=</span> <span class="dv">2</span>):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> torch.arange(n_points) <span class="op">&gt;=</span> <span class="bu">int</span>(n_points<span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="fl">1.0</span><span class="op">*</span>y</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> y[:, <span class="va">None</span>] <span class="op">+</span> torch.normal(<span class="fl">0.0</span>, noise, size <span class="op">=</span> (n_points,p_dims))</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> torch.cat((X, torch.ones((X.shape[<span class="dv">0</span>], <span class="dv">1</span>))), <span class="dv">1</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, y</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> classification_data(noise <span class="op">=</span> <span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The logistic regression training loop relies on a new implementation of <code>opt.step</code>. For gradient descent, here’s the complete implementation: just a quick Python version of the gradient descent update <a href="#eq-gradient-descent-multi" class="quarto-xref">Equation&nbsp;<span>9.3</span></a>.</p>
<div id="cell-9" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step(<span class="va">self</span>, X, y, lr <span class="op">=</span> <span class="fl">0.01</span>):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.model.w <span class="op">-=</span> lr<span class="op">*</span><span class="va">self</span>.model.grad(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The method <code>model.grad()</code> is the challenging part of the implementation: this is where we actually need to turn <a href="#eq-logistic-gradient" class="quarto-xref">Equation&nbsp;<span>9.4</span></a> into code.</p>
<p>Here’s the complete training loop. This loop is <em>very</em> similar to our perceptron training loop – we’re just using a different loss and a different implementation of <code>grad</code>.</p>
<div id="cell-fig-LR-loss-iterations" class="cell page-columns page-full" data-execution_count="15">
<div class="sourceCode cell-code" id="cb6" data-cap-location="margin"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hidden.logistic <span class="im">import</span> LogisticRegression, GradientDescentOptimizer</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># instantiate a model and an optimizer</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> LogisticRegression() </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> GradientDescentOptimizer(LR)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># for keeping track of loss values</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>loss_vec <span class="op">=</span> []</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># not part of the update: just for tracking our progress    </span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> LR.loss(X, y) </span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    loss_vec.append(loss)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># only this line actually changes the parameter value</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The whole definition is: </span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># self.model.w -= lr*self.model.grad(X, y)</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    opt.step(X, y, lr <span class="op">=</span> <span class="fl">0.02</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>plt.plot(torch.arange(<span class="dv">1</span>, <span class="bu">len</span>(loss_vec)<span class="op">+</span><span class="dv">1</span>), loss_vec, color <span class="op">=</span> <span class="st">"black"</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>plt.semilogx()</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>labs <span class="op">=</span> plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"Number of gradient descent iterations"</span>, ylabel <span class="op">=</span> <span class="st">"Loss (binary cross entropy)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display page-columns page-full">
<div id="fig-lr-loss-iterations" class="quarto-figure quarto-figure-center quarto-float anchored page-columns page-full" data-cap-location="margin">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-lr-loss-iterations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="23-gradient-descent_files/figure-html/fig-lr-loss-iterations-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-lr-loss-iterations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1: Evolution of the binary cross entropy loss function in the logistic regression training loop.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The loss quickly levels out to a constant value, which is our optimized weight vector <span class="math inline">\(\mathbf{w}\)</span>. Because our theory tells us that the loss function is convex, we know that the value of <span class="math inline">\(\mathbf{w}\)</span> we have found is the best possible, in the sense of minimizing the loss.</p>
<p>Let’s check our training accuracy:</p>
<div id="cell-13" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>(<span class="fl">1.0</span><span class="op">*</span>(LR.predict(X) <span class="op">==</span> y)).mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>tensor(0.9167)</code></pre>
</div>
</div>
<p>Not too bad!</p>
</section>
<section id="recap" class="level2">
<h2 class="anchored" data-anchor-id="recap">Recap</h2>
<p>In these lecture notes, we introduced <em>gradient descent</em> as a method for minimizing functions, and showed an application of gradient descent for logistic regression. Gradient descent is especially useful when working with convex functions, since in this case it is guaranteed to converge to the global minimum of the empirical risk (provided that the learning rate <span class="math inline">\(\alpha\)</span> is sufficiently low). The idea of gradient descent – incremental improvement to the weight vector <span class="math inline">\(\mathbf{w}\)</span> using information about the derivatives of the loss function–is a fundamental one which has led to many variations and improvements.</p>


</section>

<p><br> <br> <span style="color:grey;">© Phil Chodrow, 2024</span></p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/22-convex-erm.html" class="pagination-link" aria-label="Convex Empirical Risk Minimization">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Convex Empirical Risk Minimization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/30-features-regularization.html" class="pagination-link" aria-label="Feature Maps, Regularization, and Generalization">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Feature Maps, Regularization, and Generalization</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>