[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Welcome\nThis is a set of notes developed for an undergraduate course in machine learning. The target audience for these notes are undergraduates in computer science who have completed first courses in linear algebra and discrete mathematics. These notes draw on many sources, but are somewhat distinctive in the following ways:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#pedagogical-features",
    "href": "index.html#pedagogical-features",
    "title": "Machine Learning",
    "section": "Pedagogical Features",
    "text": "Pedagogical Features\nThese notes are explicitly designed for undergraduate instruction in computer science. For this reason:\n\nComputational examples are integrated into the text and shown throughout.\nLive versions of lecture notes are supplied as downloadable Jupyter Notebooks, with certain code components removed. The purpose is to facilitate live-coding in lectures.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#use-and-reuse",
    "href": "index.html#use-and-reuse",
    "title": "Machine Learning",
    "section": "Use and Reuse",
    "text": "Use and Reuse\nThese notes were written by Phil Chodrow for the course CSCI 0451: Machine Learning at Middlebury College. All are welcome to use them for educational purposes. Attribution is appreciated but not expected.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#source-texts",
    "href": "index.html#source-texts",
    "title": "Machine Learning",
    "section": "Source Texts",
    "text": "Source Texts\nThese notes draw on several source texts, most of which are available for free online. These are:\n\nHardt and Recht (2022) is the primary influence for the overall arc of the notes.\nA Course in Machine Learning by Hal Daumé III is an accessible introduction to many of the topics and serves as a useful source of supplementary readings.\n\nAdditional useful readings:\n\nAbu-Mostafa, Magdon-Ismail, and Lin (2012): Learning From Data: A Short Course\nBarocas, Hardt, and Narayanan (2023) is an advanced text on questions of fairness in automated decision-making for readers who have some background in probability theory.\nBishop (2006) and Murphy (2022) are advanced texts which are most suitable for advanced readers who have already taken at least one course in probability theory.\n\nDeisenroth, Faisal, and Ong (2020) and Kroese et al. (2020) are useful readings focusing on some of the mathematical fundamentals.\nZhang, Lipton, and Li (2023) tells a helpful story of the fundamentals of deep learning.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Machine Learning",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis site was generated using the Quarto publishing system. It is hosted on GitHub and published via GitHub Pages.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Machine Learning",
    "section": "References",
    "text": "References\n\n\n\n\nAbu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012. Learning from Data: A Short Course. S.l. https://amlbook.com/.\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press. https://fairmlbook.org/pdf/fairmlbook.pdf.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nDeisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020. Mathematics for Machine Learning. Cambridge, UK New York, NY: Cambridge University Press. https://mml-book.github.io/book/mml-book.pdf.\n\n\nHardt, Moritz, and Benjamin Recht. 2022. Patterns, Predictions, and Actions. Princeton University Press. https://mlstory.org/pdf/patterns.pdf.\n\n\nKroese, Dirk P., Zdravko I. Botev, Thomas Taimre, and Radislav Vaisman. 2020. Data Science and Machine Learning: Mathematical and Statistical Methods. Chapman & Hall/CRC Machine Learning & Pattern Recognition Series. Boca Raton London New York: CRC Press, Taylor & Francis Group.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. Adaptive Computation and Machine Learning Series. Cambridge, Massachusetts: The MIT Press. https://probml.github.io/pml-book/book1.html.\n\n\nZhang, Aston, Zachary Lipton, and Mu Li. 2023. Dive into Deep Learning. Cambridge, UK: Cambridge University Press.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/01-data-and-models.html",
    "href": "chapters/01-data-and-models.html",
    "title": "1  Data, Patterns, and Models",
    "section": "",
    "text": "Supervised Learning\nOur focus in these notes is almost exclusively on supervised learning. In supervised learning, we are able to view some attributes or features of a data point, which we call predictors. Traditionally, we collect these attributes into a vector called \\(\\mathbf{x}\\). Each data point then has a target, which could be either a scalar number or a categorical label. Traditionally, the target is named \\(y\\). We aim to predict the target based on the predictors using a model, which is a function \\(f\\). The result of applying the model \\(f\\) to the predictors \\(\\mathbf{x}\\) is our prediction or predicted target \\(f(\\mathbf{x})\\), to which we often give the name \\(\\hat{y}\\). Our goal is to choose \\(f\\) such that the predicted target \\(\\hat{y}\\) is equal to, or at least close to, the true target \\(y\\). We could summarize this with the heuristic statement:\n\\[\n\\begin{aligned}\n    \\text{``}f(\\mathbf{x}) = \\hat{y} \\approx y\\;.\\text{''}\n\\end{aligned}\n\\]\nHow we interpret this heuristic statement depends on context. In regression problems, this statement typically means “\\(\\hat{y}\\) is usually close to \\(y\\)”, while in classification problems this statement usually means that “\\(\\hat{y} = y\\) exactly most or all of the time.”\nIn our regression example from above, we can think of a function \\(f:\\mathbb{R}\\rightarrow \\mathbb{R}\\) that maps the predictor \\(x\\) to the prediction \\(\\hat{y}\\). In the case of classification, things are a little more complicated. Although the function \\(g(x_1) = 1 - x_1\\) is visually very relevant, that function is not itself the model we use for prediction. Instead, our prediction function should return one classification label for points on one side of the line defined by that function, and a different label for points on the other side. If we say that blue points are labeled \\(0\\) and brown points are labeled \\(1\\), then our predictor function can be written \\(f:\\mathbb{R}^2 \\rightarrow \\{0, 1\\}\\), and it could be written heuristically like this:\n\\[\n\\begin{aligned}\n    f(\\mathbf{x}) &= \\mathbb{1}[\\mathbf{x} \\text{ is above the line}] \\\\\n                  &= \\mathbb{1}[x_1 + x_2 \\geq 1] \\\\\n                  &= \\mathbb{1}[x_1 + x_2 - 1\\geq 0]\\;.\n\\end{aligned}\n\\]\nThis last expression looks a little clunky, but we will soon find out that it is the easiest one to generalize to an advanced setting.",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data, Patterns, and Models</span>"
    ]
  },
  {
    "objectID": "chapters/01-data-and-models.html#supervised-learning",
    "href": "chapters/01-data-and-models.html#supervised-learning",
    "title": "1  Data, Patterns, and Models",
    "section": "",
    "text": "Here, \\(\\mathbb{1}\\) is the indicator function which is equal to 1 if its argument is true and 0 otherwise. Formally,\n\\[\n\\begin{aligned}\n    \\mathbb{1}[P] = \\begin{cases}\n        1 &\\quad P \\text{ is true} \\\\\n        0 &\\quad P \\text{ is false.}\n        \\end{cases}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data, Patterns, and Models</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html",
    "href": "chapters/02-black-box-classification.html",
    "title": "2  Classification as a Black Box",
    "section": "",
    "text": "Classifying the Palmer Penguins\nOur data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins.\nLet’s go ahead and acquire the data.\nimport warnings\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\nLet’s take a look:\ndf.head() # first 5 rows\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\nIt’s always useful to get acquainted with the “basics” of the data. For example, how many rows and columns do we have?\ndf.shape # (rows, columns)\n\n(344, 17)\ndf.dtypes \n\nstudyName               object\nSample Number            int64\nSpecies                 object\nRegion                  object\nIsland                  object\nStage                   object\nIndividual ID           object\nClutch Completion       object\nDate Egg                object\nCulmen Length (mm)     float64\nCulmen Depth (mm)      float64\nFlipper Length (mm)    float64\nBody Mass (g)          float64\nSex                     object\nDelta 15 N (o/oo)      float64\nDelta 13 C (o/oo)      float64\nComments                object\ndtype: object\nHere’s the question we’ll ask today about this data set:",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#classifying-the-palmer-penguins",
    "href": "chapters/02-black-box-classification.html#classifying-the-palmer-penguins",
    "title": "2  Classification as a Black Box",
    "section": "",
    "text": "Image source: @allisonhorst\n\n\n\nThe Palmer Penguins data was originally collected by Gorman, Williams, and Fraser (2014) and was nicely packaged and released for use in the data science community by Horst, Hill, and Gorman (2020). You can find a very concise summary of the main workflow using a similar data set in Vanderplas (2016).\n\n\n\n\n The df variable holds a pandas.DataFrame object. You can think of a data frame as a table of data with a variety of useful behaviors for data manipulation and visualization.You can learn much more about the capabilities of pandas.DataFrame objects in Chapter 3 of Vanderplas (2016)\n\n\n\n\nWhat are the data types of the columns? str columns are represented with the generic object in Pandas.\n\n\n\nGiven some physiological measurements of a penguin, can we reliably infer its species?",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#data-preparation",
    "href": "chapters/02-black-box-classification.html#data-preparation",
    "title": "2  Classification as a Black Box",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe can select our desired columns from the data frame, operate on them, and make assignments to them using the data-frame-as-dictionary paradigm explored in Vanderplas (2016).\nIn applied data science, at least 80% of the work is typically spent acquiring and preparing data. Here, we’re going to do some simple data preparation directed by our question. It’s going to be convenient to shorten the Species column for each penguin. Furthermore, for visualization purposes today we are going to focus on the Culmen Length (mm) and Culmen Depth (mm) columns.\n\n# use only these three columns\ndf = df[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Species\"]] \n\n# remove any rows that have missing data in any of the selected columns. \ndf = df.dropna()\n\n# slightly advanced syntax: \n# replace the column with the first word in each entry\ndf[\"Species\"] = df[\"Species\"].str.split().str.get(0)\n\nLet’s take a look at what we’ve done so far:\n\ndf.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nSpecies\n\n\n\n\n0\n39.1\n18.7\nAdelie\n\n\n1\n39.5\n17.4\nAdelie\n\n\n2\n40.3\n18.0\nAdelie\n\n\n4\n36.7\n19.3\nAdelie\n\n\n5\n39.3\n20.6\nAdelie\n\n\n\n\n\n\n\n\nAs another preprocessing step, we are going to add transformed labels represented as integers.\n\n# for later: assign an integer to each species\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf[\"species_label\"] = le.fit_transform(df[\"Species\"])\n\nfor i, c in enumerate(le.classes_):\n    print(f\"Class number {i} represents {c} penguins.\")\n\nClass number 0 represents Adelie penguins.\nClass number 1 represents Chinstrap penguins.\nClass number 2 represents Gentoo penguins.\n\n\nNow our data looks like this:\n\ndf.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nSpecies\nspecies_label\n\n\n\n\n0\n39.1\n18.7\nAdelie\n0\n\n\n1\n39.5\n17.4\nAdelie\n0\n\n\n2\n40.3\n18.0\nAdelie\n0\n\n\n4\n36.7\n19.3\nAdelie\n0\n\n\n5\n39.3\n20.6\nAdelie\n0\n\n\n\n\n\n\n\n\n\nTrain-Test Split\nWhen designing predictive models, it’s important to evaluate them in a context that simulates the prediction application as accurately as possible. One important way we do this is by performing a train-test split. We keep most of the data as training data which we’ll use to design the model. We’ll hold out a bit of the data as testing data, which we’ll treat as unseen and only use once we are ready to evaluate our final design. The testing data simulates the idea of “new, unseen data” – exactly the kind of data on which it would be useful for us to make predictions!\n\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, test_size = 0.2)\n\nLet’s check the size of our two split data sets:\n\ndf_train.shape, df_test.shape\n\n((273, 4), (69, 4))\n\n\nNow we’re going to forget that df_test exists for a while. Instead, we’ll turn our attention to analysis, visualization and modeling.",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#data-analysis-and-visualization",
    "href": "chapters/02-black-box-classification.html#data-analysis-and-visualization",
    "title": "2  Classification as a Black Box",
    "section": "Data Analysis and Visualization",
    "text": "Data Analysis and Visualization\nAs a first step, it’s useful to understand how many of each species there are in the training data:\nThis is an example of a “split-apply-combine” operation (Wickham 2011). We split the dataframe into three groups depending on the species label, apply an operation (in this case, computing the number of rows), and then combine the results into a single object. Pandas implements split-apply-combine primarily through the groupby method and several associated functions. There are some nice examples of split-apply-combine in Pandas in Vanderplas (2016).\n\ndf_train.groupby(\"Species\").size()\n\nSpecies\nAdelie       119\nChinstrap     56\nGentoo        98\ndtype: int64\n\n\nThere are more Adelie penguins than Chintraps or Gentoos in this data set. Here are the proportions:\n\ndf_train.groupby(\"Species\").size() / df_train.shape[0] # divide by total rows\n\nSpecies\nAdelie       0.435897\nChinstrap    0.205128\nGentoo       0.358974\ndtype: float64\n\n\nSo, over 40% of the penguins in the data are Adelie penguins. One important consequence of this proportion is the base rate of the classification problem. The base rate refers to how well we could perform at prediction if we did not use any kind of predictive modeling, but instead simply predicted the most common class for every penguin. Here, if we always predicted “Adelie” for the species, we’d expect to be right more than 40% of the time. So, a minimal expectation of anything fancier we do is that it should be correct much more than 40% of the time.\nNow let’s take a look at our (training) data and see whether our chosen columns look like they have a chance of predicting the penguin species. We’ll show the plot both without and with the species labels.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\np1 = sns.scatterplot(df_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", ax = ax[0], color = \"darkgrey\")\np2 = sns.scatterplot(df_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue = \"Species\", style = \"Species\", ax = ax[1])\n\n\n\n\nThese plots are generated using the Seaborn library for Python. Seaborn is a high-level wrapper around the classical matplotlib library for data visualization. Although Matplotlib is very flexible, Seaborn is optimized for visualizing data contained in Pandas data frames. You can find many examples of creating Seaborn plots in the official gallery, and many tips and examples for matplotlib in Vanderplas (2016).\n\n\n\n\nWe can think of the lefthand side as “what the model will see:” just physiological measurements with no labels. On the right we can see the data with its species labels included. We can see that the species are divided into clusters: Adelie penguins have measurements which tend to be similar to other Adelies; Chinstraps are similar to other Chinstraps, etc.\nThis pattern is promising! The approximate separation of the species suggests that a machine learning model which predicts the species label from these measurements is likely to be able to beat the base rate.",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#modeling-and-model-selection",
    "href": "chapters/02-black-box-classification.html#modeling-and-model-selection",
    "title": "2  Classification as a Black Box",
    "section": "Modeling and Model Selection",
    "text": "Modeling and Model Selection\nLet’s go ahead and fit some models! We’re going to fit two models that are pre-implemented in the package scikit-learn. For now, you can think of these models as black-box algorithms that accept predictor variables as inputs and return a predicted target as an output. In our case, the predictor variables are the culmen length and culmen depth columns, while the target we are attempting to predict is the species. Later on, we’ll learn more about how some of these models actually work.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\nIt’s convenient to split our data into predictors \\(\\mathbf{X}\\) and targets \\(\\mathbf{y}\\). We need to do this once for each of the training and test sets.\n\npredictor_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\ntarget_col = \"species_label\"\n\nX_train = df_train[predictor_cols]\ny_train = df_train[target_col]\n\nX_test = df_test[predictor_cols]\ny_test = df_test[target_col]\n\nLet’s take a quick look at X_train\n\nX_train\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\n\n\n\n\n281\n46.2\n14.9\n\n\n320\n48.5\n15.0\n\n\n100\n35.0\n17.9\n\n\n206\n42.5\n17.3\n\n\n117\n37.3\n20.5\n\n\n...\n...\n...\n\n\n332\n43.5\n15.2\n\n\n73\n45.8\n18.9\n\n\n210\n50.2\n18.8\n\n\n250\n47.3\n15.3\n\n\n282\n45.7\n13.9\n\n\n\n\n273 rows × 2 columns\n\n\n\n\nWe’ll go in-depth on logistic regression later in this course.\nNow we’re ready to fit our first machine learning model. Let’s try logistic regression! In the Scikit-learn API, we first need to instantiate the LogisticRegression() class, and then call the fit() method of this class on the training predictors and targets.\n\nLR = LogisticRegression()\nm = LR.fit(X_train, y_train)\n\nSo, uh, did it work? The LogisticRegression() class includes a handy method to compute the accuracy of the classifier:\n\nLR.score(X_train, y_train)\n\n0.9706959706959707\n\n\nWow! Much better than the base rate. Note that this is the accuracy on the training data. In theory, accuracy on the test data could look very different.\nA useful way to visualize models with two numerical predictors is via decision regions. Each region describes the set of possible measurements that would result in a given classification.\nYou can unfold this code to see a simple implementation of a function for plotting decision regions which wraps the plot_decision_regions function of the mlxtend package.\n\n\nCode\ndef decision_regions(X, y, model, title):\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        ax = plot_decision_regions(X_train.to_numpy(), y_train.to_numpy(), clf = model, legend = 2)\n\n        handles, labels = ax.get_legend_handles_labels()\n        ax.legend(handles, \n                le.classes_, \n                framealpha=0.3, scatterpoints=1)\n\n        ax.set(xlabel = \"Culmen Length (mm)\", ylabel = \"Culmen Depth (mm)\", title = f\"{title}: Accuracy = {model.score(X, y):.3f}\")\n\ndecision_regions(X_train, y_train, LR, \"Decision Regions for Logistic Regression\")\n\n\n\n\n\n\n\n\n\nYou can learn more about how support vector machines work in Vanderplas (2016). We’ll also study these models later in the course.\nWhile we’re at it, let’s try fitting a different classifier, also supplied by Scikit-learn. This classifier is called support vector machine (SVM).\n\nSVM = SVC(gamma = 5)\nSVM.fit(X_train, y_train)\ndecision_regions(X_train, y_train, SVM, \"Decision Regions for Support Vector Machine\")\n\n\n\n\n\n\n\n\nWow! The support vector machine classifier achieved even higher accuracy on the training data. This is enabled by the greater flexibility of the SVM. Flexibility comes from a lot of places in machine learning, and generally refers to the ability of models to learn complicated decision boundaries like the ones shown here.\nBut is this increased flexibility a good thing? You might look at this predictor and think that something funny is going on. For example, shouldn’t a point on the bottom right be more likely to be a Gentoo penguin than an Adelie?…\n\nSimulating Evaluation: Cross-Validation\nNow we have two competing classification models: logistic regression and support vector machine. Which one is going to do the best job of prediction on totally new, unseen data? We could go ahead and evaluate on our test set, but for statistical reasons we need to avoid doing this until we’ve made a final choice of classifier.\nVanderplas (2016) has more on cross-validation and overfitting. We’ll confront overfitting agian many times in this course.\nIn order to make an assessment, we can simulate the process of fitting the model and evaluating on “test” data by witholding parts of our training data to use as testing. We split the data into chunks and withold each chunk, using the other chunks to train the data. This is called cross-validation, and it is illustrated in this figure:\n\n\n\nImage source: scikit-learn\n\n\nWe could do this with a janky for-loop, but the nice scikit-learn developers have implemented this for us. Here’s an example of cross-validation with 5 folds. This can take a little while, as there are actually 5 calls to model.fit() happening under the hood each time.\n\nfrom sklearn.model_selection import cross_val_score\n\nFirst let’s compute the cross-validation accuracies for logistic regression:\n\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv=5)\ncv_scores_LR\n\narray([0.982, 0.945, 0.982, 0.944, 0.963])\n\n\nA convenient way to summarize these results is by computing the average:\n\ncv_scores_LR.mean()\n\n0.9632996632996633\n\n\nLet’s compare to SVM:\n\ncv_scores_SVM = cross_val_score(SVM, X_train, y_train, cv=5)\ncv_scores_SVM.mean()\n\n0.8824242424242424\n\n\nAh! It looks like our SVM classifier was indeed too flexible to do well in predicting data that it hasn’t seen before. Although the SVM had better training accuracy than the logistic regression model, it failed to generalize to the task of unseen prediction. This phenomenon is called overfitting. Dealing with overfitting is one of the fundamental modeling challenges in applied machine learning.",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#model-evaluation",
    "href": "chapters/02-black-box-classification.html#model-evaluation",
    "title": "2  Classification as a Black Box",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nSo far, we’ve fit a logistic regression model and a support vector machine model; compared the two on a cross-validation task; and determined that the logistic regression model is most likely to generalize. Let’s now retrain the logistic regression model on the complete training data and finally evaluate it on the test set:\n\nLR.fit(X_train,y_train) \nLR.score(X_test, y_test)\n\n0.927536231884058\n\n\nNot bad! This is our final estimate for the accuracy of our model as a classification tool on unseen penguin data.\n\nBeyond Accuracy\nAccuracy is a simple measure of how many errors a model makes. In many applications, it’s important to understand what kind of errors the model makes, a topic which we’ll study much more when we come to decision theory in the near future. We can get a quick overview of the kinds of mistakes that a model makes by computing the confusion matrix between the true labels and predictions. This matrix cross-tabulates all the true labels with all the predicted ones.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[32,  0,  0],\n       [ 3,  8,  1],\n       [ 0,  1, 24]])\n\n\nThe entry in the ith row and jth column of the confusion matrix gives the number of data points that have true label i and predicted label j from our model.\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 32 Adelie penguin(s) who were classified as Adelie.\nThere were 0 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 3 Chinstrap penguin(s) who were classified as Adelie.\nThere were 8 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 1 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 1 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 24 Gentoo penguin(s) who were classified as Gentoo.",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#recap",
    "href": "chapters/02-black-box-classification.html#recap",
    "title": "2  Classification as a Black Box",
    "section": "Recap",
    "text": "Recap\nIn these notes, we took a very quick tour of the core data science workflow. We considered a simple classification problem in which we acquired some data, cleaned it up a bit, visualized several of its features, used those features to make a predictive classification model, visualized that model, and evaluated its accuracy. Along the way, we encountered the phenomenon of overfitting: models that are too flexible will achieve remarkable accuracy on the training set but will generalize poorly to unseen data. The problem of designing models that are “flexible enough” and “in the right way” is a fundamental driving force in modern machine learning, and the deep learning revolution can be viewed as the latest paradigm for seeking appropriately flexible models.\nSo far, we haven’t attempted to understand how any of these predictive models actually work. We’ll dive into this topic soon.",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#references",
    "href": "chapters/02-black-box-classification.html#references",
    "title": "2  Classification as a Black Box",
    "section": "References",
    "text": "References\n\n\n\n\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” Edited by André Chiaradia. PLoS ONE 9 (3): e90081. https://doi.org/10.1371/journal.pone.0090081.\n\n\nHorst, Allison M, Alison Presmanes Hill, and Kristen B Gorman. 2020. “Allisonhorst/Palmerpenguins: V0.1.0.” Zenodo. https://doi.org/10.5281/ZENODO.3960218.\n\n\nVanderplas, Jacob T. 2016. Python Data Science Handbook: Essential Tools for Working with Data. First edition. Sebastopol, CA: O’Reilly Media, Inc.\n\n\nWickham, Hadley. 2011. “The Split-Apply-Combine Strategy for Data Analysis.” Journal of Statistical Software 40 (1). https://doi.org/10.18637/jss.v040.i01.",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/03-score-based-classification.html",
    "href": "chapters/03-score-based-classification.html",
    "title": "3  Score-Based Classification",
    "section": "",
    "text": "To Lend Or Not To Lend?\nBanks are in the business of lending money, and they must often decide when to loan whom how much money and under what terms. When deciding whether to loan a sum of money, there are two major competing questions:\nBanks can try to balance these risks by controlling interest rates. Higher interest rates increase prospective profit if the loan is repaid in full, but also increase the risk that an individual may be unable to keep up with payments.\nThe judgment of whether to extend an individual a loan is handled by human experts. Recently, human experts have been seeking assistance from machine learning algorithms. As in most predictive modeling, the idea is to use the past to predict the future. Here, we’ll consider simple modeling problem in which we aim to learn patterns in when individuals are able to pay off loans, and use these patterns to make predictions.\nWe’ll first load in the pandas package and use the read_csv command to acquire our data for this problem as a pd.DataFrame. This data set was produced by Kaggle; it is a simulated data set based on patterns in real world data, which, of course, is sensitive and confidential. For today, we are only going to focus on the first 1,000 rows of data.\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np \nplt.style.use('seaborn-v0_8-whitegrid')\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/credit_risk_dataset.csv\"\ndf = pd.read_csv(url)\ndf = df.head(1000).copy()\nLet’s take a look at an excerpt of the data.\ndf\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n22\n59000\nRENT\n123.0\nPERSONAL\nD\n35000\n16.02\n1\n0.59\nY\n3\n\n\n1\n21\n9600\nOWN\n5.0\nEDUCATION\nB\n1000\n11.14\n0\n0.10\nN\n2\n\n\n2\n25\n9600\nMORTGAGE\n1.0\nMEDICAL\nC\n5500\n12.87\n1\n0.57\nN\n3\n\n\n3\n23\n65500\nRENT\n4.0\nMEDICAL\nC\n35000\n15.23\n1\n0.53\nN\n2\n\n\n4\n24\n54400\nRENT\n8.0\nMEDICAL\nC\n35000\n14.27\n1\n0.55\nY\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n22\n46610\nRENT\n6.0\nDEBTCONSOLIDATION\nB\n18000\n10.71\n1\n0.39\nN\n3\n\n\n996\n24\n48000\nRENT\n5.0\nPERSONAL\nA\n18000\n6.54\n1\n0.38\nN\n2\n\n\n997\n23\n24000\nOWN\n0.0\nPERSONAL\nA\n8000\n5.79\n0\n0.33\nN\n3\n\n\n998\n25\n55000\nRENT\n2.0\nEDUCATION\nC\n18000\n12.84\n1\n0.33\nN\n2\n\n\n999\n25\n55344\nRENT\n2.0\nHOMEIMPROVEMENT\nB\n18000\n10.99\n1\n0.33\nN\n4\n\n\n\n\n1000 rows × 12 columns\nEach row of this data set describes a single loan and the attributes of the borrower. For visualization purposes, today we are going to focus on just three of the columns:\nOur primary predictive interest is whether or not a borrower is likely to default on a loan. How common is this in our data set?\ndf[\"loan_status\"].mean()\n\n0.553\nIn this data, roughly 55% of borrowers default on their loan. An important aspect of this learning is the base rate for prediction. If we predicted that every borrower would default on a loan, we would be right 55% of the time. So, if we want to find patterns in our data set and use those patterns to make predictions, we should aim for accuracy greater than 55%.\nSo, can we find some patterns? Here is a labeled scatterplot of our simplified data set.\ndef scatter_data(ax, df):\n\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        to_plot = df[df[\"loan_status\"] == i]\n        ax.scatter(to_plot[\"loan_int_rate\"], to_plot[\"loan_percent_income\"], c = to_plot[\"loan_status\"], vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['repaid', 'defaulted'][i]}\", cmap = \"BrBG\", marker = markers[i])\n        ax.legend()\n        ax.set(xlabel = \"Loan interest rate\", ylabel = \"Loan / income ratio\")\n\ndf = df.head(1000)\nfig, ax = plt.subplots(1, 1)\nscatter_data(ax, df)\nAlthough it looks difficult to completely separate the defaulted loans from the loans which were repaid in full, it does look like there is some pattern to find. Loans which were repaid in full concentrate in the lower right corner of the visualization. This makes sense – these are loans which have low interest rates and which are relatively small sums relative to the annual resources of the borrower.\nA very common approach in problems like this one is to assign, to each loan applicant \\(i\\), a score \\(s_i\\) which predicts their likelihood to default on a loan. Higher scores indicate greater reliability. Let’s formulate a linear score function:\n\\[\n\\begin{aligned}\n    s_i = w_1 \\times (\\text{loan interest rate}_i) + w_2 \\times (\\text{loan percent income}_i)\\;.\n\\end{aligned}\n\\]\nWe can write this score function much more compactly by defining a data point\n\\[\n\\begin{aligned}\n    \\mathbf{x}_{i} = \\left(\\text{loan interest rate}_i, \\text{loan percent income}_i\\right)\n\\end{aligned}\n\\]\nand weight vector\n\\[\n\\begin{aligned}\n    \\mathbf{w} = \\left(w_1, w_2\\right)\\;.\n\\end{aligned}\n\\]\nThen, we can compactly write our score function as\n\\[\n\\begin{aligned}\n    s_i = \\langle \\mathbf{w}, \\mathbf{x}_i\\rangle\\;.\n\\end{aligned}\n\\tag{3.1}\\]\nLet’s implement this score in Python.\ndef linear_score(w, x0, x1):\n    return w[0]*x0 + w[1]*x1\nNow we can plot this score function in the data space.\nShow code\ndef plot_score(ax, score_fun, w, df):\n    \"\"\"\n    Plot a given score function on axis ax with weights w and data df. \n    \"\"\"\n\n    x0_col = \"loan_int_rate\"\n    x1_col = \"loan_percent_income\"\n\n    x0_min, x0_max = df[x0_col].min(), df[x0_col].max()\n    x1_min, x1_max = df[x1_col].min(), df[x1_col].max()\n\n    x0 = np.linspace(x0_min, x0_max, 101)\n    x1 = np.linspace(x1_min, x1_max, 101)\n\n    X0, X1 = np.meshgrid(x0, x1)\n    S = score_fun(w, X0, X1)\n\n    ticks = np.linspace(0, 101, 6)\n\n    im = ax.contourf(X0, X1, S, origin = \"lower\", extent = (x0_min, x0_max, x1_min, x1_max),  cmap = \"BrBG\", vmin = 2*S.min() - S.max(), vmax = 2*S.max() - S.min())\n    \n    ax.set(xlabel = \"Loan interest rate\", ylabel = \"Loan / income ratio\")\n    \n    cbar = plt.colorbar(im, )\n    cbar.set_label(\"Predicted score\")\n\ndef score_viz(score_fun, w, df):\n    fig, ax = plt.subplots(1, 2, figsize = (7, 2.7)) \n    plot_score(ax[0], score_fun, w, df)\n    plot_score(ax[1], score_fun, w, df)\n    scatter_data(ax[1], df)\n    plt.tight_layout()\nTo see the scores, we need to make an initial choice about the weight vector \\(\\mathbf{w}\\).\nw = np.array([0.4, -2])\nscore_viz(linear_score, w, df)\nHmmm, that doesn’t look so good. Ideally, we’d like the higher scores to line up with the borrowers who defaulted, and the lower scores to line up with the borrowers who fully repaid their loans. Can we find a better choice?\nw = np.array([0.01, 1.1])\nscore_viz(linear_score, w, df)",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Score-Based Classification</span>"
    ]
  },
  {
    "objectID": "chapters/03-score-based-classification.html#what-about-nonlinear-scores",
    "href": "chapters/03-score-based-classification.html#what-about-nonlinear-scores",
    "title": "3  Score-Based Classification",
    "section": "What About Nonlinear Scores?",
    "text": "What About Nonlinear Scores?\nYou’ll notice in Figure 3.1 that the decision boundary is a straight line. This is due to the way that we chose to compute scores. Recall that the score function we used is \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\). Since we imposed a threshold \\(t\\), the decision boundary is defined by the equation \\(t = s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\). Generically, this is the equation of a hyperplane (technically, an affine subspace). The dimension of this space is \\(p-1\\), where \\(p\\) is the number of features. Here we have two features, so the decision boundary is a \\(2-1=1\\)-dimensional subspace–i.e. a line.\nWhat if we think a curved decision boundary would be more appropriate? In that case, we need to define a score function that factors in the features in a nonlinear way.\nWe started by representing each point as a 2-vector of predictors \\(\\mathbf{x} = \\left(\\text{loan interest rate}, \\text{loan percent income}\\right)\\). Let’s now add a feature map \\(\\phi\\) that accepts this vector and adds three nonlinear functions of the predictors:\n\\[\n\\begin{aligned}\n    \\phi(\\mathbf{x}) =\n        \\left(\\begin{matrix}\n            \\text{loan interest rate} \\\\\n            \\text{loan percent income} \\\\\n            \\left(\\text{loan interest rate}\\right)^2 \\\\  \n            \\left(\\text{loan percent income}\\right)^2 \\\\\n            \\text{loan interest rate} \\times \\text{loan percent income}\n        \\end{matrix}\\right)\n\\end{aligned}\n\\]\nBecause the new features are order-2 polynomials in the predictors, this feature map is often called the quadratic feature map.\nWe’ll still use an inner product to compute our score but now the formula will be  \\[\n\\begin{aligned}\n    s_i = \\langle \\mathbf{w}, \\phi(\\mathbf{x}_i) \\rangle\\;.\n\\end{aligned}\n\\]In order for this formula to make sense, we now need \\(\\mathbf{w}\\in \\mathbb{R}^5\\).\nHere’s an implementation of a score function with quadratic features:\n\ndef quadratic_score(w, X0, X1):\n    return w[0]*X0 + w[1]*X1 + w[2]*X0**2 + w[3]*X1**2 + w[4]*X0*X1\n\nNow we can set a new vector of weights \\(\\mathbf{w}\\in \\mathbb{R}^5\\) and a threshold \\(t\\).\n\nw = np.array([0.01, 1, 0.0005, 0.6, 0.001])\nthreshold = 0.5\n\nOur classification now looks like this:\n\nfig, ax = plt.subplots(1, 1)\nplot_score(ax, quadratic_score, w, df)\nscatter_data(ax, df)\nplot_threshold(ax, quadratic_score, w,  df, threshold)\n\n\n\n\n\n\n\nFigure 3.2: quadratic score-based classification.\n\n\n\n\n\nHow accurate were we?\n\ndf[\"decision\"] = predict(quadratic_score, w, threshold, df)\n(df[\"decision\"] == df[\"loan_status\"]).mean()\n\n0.777\n\n\nOur nonlinear score function was very slightly more accurate than our linear score function on training data. A few things to keep in mind:\n\nPerformance on training data is not always a reliable indicator of performance on unseen data.\nAdding nonlinear features is one way of adding flexibility to a model, allowing that model to learn complicated, “wiggly” decision patterns. As we saw with the Palmer penguins case study, too much model flexibility can lead to worse predictive performance. We’ll regularly revisit the problem of balancing flexibility/features against predictive generalization throughout these notes.\n\n\nRecap\nSo, we looked at a simplified data set in which we were able to observe some features of each prospective borrower \\(i\\) in the form of a vector \\(\\mathbf{x}_i\\). We then computed a score for each borrower \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) and used a threshold to decide whether or not to make a loan: the loan is approved if \\(s_i \\leq t\\) for a chosen threshold \\(t\\). We can think of this as a decision-making model for the loan approval problem.\nIs that the end of the story? Of course not! There are many questions remaining.\n\nModel Evaluation: How do we actually measure whether our decision-making model is good or not? Is accuracy the right measure? Is computing accuracy on the training data reliable? How would the model perform on unseen data that wasn’t used to decide \\(\\mathbf{w}\\) or \\(t\\)? What other ways could we measure the performance of models?\nLegitimacy: Is it morally and politically appropriate to use algorithmic decision-making in the context of loan applications? What is the potential for disparate harm? What is the potential for contributing to the reinforcement of historically disparity? In what cases could algorithmic loan-making be appropriate in a democratic society? In what cases could it constitute a violation of personal political or moral rights?\nTask Choice: How was the data collected? Is it complete? Why did I choose a certain set of predictors and targets? Are my predictors and targets reliable measurements of what they claim to represent? Whose interests are served by the existence of a machine learning model that completes this task?\nAlgorithm Design: What algorithm was used to find the model (i.e. the separating line)? Is that algorithm guaranteed to converge? Will it converge quickly? Would a different algorithm find a better model? Or would it find a model that is equally good more quickly?\nVectorization: Instead of classifying points in a measurement space, how could I instead classify images, videos, or bodies of text?\n\nWe’ll discuss all of these questions – in approximately this order – later in these notes.",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Score-Based Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04-decision-theory.html",
    "href": "chapters/04-decision-theory.html",
    "title": "4  Decision Theory in Classification",
    "section": "",
    "text": "Last time…\n…we considered a prediction problem in which we observed \\(p\\) attributes of prospective borrower \\(i\\) in the form of a vector \\(\\mathbf{x}_i \\in \\mathbb{R}^p\\). We then formed a score for prospective borrower \\(i\\) using a weight vector \\(\\mathbf{w}\\in \\mathbb{R}^p\\) and an inner product:\n\\[\n\\begin{aligned}\n    s_i = \\langle \\mathbf{x}_i, \\mathbf{w}  \\rangle\\;.\n\\end{aligned}\n\\tag{4.1}\\]\nThen, we classified prospective borrowers into two categories based on a threshold \\(t \\in \\mathbb{R}\\):\nEquation 4.1 says that the score should be computed as a linear function of the features \\(\\mathbf{x}_i\\). Models with this property are called linear models and are fundamental in both classification and regression tasks.",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Theory in Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04-decision-theory.html#last-time",
    "href": "chapters/04-decision-theory.html#last-time",
    "title": "4  Decision Theory in Classification",
    "section": "",
    "text": "We also developed the ability to compute nonlinear scores by instead computing the score as \\(s_i = \\langle \\mathbf{w},\\phi(\\mathbf{x}_i) \\rangle\\), where \\(\\phi\\) was a feature map that computed nonlinear functions of the entries of \\(\\mathbf{x}_i\\). For reasons that we’ll learn about when we study the theory of machine learning, this is still called a linear model, due to the fact that the score is a linear function of the vector \\(\\mathbf{w}\\). In this set of notes, we’ll always assume that \\(\\mathbf{x}\\) has already had a feature map applied to it, so that we can just focus on the simpler form of Equation 4.1.\n\n\n\nBorrowers who receive a loan had the property \\(s_i \\leq t\\).\nBorrowers who do not receive a loan have the property \\(s_i &gt; t\\).\n\n\nIn this set of notes, we are going to focus on one of the many questions we might ask about this framework: how do we choose the threshold \\(t\\)?  As we’ll see, this is a surprisingly tricky question that depends heavily on context.We’ll study later how to find \\(\\mathbf{w}\\).",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Theory in Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04-decision-theory.html#lending-data-set",
    "href": "chapters/04-decision-theory.html#lending-data-set",
    "title": "4  Decision Theory in Classification",
    "section": "Lending Data Set",
    "text": "Lending Data Set\nTo illustrate our discussion, we are going to pull up the lending data set from the previous section.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/credit_risk_dataset.csv\"\n\n\n\ndf_all = pd.read_csv(url)\ndf = df_all[[\"loan_int_rate\", \"loan_percent_income\", \"loan_status\"]]\ndf = df.dropna()\n\nFollowing the usual paradigm in machine learning, we’re going to incorporate two elements which we previously saw when studying the Palmer penguins. First, we are going to hold off a part of our data set that we will not use for making any choices about how we design our decision algorithm. This held-off part of the data is called the test set. We’ll use it for a final evaluation of our model’s performance.\n\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size = 0.2, random_state = 123) # 20% test set\n\nNext, we’ll distinguish our predictor and target variables in each of the train and test sets.\n\nX_train = df_train[[\"loan_int_rate\", \"loan_percent_income\"]]\ny_train = df_train[\"loan_status\"]\n\nX_test = df_test[[\"loan_int_rate\", \"loan_percent_income\"]]\ny_test = df_test[\"loan_status\"]",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Theory in Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04-decision-theory.html#vectorized-computation-of-scores",
    "href": "chapters/04-decision-theory.html#vectorized-computation-of-scores",
    "title": "4  Decision Theory in Classification",
    "section": "Vectorized Computation of Scores",
    "text": "Vectorized Computation of Scores\nSuppose that we have a weight vector \\(\\mathbf{w}\\) and that we’d like to choose a threshold \\(t\\). To do this, we will compute all the scores on the training data and do some experiments. How should we compute training scores? As we know, the \\(i\\)th score is given by Equation 4.1. To compute scores for all \\(n\\) of our training points, we could write a loop like this: In our case, \\(n =\\) 23572, the number of rows in the training data.\n\ns = [] # vector of scores\nfor i in range(n):\n    s.append(compute_score(X[i], w))\n\nwhere X[i] is the ith data point \\(\\mathbf{x}_i\\) and compute_score is a function that computes the score according to Equation 4.1. However, there’s a better way to do this if we step back from code into math for a moment. If \\(\\mathbf{s} \\in \\mathbb{R}^n\\) is a vector whose \\(i\\)th entry is the score \\(s_i\\), then we have\n\\[\n\\begin{aligned}\n    \\mathbf{s} = \\left(\n        \\begin{matrix}\n            \\langle \\mathbf{x}_1, \\mathbf{w} \\rangle \\\\\n            \\langle \\mathbf{x}_2, \\mathbf{w} \\rangle \\\\\n            \\vdots \\\\\n            \\langle \\mathbf{x}_n, \\mathbf{w} \\rangle\n        \\end{matrix}\n        \\right) = \\mathbf{X}\\mathbf{w}\\;,\n\\end{aligned}\n\\]\nwhere we have defined the predictor matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\)\n\\[\n\\begin{aligned}\n    \\mathbf{X} = \\left[\n        \\begin{matrix}\n            - \\mathbf{x}_1 -  \\\\\n            -\\mathbf{x}_2-  \\\\\n            \\vdots \\\\\n            -\\mathbf{x}_n -\n        \\end{matrix}\n        \\right] =\n        \\left[\n        \\begin{matrix}\n            x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n            x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n            \\vdots \\\\\n            x_{n1} & x_{n2} & \\cdots & x_{np}\n        \\end{matrix}\n        \\right]\\;.\n\\end{aligned}\n\\]\nThis is good news because it simplifies our life both mathematically and in code: the Numpy package supplies very fast matrix multiplication:\n\ndef linear_score(X, w):\n    return X@w\n\nNow, given \\(\\mathbf{w}\\), we can compute all the scores at once.\n\nw = np.array([0.01, 1.0])\ns = linear_score(X_train, w)\n\nHere is a histogram of the scores we just computed:\n\nhist = plt.hist(s)\nlabs = plt.gca().set(xlabel = r\"Score $s$\", ylabel = \"Frequency\")",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Theory in Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04-decision-theory.html#types-of-error",
    "href": "chapters/04-decision-theory.html#types-of-error",
    "title": "4  Decision Theory in Classification",
    "section": "Types of Error",
    "text": "Types of Error\nNow that we have the scores, we can easily simulate decision-making with a given threshold. For example, the proportion predicted to default on their loan with a given threshold \\(t\\) can be computed like this:\n\nt = 0.4\npreds = s &gt;= t\npreds.mean()\n\n0.15386899711522145\n\n\nSo, how should we choose the threshold \\(t\\)? One possibility would be to try to choose the threshold in a way that maximizes the training accuracy, the number of times that the prediction agrees with the actual outcome (repaid or default) on the training data. Here’s an example of a quick grid search:\n\nfor t in np.linspace(0, 1, 11):\n    y_pred = s &gt;= t\n    acc = (y_pred == y_train).mean()\n    print(f\"A threshold of {t:.1f} gives an accuracy of {acc:.2f}.\")\n\nA threshold of 0.0 gives an accuracy of 0.22.\nA threshold of 0.1 gives an accuracy of 0.23.\nA threshold of 0.2 gives an accuracy of 0.46.\nA threshold of 0.3 gives an accuracy of 0.71.\nA threshold of 0.4 gives an accuracy of 0.82.\nA threshold of 0.5 gives an accuracy of 0.80.\nA threshold of 0.6 gives an accuracy of 0.79.\nA threshold of 0.7 gives an accuracy of 0.78.\nA threshold of 0.8 gives an accuracy of 0.78.\nA threshold of 0.9 gives an accuracy of 0.78.\nA threshold of 1.0 gives an accuracy of 0.78.\n\n\nHowever, accuracy is not always the most relevant measure. For example, Field and Stream estimates that there are, globally, approximately 70 unprovoked shark attacks each year. Since the population of the world is currently around \\(8.1\\times 10^9\\) people, the average probability that a specific individual will suffer an unprovoked shark attack in a year is approximately \\(70 / (8.1 \\times 10^9) \\approx 8.6 \\times 10^{-9}\\). So, if we created a shark attack predictor which always predicted “no shark attack,” our model would be correct approximately 99.999999% of the time. However, this model wouldn’t be very useful, and wouldn’t have anything to tell us about the activities that increase or reduce the risk of experience an attack.\nA second reason we may wish to measure something other than accuracy has to do with asymmetrical costs of error. If we incorrectly predict that an individual will suffer a shark attack but no attack occurs, this is not that big a problem. Yes, we were wrong, but no one got hurt. In contrast, if we incorrectly predict that an individual will not suffer a shark attach, then this is a big problem which potentially involves grievous bodily injury, death, trauma, legal liability, etc. So, in designing our predictor, we might want to prioritizing avoiding the second kind of error, even if that leads us to make more of the first kind of error.\nWhat are the types of error? For a binary outcome with a binary predictor, there are four possibilities:\n\n\n\nTable 4.1: Types of correct classifications and errors in a binary classification problem.\n\n\n\n\n\n\nAbbreviation\nTrue Outcome\nPredicted Outcome\n\n\n\n\nTrue positive\nTP\n1\n1\n\n\nFalse negative\nFN\n1\n0\n\n\nFalse positive\nFP\n0\n1\n\n\nTrue negative\nTN\n0\n0\n\n\n\n\n\n\nGiven a vector of true outcomes \\(\\mathbf{y}\\) and a vector of predictions \\(\\hat{\\mathbf{y}}\\), we can calculate frequencies of each outcome. For example, here are the false positives associated with a given threshold value:\n\nt = 0.5\ny_pred = s &gt;= t \n\n# number where outcome == 0 and prediction == 1\n((y_train == 0)*(y_pred == 1)).sum()\n\n286\n\n\nIn practice, it’s more convenient to compute all the error rates at once using the confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_train, y_pred)\n\narray([[18062,   286],\n       [ 4319,   905]])\n\n\nThe layout of the confusion matrix is:\ntrue negative,  false positive \nfalse negative, true positive\nIt is common to normalize these counts into rates:\n\n\n\n\n\n\n\n\n\n\nAbbreviation\nFormula\n\n\n\n\n\nTrue negative rate\nTNR\n\\(\\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}}\\)\n\n\n\nFalse positive rate\nFPR\n\\(\\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}}\\)\n\n\n\nFalse negative rate\nFNR\n\\(\\frac{\\mathrm{FN}}{\\mathrm{TP} + \\mathrm{FN}}\\)\n\n\n\nTrue positive rate\nTPR\n\\(\\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}\\)\n\n\n\n\nIntuitively, the TPR measures the proportion of the time that the classifier predicts the correct (positive) label when the true outcome was positive. Similarly, the FPR measures the proportion of the time that the classifier predicts the incorrect (positive) label when the true outcome was negative. Because \\(\\mathrm{TPR} = 1 - \\mathrm{FNR}\\) and \\(\\mathrm{FPR} = 1 - \\mathrm{TNR}\\), folks usually only bother remembering and using \\(\\mathrm{TPR}\\) and \\(\\mathrm{FNR}\\).\nRather than computing these by hand, Scikit-learn offers a handy argument to confusion_matrix for computing these automatically and simultaneously:\n\nconfusion_matrix(y_train, y_pred, normalize = \"true\")\n\narray([[0.984, 0.016],\n       [0.827, 0.173]])\n\n\nLet’s do a quick check against the FPR using manual vectorized code. Cases where y_pred == 1 correspond to positive predictions, while cases where y_train == 0 correspond to true negative outcomes.\n\n# agrees with the top right corner of the normalized confusion matrix\n((y_pred == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n\n0.015587529976019185\n\n\n\nThe ROC Curve\nFor any given value of the threshold \\(t\\), we can compute the TPR and FPR. We can think of this process as defining a parametrized function, a curve in TPR-FPR space. This curve is the ROC curve ROC stands for “receiver operating characteristic,” a term that reflects the origin of the curve in detection of objects by radar.\nTo compute an ROC curve, we simply need to compute the TPR and FPR for many different values of the threshold \\(t\\) and plot them.\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\n\nnum_thresholds = 101\n\nFPR = np.zeros(num_thresholds)\nTPR = np.zeros(num_thresholds)\nT = np.linspace(s.min()-0.1, s.max()+0.1, num_thresholds)\ns    = linear_score(X_train, w)\n\nfor i in range(num_thresholds):\n    t = T[i]\n    preds    = s &gt;= t\n    FPR[i]   = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR[i]   = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\nax.plot(FPR, TPR, color = \"black\")\nax.plot([0,1], [0,1], linestyle=\"--\", color = \"grey\")\nax.set_aspect('equal')\n\nlabs = ax.set(xlabel = \"False Positive Rate\", ylabel = \"True Positive Rate\", title = \"ROC Curve\")\n\n\n\n\n\n\n\n\nWe can think of the ROC curve as a description of all the possible tradeoffs between the TPR and FPR that are achievable for a given score as we vary the threshold \\(t\\). For example, the curve tells us that if we are willing to tolerate a false positive rate of 0.40, then the best true positive rate we can achieve is approximately 0.77.\nROC curves are often used as a measure of the ability of a score function to classify data into two groups. Curves that bend farther towards the upper left corner of the plot are generally viewed as more effective classifiers. The area under the curve (AUC) is sometimes used as a single quantitative measure describing the classification quality.\n\n\nCost of Errors and Optimal Thresholding\nHow do we choose the tradeoff that works best for us? To answer this kind of question, we need to reflect back on the purpose for which we are building a classifier. According to Table 4.1, there are two ways to be correct (true positive, true negative) and two ways to make an error (false positive, false negative). In order to choose an appropriate tradeoff, we need to think about the benefit of being right in relation to the cost of being wrong.\nA logical way for a bank to approach this problem would be from the perspective of profit-maximization. In the lending business, a bank can make money when loans are fully repaid with interest, but lose money (usually much more) when an individual defaults on the loan. To keep the problem simple, suppose that the bank gains $1 every time they make a loan which is successfully paid back, and that the bank loses $2 every time they make a loan which ends in default. The first scenario happens when the bank makes a true positive identification, while the second case happens when the bank makes a false negative classification.  For a given threshold, the expected gain for the bank when making a loan is thenRemember that the “positive” outcome in this data set is default.\n\\[\n\\begin{aligned}\n    \\mathbb{E}[\\mathrm{gain}] = 1\\times \\text{TN} - 2\\times \\text{FN}\\;.\n\\end{aligned}\n\\]\nLet’s plot the expected gain as a function of the threshold:\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\ncost_of_FN = -2.0\ngain_of_TN = 1.0\n\ngain =  gain_of_TN*TNR  + cost_of_FN*FNR \n\nplt.plot(T, gain)\nplt.gca().set(ylim = (-0.2, 0.2), xlim = (0, 0.5))\nlabs = plt.gca().set(xlabel = r\"Threshold $t$\", ylabel = \"Expected profit per loan\")\n\n\n\n\n\n\n\n\nFor these costs, we observe that the bank can make a small expected profit (roughly 17 cents per loan) by using the given score function with threshold of roughly \\(t \\approx 0.21\\). Note that this is very different from the value of the thresold \\(t \\approx 0.4\\) which maximized the unweighted accuracy of the predictor.\nAt this stage, we could go on to estimate the profit gained by using this predictor and threshold on the test data set instead of the training data set. The code below simply consolidates the many steps that we have walked through in these notes, applied to the test data.\n\nt = 0.21\n\n# compute the scores\ns     = linear_score(X_test, w)\npreds = s &gt;= t\n\n# compute error rates\nFPR   = ((preds == 1) & (y_test == 0)).sum() / (y_test == 0).sum()\nTPR   = ((preds == 1) & (y_test == 1)).sum() / (y_test == 1).sum()\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\n# compute the expected gain\ngain = gain_of_TN*TNR  + cost_of_FN*FNR \ngain\n\n0.17641653321131712\n\n\nOur performance on the test data is very slightly worse than our performance on the training data, which is to be expected.",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Theory in Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04-decision-theory.html#recap",
    "href": "chapters/04-decision-theory.html#recap",
    "title": "4  Decision Theory in Classification",
    "section": "Recap",
    "text": "Recap\nIn these notes, we studied a simple question: given a score \\(s_i = \\langle \\mathbf{x}_i, \\mathbf{w}\\rangle\\), how should we convert that score into a yes/no decision? We found that adjusting the threshold can have major consequences for the accuracy of the resulting classification algorithm, but also that pure accuracy may not be the most relevant metric to measure or optimize. We computed the ROC curve of the score, which is a visual indicator of the overall ability of the score function to balance the false positive rate against the true positive rate. Finally, we explored the possible tradeoffs between different kinds of errors by considering a simplified scenario in which different kinds of errors have different costs associated with them. We found that the threshold that optimizes expected gain under this setting can be very different from the threshold that optimizes unweighted accuracy.",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Theory in Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04-decision-theory.html#who-sets-the-cost-who-pays-the-cost",
    "href": "chapters/04-decision-theory.html#who-sets-the-cost-who-pays-the-cost",
    "title": "4  Decision Theory in Classification",
    "section": "Who Sets The Cost? Who Pays the Cost?",
    "text": "Who Sets The Cost? Who Pays the Cost?\nIn our analysis above, we assumed a simple optimization objective: the bank is going to maximize its net profit. In formulating this objective, we made assumptions about the costs of different outcomes – to the bank. It’s important to note that the costs of errors to the bank may look very different from the costs of those errors to individuals. For example, if the bank’s prediction system recommends that an individual be denied a loan and the bank acts on this recommendation, then the bank pays no cost. On the other hand, the individual may experience major costs, depending on the purpose for which the loan was requested.\nThis data set includes a coarse description of the purpose of each loan:\n\ndf_all.groupby(\"loan_intent\").size()\n\nloan_intent\nDEBTCONSOLIDATION    5212\nEDUCATION            6453\nHOMEIMPROVEMENT      3605\nMEDICAL              6071\nPERSONAL             5521\nVENTURE              5719\ndtype: int64\n\n\nWhat are the costs of being denied access to borrowed funds to pursue education? What about for medical care?\nIt is of fundamental importance to remember that machine learning systems are embedded in social context; that they are generally developed and implemented by people and organizations that occupy positions of power; and that the costs of these systems are often unequally shared by the people they impact. We will discuss these considerations in much greater detail soon.",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Theory in Classification</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html",
    "href": "chapters/10-compas.html",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Data Preparation\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nsns.set_style(\"whitegrid\")\nnp.set_printoptions(precision = 3)\npd.set_option('display.precision', 3)\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/compas/compas.csv\"\ncompas = pd.read_csv(url)\nFor today we are only going to consider a subset of columns.\ncols = [\"sex\", \"race\", \"decile_score\", \"two_year_recid\"]\ncompas = compas[cols]\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\nis_white = compas[\"race\"] == \"Caucasian\"\nis_black = compas[\"race\"] == \"African-American\"\n\ncompas = compas[is_white | is_black]\ncompas = compas.copy()\nOur data now looks like this:\ncompas.head()\n\n\n\n\n\n\n\n\n\nsex\nrace\ndecile_score\ntwo_year_recid\n\n\n\n\n1\nMale\nAfrican-American\n3\n1\n\n\n2\nMale\nAfrican-American\n4\n1\n\n\n3\nMale\nAfrican-American\n8\n0\n\n\n6\nMale\nCaucasian\n6\n1\n\n\n8\nFemale\nCaucasian\n1\n0",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html#data-preparation",
    "href": "chapters/10-compas.html#data-preparation",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.This data set was obtained by Angwin et al. (2022) through a public records request. The data comprises two years worth of COMPAS scoring in Broward County, Florida.",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html#preliminary-explorations",
    "href": "chapters/10-compas.html#preliminary-explorations",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Preliminary Explorations",
    "text": "Preliminary Explorations\nLet’s do some quick exploration of our data. How many defendants are present in this data of each sex?\n\ncompas.groupby(\"sex\").size()\n\nsex\nFemale    1219\nMale      4931\ndtype: int64\n\n\nWhat about race?\n\ncompas.groupby(\"race\").size()\n\nrace\nAfrican-American    3696\nCaucasian           2454\ndtype: int64\n\n\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as being produced by computing a score like \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) for each defendant \\(i\\), and then dividing these into the lowest 10% (decile score 1), the next 10% (decile score 2), the next 10% (decile score 3) and so on.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\np = sns.barplot(data = counts, \n                x = \"decile_score\", \n                y = \"n\", \n                hue = \"race\", \n                palette = \"BuPu\", \n                saturation = 0.5)\n\n\n\n\n\n\n\n\nYou may notice that the number of white defendants who receive a given decile score tends to decrease as the score increases, whereas the number of Black defendants remains relatively constant.\nLet’s also take a look at the recidivism rate in the data:\n\ncompas[\"two_year_recid\"].mean()\n\n0.4661788617886179\n\n\nSo, in these data, approximately 47% of all defendants went on to be charged of another crime within the next two years. This is sometimes called the prevalence of the outcome. Although this is not a “good” outcome, it is labeled 1 in the target data and so we refer to this as the “positive” outcome. Prevalence without further specification usually refers to prevalence of the positive outcome.\nThe base rate of prediction accuracy in this problem is 53%: if we always guessed that the defendant was not arrested within two years, we would be right 53% of the time.\nWe can also compute the prevalence broken down by race of the defendant:\n\ncompas.groupby(\"race\")[\"two_year_recid\"].mean()\n\nrace\nAfrican-American    0.514\nCaucasian           0.394\nName: two_year_recid, dtype: float64\n\n\n\n\nWhen interpreting these different prevalences, it is important to remember that\n\nRace is itself a socially-constructed system of human categorization invented by humans with political and economic motives to describe other humans as property (Bonilla-Silva 2018).\n\nThe relation between arrest and actual criminal offense can display racial bias, with effects varying by geography (Fogliato et al. 2021).\nDecisions about which behaviors are criminal are contingent political decisions which have, historically, fallen hardest on Black Americans (Yusef and Yusef 2017).\n\nThe prevalences between the two groups are substantially different. This difference will have major consequences later on for the possibility of different kinds of fairness in classifiers.\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of Angwin et al. (2022), we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\n\ncompas[\"predicted_high_risk\"] = (compas[\"decile_score\"] &gt; 4)\n\nNow that we’ve done that, we can ask: how likely are Black and white defendants to receive positive predictions in this data?\n\ncompas.groupby(\"race\")[\"two_year_recid\", \"predicted_high_risk\"].mean()\n\n/var/folders/xn/wvbwvw0d6dx46h9_2bkrknnw0000gn/T/ipykernel_16595/3539224628.py:1: FutureWarning:\n\nIndexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n\n\n\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\nrace\n\n\n\n\n\n\nAfrican-American\n0.514\n0.588\n\n\nCaucasian\n0.394\n0.348\n\n\n\n\n\n\n\n\nBlack defendants are substantially more likely to receive a positive prediction than white defendants, and the disparity is larger than the observed prevalence of the positive outcome.\n\n\n\n\n\n\nFairness (Part 1)\n\n\n\nIs this fair? What is your gut telling you? Yes, no, possibly? What information would you need in order to make a judgment? What is the principle on which your judgment rests?",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html#the-propublica-findings",
    "href": "chapters/10-compas.html#the-propublica-findings",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "The ProPublica Findings",
    "text": "The ProPublica Findings\nLet’s now ask a few questions about the the predictive accuracy of this algorithm. First, how accurate it is it overall?\n\ncompas[\"correct_prediction\"] = (compas[\"predicted_high_risk\"] == compas[\"two_year_recid\"])\ncompas[\"correct_prediction\"].mean()\n\n0.6508943089430894\n\n\nRecall that the base rate in this problem is 53%, so our accuracy is somewhat better than random guessing.\nWhat about the accuracy on Black and white defendants separately?\n\ncompas.groupby([\"race\"])[\"correct_prediction\"].mean()\n\nrace\nAfrican-American    0.638\nCaucasian           0.670\nName: correct_prediction, dtype: float64\n\n\nThe overall accuracies for Black and white defendants are comparable, and both are somewhat higher than the base rate of 53%.\nWhat about the error rates? Here is a simple calculation which computes the false positive rate (FPR) in the first row and the true positive rate (TPR) on the bottom row:\n\ncompas.groupby([\"two_year_recid\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\n\n\n0\n0\n0.352\n\n\n1\n1\n0.654\n\n\n\n\n\n\n\n\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race:\n\ncompas.groupby([\"two_year_recid\", \"race\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\n\ntwo_year_recid\nrace\npredicted_high_risk\n\n\n\n\n0\n0\nAfrican-American\n0.448\n\n\n1\n0\nCaucasian\n0.235\n\n\n2\n1\nAfrican-American\n0.720\n\n\n3\n1\nCaucasian\n0.523\n\n\n\n\n\n\n\n\nThe false positive rate for Black defendants is much higher than the false positive rate for white defendants. This was the main finding of Angwin et al. (2022). The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to the idea of representational bias, in which algorithms learn and reproduce toxic stereotypes about certain groups of people.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. This is sometimes called allocative or distributional bias: bias in how resources or opportunities (in this case, freedom) are allocated or distributed between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well.\nWe can think about the argument of Angwin et al. (2022) as a two-step argument:\n\n\nThe COMPAS algorithm has disparate error rates by race.\nTherefore, the COMPAS algorithm is unjustly biased with respect to race.\n\n\nThis argument implicitly equates equality of error rates with lack of bias.\n\n\n\n\n\n\nFairness (Part 2)\n\n\n\n\nSuppose that we developed an alternative algorithm in which the false positive rates were equal, but there were still more positive predictions for Black defendants overall. Would that be enough to ensure fairness?\nSuppose that we developed an alternative prediction algorithm in which the rate of positive prediction was the same across racial groups, but the false positive rates were different. Would that be to ensure fairness?",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html#the-rebuttal",
    "href": "chapters/10-compas.html#the-rebuttal",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "The Rebuttal",
    "text": "The Rebuttal\nAngwin et al. (2022) kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. In particular, Northpointe, the company that developed COMPAS, issued a report Flores, Bechtel, and Lowenkamp (2016) in which they argued that their algorithm was fair. Their argument is based on an idea of fairness which is sometimes called sufficiency Corbett-Davies et al. (2017).\nHere’s the intuition expressed by sufficiency. Imagine that you and your friend both received an A- in Data Structures. Suppose, however, that the instructor says different things to each of you:\n\nTo you, the instructor says: “You did fine in this class, but I don’t think that you are prepared to take Computer Architecture. I gave you a higher grade than I would normally because you wear cool hats in class.”\nTo your friend, the instructor says: “*You did fine in this class and I think you are prepared to take Computer Architecture. Some students got a bump in their grade because they are cool-hat-wearers, but you didn’t get that benefit.”\n\nFeels unfair, right? The instructor is saying that:\n\nWhat a grade means for you in terms of your future success depends on your identity group.\n\n\n\n\n\n\n\nSuppose that you heard this, but instead of cool hats it was because you are a member of an identity group that “needs some help” in order to achieve equitable representation in the CS major. How would you feel? Would that feel fair to you?\n\n\n\nWe’ll formally define sufficiency in a future lecture. For now, let’s use an informal definition:\n\nSufficiency means that a positive prediction means the same thing for future outcomes for each racial group.\n\nTo operationalize this idea, we are looking for the rate of re-arrest to be the same between (a) Black defendants who received a positive prediction and (b) white defendants who received a positive prediction.\nLet’s check this:\n\ncompas.groupby([\"predicted_high_risk\", \"race\"])[\"two_year_recid\"].mean().reset_index()\n\n\n\n\n\n\n\n\n\npredicted_high_risk\nrace\ntwo_year_recid\n\n\n\n\n0\nFalse\nAfrican-American\n0.350\n\n\n1\nFalse\nCaucasian\n0.288\n\n\n2\nTrue\nAfrican-American\n0.630\n\n\n3\nTrue\nCaucasian\n0.591\n\n\n\n\n\n\n\n\nThe rates of rearrest are relatively similar between groups when controlling for the predictions they collectively received. Formal statistical hypothesis tests are typically used to determine whether this difference is sufficiently “real” to warrant correction. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be relatively close to satisfying sufficiency.\nIndeed, in a rejoinder article published by affiliates of the company Northpointe which produced COMPAS, the fact that COMPAS satisfies sufficiency is one of the primary arguments (Flores, Bechtel, and Lowenkamp 2016).",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html#recap",
    "href": "chapters/10-compas.html#recap",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Recap",
    "text": "Recap\nIn these notes, we replicated the data analysis of Angwin et al. (2022), finding that the COMPAS algorithm has disparate error rates between Black and white defendants. We introduced the idea that fairness actually has several different facets in our moral intuitions, and found that the COMPAS algorithm satisfies one of them (sufficiency: equal scores mean the same thing regardless of your group membership) but not the others (equal prediction rates and equal error rates).",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html#some-questions-moving-forward",
    "href": "chapters/10-compas.html#some-questions-moving-forward",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Some Questions Moving Forward",
    "text": "Some Questions Moving Forward\n\nCan we have it all? Could we modify the COMPAS algorithm in such a way that it satisfies all the ideas of fairness that we discussed above? Could we then call it “fair” or “unbiased?”\nAre there other ways to define fairness? Which ones are most compelling to us? Does the right idea of fairness depend on the context in which we apply it?\nHow did this happen? The COMPAS algorithm was never trained on race data about the defendant. How did it happen that this algorithm nevertheless made recommendations at different rates across groups?\nIs automated decision-making legitimate in this setting? Can it be legitimate (just, fair) to use an automated decision-system for making recommendations about parole and sentencing decisions at all? What safeguards and forms of recourse are necessary for the legitimate use of automated decision-making in criminal justice?\nWhat are the systemic impacts? Disparate sentencing decisions can have downstream impacts on communities and institutions. How could application of the COMPAS algorithm exacerbate systemic inequalities?",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html#references",
    "href": "chapters/10-compas.html#references",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "References",
    "text": "References\n\n\n\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2022. “Machine Bias.” In Ethics of Data and Analytics, 254–64. Auerbach Publications.\n\n\nBonilla-Silva, Eduardo. 2018. Racism Without Racists: Color-Blind Racism and the Persistence of Racial Inequality in America. Fifth edition. Lanham: Rowman & Littlefield.\n\n\nCorbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. “Algorithmic Decision Making and the Cost of Fairness.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 797–806. Halifax NS Canada: ACM. https://doi.org/10.1145/3097983.3098095.\n\n\nFlores, Anthony W, Kristin Bechtel, and Christopher T Lowenkamp. 2016. “False Positives, False Negatives, and False Analyses: A Rejoinder to Machine Bias: There’s Software Used Across the Country to Predict Future Criminals. And It’s Biased Against Blacks.” Federal Probation 80: 38.\n\n\nFogliato, Riccardo, Alice Xiang, Zachary Lipton, Daniel Nagin, and Alexandra Chouldechova. 2021. “On the Validity of Arrest as a Proxy for Offense: Race and the Likelihood of Arrest for Violent Crimes.” In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 100–111. Virtual Event USA: ACM. https://doi.org/10.1145/3461702.3462538.\n\n\nYusef, Kideste Wilder, and Tseleq Yusef. 2017. “Criminalizing Race, Racializing Crime: Assessing the Discipline of Criminology Through a Historical Lens.” In The Handbook of the History and Philosophy of Criminology, edited by Ruth Ann Triplett, 1st ed., 272–88. Wiley. https://doi.org/10.1002/9781119011385.ch16.",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/12-statistical-fairness.html",
    "href": "chapters/12-statistical-fairness.html",
    "title": "6  Statistical Definitions of Fairness in Decision-Making",
    "section": "",
    "text": "Three Statistical Definitions of Fairness\nLast time, we introduced the idea that fairness in decision-making could be defined formally, and models could be audited to determine the extent to which those models conformed to a given definition. In this section, we’ll discuss some of the definitions in Chapter 3 of Barocas, Hardt, and Narayanan (2023) and implement Python functions to measure the extent to which the COMPAS risk score conforms to those definitions.\nTo line ourselves up with the notation of Barocas, Hardt, and Narayanan (2023), let’s define the following random variables: Let \\(A\\) be a random variable that describes the group membership of an individual. Let \\(Y\\) be the outcome we want to predict. Let \\(R\\) be the value of our risk score. Let \\(\\hat{Y}\\) be our model’s prediction about whether \\(Y\\) occurs.\nIn the case of COMPAS:",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Definitions of Fairness in Decision-Making</span>"
    ]
  },
  {
    "objectID": "chapters/12-statistical-fairness.html#three-statistical-definitions-of-fairness",
    "href": "chapters/12-statistical-fairness.html#three-statistical-definitions-of-fairness",
    "title": "6  Statistical Definitions of Fairness in Decision-Making",
    "section": "",
    "text": "\\(A\\) is the race of the individual, with possible values \\(A = a\\) and \\(A = b\\).\n\\(Y = 1\\) if the individual was arrested within two years after release, and \\(Y = 0\\) if not.\n\\(R\\) is the decile risk score.\n\\(\\hat{Y} = 1\\) if \\(R \\geq 4\\) and \\(\\hat{Y} = 0\\) otherwise.\n\n\nStatistical Independence\nHere’s our first concept of fairness: independence. For our present purposes, we focus on the definition of independence for binary classifiers as given by Barocas, Hardt, and Narayanan (2023).\n\n\n\n\n\n\n\nDefinition 6.1 (Statistical Independence For Binary Classifiers)  The model predictions \\(\\hat{Y}\\) satisfy statistical independence if \\(\\mathbb{P}(\\hat{Y} = 1 | A = a) = {P}(\\hat{Y} = 1 | A = b)\\).\n\n\n\n\nRecall that \\(\\mathbb{P}(Y = 1|A = a)\\) is the probability that \\(Y = 1\\) given that \\(A=a\\). It can be computed using the formula \\(\\mathbb{P}(Y = 1|A = a) = \\frac{\\mathbb{P}(Y = 1, A = a)}{\\mathbb{P}(A = a)}\\).Colloquially, Definition 6.1 says that the probability of a positive prediction \\(\\hat{Y} = 1\\) does not depend on the group membership \\(A\\). In the COMPAS data, independence would require that the probability of the model predicting that an individual will be arrested within two years be the same for Black and white defendants.\nLet’s write a Python function to empirically check independence that will accept a data frame df and three additional arguments:\nFor independence, we don’t actually need the target column, but this approach will let us keep a consistent API for our more complicated implementations below.\n\ngroup_col, the name of the column describing group memberships.\ntarget, the name of the column holding the binary outcomes.\npred, the name of the column holding the predicted binary outcomes.\n\n\ndef test_independence(df, group_col, target, pred):\n    return df.groupby(group_col)[pred].aggregate([np.mean, len])\n\nLet’s run our function to check for independence:\n\ntest_independence(compas, \"race\", \"two_year_recid\", \"predicted_high_risk\")\n\n\n\n\n\n\n\n\n\nmean\nlen\n\n\nrace\n\n\n\n\n\n\nAfrican-American\n0.588\n3696\n\n\nCaucasian\n0.348\n2454\n\n\n\n\n\n\n\n\nThe mean column gives the proportion of the time in which the predictor \\(\\hat{Y}\\) had value equal to 1, for each of the two groups. This is an empirical estimate of the probability \\(\\mathbb{P}(\\hat{Y} = 1 | A = a)\\). We can see that the two proportions are substantially different between groups, strongly suggesting that this model violates the independence criterion. Formally, statistical tests beyond the scope of this course would be needed to reject the hypothesis that the two proportions are different. In this case, you can take my word for it that the relevant test provides strong support for rejecting the null.\nAs discussed in Barocas, Hardt, and Narayanan (2023), independence is a very strong expression of the idea that predictions, and therefore automated decisions, should be the same in aggregate across all groups present in the data. This idea sometimes accompanies another idea, that all groups are equally worthy, meritorious, or deserving of a given decision outcome.\n\n\nError-Rate Balance\n The primary finding of Angwin et al. (2022) was, famously, that the COMPAS algorithm makes very different kinds of errors on Black and white defendants.This definition can be generalized from binary classifiers to score functions via the concept of separation, which is discussed in Barocas, Hardt, and Narayanan (2023).\n\n\n\n\n\n\n\nDefinition 6.2 (Error Rate Balance for Binary Classifiers) The model predictions \\(\\hat{Y}\\) satisfy error-rate balance if the following conditions both hold:\n\\[\n\\begin{aligned}\n    \\mathbb{P}(\\hat{Y} = 1 | Y = 1, A = a) &= \\mathbb{P}(\\hat{Y} =1  | Y = 1, A = b) & \\text{(balanced true positives)} \\\\\n    \\mathbb{P}(\\hat{Y} = 1 | Y = 0, A = a) &= \\mathbb{P}(\\hat{Y} =1  | Y = 0, A = b)\\;. & \\text{(balanced false positives)}\n\\end{aligned}\n\\]\n\n\n\n\nError rate balance requires that the true positive rate and false positive rates be equal on the two groups. Given some data in which we have \\(\\mathrm{TP}\\) instances of true positives, \\(\\mathrm{FP}\\) instances of false positives, \\(\\mathrm{TN}\\) instances of true negatives, and \\(\\mathrm{FN}\\) instances of false negatives, we can estimate the TPR and FPR via the formulas\n\\[\n\\begin{aligned}\n    \\mathrm{TPR} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} \\\\  \n    \\mathrm{FPR} &= \\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}}\\;.\n\\end{aligned}\n\\]\nLet’s write another function with the same API to give a summary of error rates between two groups using these formulas. As we know, it’s pretty convenient to do this with confusion matrices. It’s not much more difficult to do it “by hand” using vectorized Pandas computations:\n\ndef test_error_rate_balance(df, group_col, target, pred):\n    return df.groupby([group_col, target])[pred].mean().reset_index()\n\nWe can use this function to do an empirical test for error rate balance:\n\ntest_error_rate_balance(compas, \"race\", \"two_year_recid\", \"predicted_high_risk\")\n\n\n\n\n\n\n\n\n\nrace\ntwo_year_recid\npredicted_high_risk\n\n\n\n\n0\nAfrican-American\n0\n0.448\n\n\n1\nAfrican-American\n1\n0.720\n\n\n2\nCaucasian\n0\n0.235\n\n\n3\nCaucasian\n1\n0.523\n\n\n\n\n\n\n\n\n The false positive rates are in the rows in which two_year_recid == 0, and the true positive rates are in the rows in which two_year_recid == 1.As before, before concluding that the COMPAS algorithm violates error rate balance as in Definition 6.2, it is technically necessary to perform a statistical test to reject the null hypothesis that the true population error rates are the same.\n\n\nSufficiency\nFinally, as we mentioned last time, the analysis of Angwin et al. (2022) received heavy pushback from Flores, Bechtel, and Lowenkamp (2016) and others, who argued that error rate balance wasn’t really the right thing to measure. Instead, we should check sufficiency, which we’ll define here for binary classifiers:\n\n\n\n\n\n\n\nDefinition 6.3 (Sufficiency) Model predictions \\(\\hat{Y}\\) satisfy sufficiency if the following two conditions hold: \\[\n\\begin{aligned}\n    \\mathbb{P}(Y = 1 | \\hat{Y} = 1, A = a) &= \\mathbb{P}(Y = 1 | \\hat{Y} = 1, A = b) \\\\\n    \\mathbb{P}(Y = 0 | \\hat{Y} = 0, A = a) &= \\mathbb{P}(Y = 0 | \\hat{Y} = 0, A = b) \\;.\n\\end{aligned}\n\\]\n\n\n\n\nThe quantity \\(\\mathbb{P}(Y = 1 | \\hat{Y} = 1, A = a)\\) is sometimes called the positive predictive value (PPV) of \\(\\hat{Y}\\) for group \\(a\\). You can think of it as the “value” of a positive prediction: given that the prediction is positive (\\(\\hat{Y} = 1\\)) for a member of group \\(a\\), how likely is it that the prediction is accurate? Similarly, \\(\\mathbb{P}(Y = 0 | \\hat{Y} = 0, A = a)\\) is sometimes called the negative predictive value (NPV) of \\(\\hat{Y}\\) for group \\(a\\). So, the sufficiency criterion demands that the positive and negative predictive values be equal across groups.\nGiven some data in which we have \\(\\mathrm{TP}\\) instances of true positives, \\(\\mathrm{FP}\\) instances of false positives, \\(\\mathrm{TN}\\) instances of true negatives, and \\(\\mathrm{FN}\\) instances of false negatives, we can estimate the PPV and NPV via the formulas\n\\[\n\\begin{aligned}\n    \\mathrm{PPV} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}} \\\\  \n    \\mathrm{NPV} &= \\frac{\\mathrm{TN}}{\\mathrm{TN} + \\mathrm{FN}}\\;.\n\\end{aligned}\n\\]\nLet’s write a function to check for sufficiency in the COMPAS predictions. This function will compute the positive and negative predictive values by group:\n\ndef test_sufficiency(df, group_col, target, pred):\n    df_ = df.copy()\n    df_[\"correct\"] = df_[pred] == df_[target]\n    return df_.groupby([pred, group_col])[\"correct\"].mean().reset_index()\n\n\ntest_sufficiency(compas, \"race\", \"two_year_recid\", \"predicted_high_risk\")\n\n\n\n\n\n\n\n\n\npredicted_high_risk\nrace\ncorrect\n\n\n\n\n0\n0\nAfrican-American\n0.650\n\n\n1\n0\nCaucasian\n0.712\n\n\n2\n1\nAfrican-American\n0.630\n\n\n3\n1\nCaucasian\n0.591\n\n\n\n\n\n\n\n\nThe negative predictive values are in the rows in which predicted_high_risk == 0 and the positive predictive values are in the rows in which predicted_high_risk == 1. We observe that the negative predictive value is slightly higher for white defendants, while the positive predictive value is slightly higher for Black defendants. These differences, however, are much lower than the error rate disparity noted above.",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Definitions of Fairness in Decision-Making</span>"
    ]
  },
  {
    "objectID": "chapters/12-statistical-fairness.html#can-we-have-it-all",
    "href": "chapters/12-statistical-fairness.html#can-we-have-it-all",
    "title": "6  Statistical Definitions of Fairness in Decision-Making",
    "section": "Can We Have It All?",
    "text": "Can We Have It All?\nOk, well COMPAS isn’t an ideal algorithm by any means. But couldn’t we just define some more conceptions of fairness, pick the ones that we wanted to use, and then design an algorithm that satisfied all of them?\nSadly, no: we can’t even have error rate balance and sufficiency simultaneously.\n\n\n\n\n\n\n\nTheorem 6.1 (Incompatibility of Error Rate Balance and Sufficiency (Chouldechova 2017)) If the true rates \\(p_a\\) and \\(p_b\\) of positive outcomes in the groups \\(a\\) and \\(b\\) are not equal (\\(p_a \\neq p_b\\)), then there does not exist a model that produces predictions which satisfy both error rate balance and sufficiency.\n\n\n\n\n\nProof. Our big-picture approach is proof by contrapositive. We’ll show that if there were a model that satisfied error rate balance and sufficiency, then \\(p_a = p_b\\).\nLet’s briefly forget about group labels – we’ll reintroduce them in a moment.\nFirst, the prevalence of positive outcomes is the fraction of positive outcomes. There are \\(\\mathrm{TP} + \\mathrm{FN}\\) total positive outcomes, and \\(\\mathrm{TP} + \\mathrm{FP} + \\mathrm{TN} + \\mathrm{FN}\\) outcomes overal, so we can write the prevalence as\n\\[\n\\begin{aligned}\n    p = \\frac{\\mathrm{TP} + \\mathrm{FN}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{TN} + \\mathrm{FN}};.\n\\end{aligned}\n\\]\nFrom above, the true and false positive rates are:\n\\[\n\\begin{aligned}\n    \\mathrm{TPR} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} \\\\\n    \\mathrm{FPR} &= \\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}}\\;.  \n\\end{aligned}\n\\]\nThe PPV is: \\[\n\\begin{aligned}\n    \\mathrm{PPV} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}\\;.\n\\end{aligned}\n\\]\nOk, now it’s time to do some algebra. Let’s start with the \\(\\mathrm{TPR}\\) and see if we can find an equation that relates it to the \\(\\mathrm{FPR}\\). First, we’ll multiply by \\(\\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}}\\). If we do this and insert the definitions of these quantities, we’ll get\n\\[\n\\begin{aligned}\n    \\mathrm{TPR} \\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} \\frac{\\mathrm{FP}}{\\mathrm{TP} + \\mathrm{FP}} \\frac{\\mathrm{TP} + \\mathrm{FP}}{\\mathrm{TP}} \\\\\n    &= \\frac{\\mathrm{FP}}{\\mathrm{TP} + \\mathrm{FN}}\\;.\n\\end{aligned}\n\\]\nLet’s now also multiply by a factor of \\(\\frac{p}{1-p}\\):\n\\[\n\\begin{aligned}\n    \\mathrm{TPR} \\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}}\\frac{p}{1-p} &= \\frac{\\mathrm{FP}}{\\mathrm{TP} + \\mathrm{FN}}\\frac{p}{1-p} \\\\\n    &= \\frac{\\mathrm{FP}}{\\mathrm{TP} + \\mathrm{FN}} \\frac{\\mathrm{TP} + \\mathrm{FN}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{TN} + \\mathrm{FN}} \\frac{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{TN} + \\mathrm{FN}}{\\mathrm{FP} + \\mathrm{TN}} \\\\\n    &= \\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}} \\\\\n    &= \\mathrm{FPR}\\;.\n\\end{aligned}\n\\]\nSo, with some algebra, we have proven an equation for any classifier:\n\\[\n\\begin{aligned}\n    \\mathrm{TPR} \\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}}\\frac{p}{1-p} = \\mathrm{FPR}\\;.\n\\end{aligned}\n\\]\nIt’s convenient to rearrange this equation slightly:\n\\[\n\\begin{aligned}\n    \\frac{\\mathrm{TPR}}{\\mathrm{FPR}} \\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}} = \\frac{1-p}{p}\\;.\n\\end{aligned}\n\\]\nor\n\\[\n\\begin{aligned}\n    p = \\left(1 + \\frac{\\mathrm{TPR}}{\\mathrm{FPR}} \\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}}\\right)^{-1}\\;.\n\\end{aligned}\n\\tag{6.1}\\]\nNow suppose that I want to enforce error rate balance and sufficiency for two groups \\(a\\) and \\(b\\), where \\(p_a \\neq p_b\\). So, from error rate balance I am going to require that \\(\\mathrm{TPR}_a = \\mathrm{TPR}_b\\), \\(\\mathrm{FPR}_a = \\mathrm{FPR}_b\\), and from sufficiency I am going to enforce that \\(\\mathrm{PPV}_a = \\mathrm{PPV}_b\\). Now, however, we have a problem: by Equation 6.1, it must also be the case that \\(p_a = p_b\\). This contradicts our assumption from the theorem. We cannot mathematically satisfy both error rate balance and sufficiency. This completes the proof.\n\n\n\n\n\n\n\nDiscussion\n\n\n\nDo you feel that it is more important for a recidivism prediction algorithm like COMPAS to satisfy error rate balance or sufficiency? Why?",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Definitions of Fairness in Decision-Making</span>"
    ]
  },
  {
    "objectID": "chapters/12-statistical-fairness.html#fairness-context-and-legitimacy",
    "href": "chapters/12-statistical-fairness.html#fairness-context-and-legitimacy",
    "title": "6  Statistical Definitions of Fairness in Decision-Making",
    "section": "Fairness, Context, and Legitimacy",
    "text": "Fairness, Context, and Legitimacy\nThe proof above shows that, when the prevalences of positive outcomes differ between groups, we have no hope of being able to have both balanced error rates and sufficiency. In Chapter 3, Barocas, Hardt, and Narayanan (2023) give a few other examples of fairness definitions, as well as proofs that some of these definitions are incompatible with each other. We can’t just have it all – we have to choose.\nThe quantitative story of fairness in automated decision-making is not cut-and-dried – we need to make choices, which may be subject to politics. Let’s close this discussion with three increasingly difficult questions:\n\nWhat is the right definition of fairness by which to judge the operation of a decision-making algorithm?\nIs “fairness” even the right rubric for assessing the impact of a given algorithm?\nIs it legitimate to use automated decision-making at all for a given application context?\n\nWe’ll consider each of these questions soon.",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Definitions of Fairness in Decision-Making</span>"
    ]
  },
  {
    "objectID": "chapters/12-statistical-fairness.html#references",
    "href": "chapters/12-statistical-fairness.html#references",
    "title": "6  Statistical Definitions of Fairness in Decision-Making",
    "section": "References",
    "text": "References\n\n\n\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2022. “Machine Bias.” In Ethics of Data and Analytics, 254–64. Auerbach Publications.\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press. https://fairmlbook.org/pdf/fairmlbook.pdf.\n\n\nChouldechova, Alexandra. 2017. “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.” Big Data 5 (2): 153–63. https://doi.org/10.1089/big.2016.0047.\n\n\nFlores, Anthony W, Kristin Bechtel, and Christopher T Lowenkamp. 2016. “False Positives, False Negatives, and False Analyses: A Rejoinder to Machine Bias: There’s Software Used Across the Country to Predict Future Criminals. And It’s Biased Against Blacks.” Federal Probation 80: 38.",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Definitions of Fairness in Decision-Making</span>"
    ]
  },
  {
    "objectID": "chapters/20-perceptron.html",
    "href": "chapters/20-perceptron.html",
    "title": "7  Introduction to Classification: The Perceptron",
    "section": "",
    "text": "Our Data\nThe perceptron algorithm aims to find a rule for separating two distinct groups in some data. Here’s an example of some data on which we might aim to apply the perceptron:\nCode\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nFigure 7.1: 300 data points in the 2d plane, each of which has one of two labels.\nThere are \\(n = 300\\) points of data. Each data point \\(i\\) has three pieces of information associated with it:\nMore generally, supervised prediction problems with \\(n\\) data points and \\(k\\) features can be summarized in terms of a feature matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) and a target vector \\(\\mathbf{y} \\in \\mathbb{R}^n\\).",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Classification: The Perceptron</span>"
    ]
  },
  {
    "objectID": "chapters/20-perceptron.html#illustration",
    "href": "chapters/20-perceptron.html#illustration",
    "title": "7  Introduction to Classification: The Perceptron",
    "section": "Illustration",
    "text": "Illustration\nThe following figure illustrates the perceptron algorithm in action over several iterations.\n\n\nCode\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    i, local_loss = opt.step(X, y)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nFigure 7.3: Several iterations of the perceptron algorithm. In each panel, the dashed line is the hyperplane corresponding to the previous weight vector \\(\\mathbf{w}^{(t)}\\), while the solid line is the hyperplane for the updated weight vector \\(\\mathbf{w}^{t+1}\\). The empty circle is the point \\(i\\) used in the update; only iterations in which \\(i\\) was a mistake are shown. The loss is computed as in Equation 7.4.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Classification: The Perceptron</span>"
    ]
  },
  {
    "objectID": "chapters/20-perceptron.html#theory-of-the-perceptron-algorithm",
    "href": "chapters/20-perceptron.html#theory-of-the-perceptron-algorithm",
    "title": "7  Introduction to Classification: The Perceptron",
    "section": "Theory of the Perceptron Algorithm",
    "text": "Theory of the Perceptron Algorithm\nDoes the perceptron algorithm always improve our accuracy? Is it guaranteed to terminate? If it does terminate, is the result guaranteed to be a weight vector \\(\\mathbf{w}\\) that perfectly separates the two data classes?\n\nLocal Improvement on Data Point \\(i\\)\nLet’s first check that the perceptron update in Equation 7.6 actually improves the prediction on data point \\(i\\) if there is currently a mistake on that point (i.e. if \\(s^{(t)}_i y_i &lt; 0\\)). We can do this by computing the new \\(s_i^{(t+1)}\\). Remember, what we want is for the sign of \\(s_i^{(t+1)}\\) to match \\(y_i\\).\n\\[\n\\begin{align}\ns_i^{(t+1)} &= \\langle \\mathbf{w}^{(t+1)}, \\mathbf{x}_i\\rangle  &\\text{(definition of $s_i^{(t+1)}$)}\\\\\n               &= \\langle \\mathbf{w}^{(t)} + y_i\\mathbf{x}_i, \\mathbf{x}_i\\rangle &\\text{(perceptron update)} \\\\\n               &= \\langle \\mathbf{w}^{(t)},\\mathbf{x}_i\\rangle + y_i\\langle \\mathbf{x}_i, \\mathbf{x}_i\\rangle &\\text{(linearity of $\\langle \\cdot\\rangle$)}\\\\\n               &= s_i^{(t)} + y_i \\lVert \\mathbf{x}_i\\rVert_2^2\\;. &\\text{(definition of $s_i^{(t)}$ and $\\lVert \\mathbf{x}_i\\rVert$)}\n\\end{align}\n\\]\n Since \\(\\lVert\\mathbf{x}_i\\rVert &gt; 0\\), we conclude that \\(s_i\\) always moves in the right direction: if \\(y_i = 1\\) then \\(s_i^{(t+1)} &gt; s_i^{(t)}\\), while if \\(y_i = -1\\) then \\(s_i^{(t+1)} &lt; s_i^{(t)}\\).Again, this is only if \\(s^{(t)}_i y_i &lt; 0\\); otherwise there is no change in \\(s_i^{(t)}\\) in the current iteration.\n\n\nGlobal Properties\n\n\n\n\n\n\n\nDefinition 7.1 (Linear Separability) A data set with feature matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n\\times k}\\) and target vector \\(y\\in \\{-1, 1\\}\\) is linearly separable if there exists a weight vector \\(\\mathbf{w}\\) such that, for all \\(i \\in [n]\\),\n\\[\n\\langle\\mathbf{w}, \\mathbf{x}_i\\rangle &gt; 0 \\Leftrightarrow y = 1\\;.\n\\]\n\n\n\n\nTake a moment to convince yourself of the following:\n\n\n\n\n\n\n\nProposition 7.1 (Nonconvergence of perceptron for nonseparable data) Suppose that \\((\\mathbf{X}, \\mathbf{y})\\) is not linearly separable. Then, the perceptron update does not converge. Furthermore, at no iteration of the algorithm is it the case that \\(A(\\mathbf{w}) = 1\\).\n\n\n\n\nIt’s not as obvious that, if the data is linearly separable, then the perceptron algorithm will converge to a correct answer. Perhaps surprisingly, this is also true:\n\n\n\n\n\n\n\nTheorem 7.1 (Convergence of perceptron for separable data) Suppose that \\((\\mathbf{X}, \\mathbf{y})\\) is linearly separable. Then:\n\nThe perceptron algorithm converges in a finite number of iterations to a vector \\(\\mathbf{w}\\) that separates the data, so that \\(A(\\mathbf{w}) = 1\\).\n\nDuring the running of the perceptron algorithm, the total number of updates made is no more than \\[\\frac{2 + r(\\mathbf{X})^2}{\\gamma(\\mathbf{X}, \\mathbf{y})}\\;,\\]\n\nwhere \\(r(\\mathbf{X}) = \\max_{i \\in [n]} \\lVert \\mathbf{x}_i \\rVert\\) and \\(\\gamma(\\mathbf{X}, \\mathbf{y})\\) is a geometric measure called the margin of how far apart the two label classes are.\n\n\n\n\nFor a proof of Theorem 7.1, see p. 37-44 of Hardt and Recht (2022).",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Classification: The Perceptron</span>"
    ]
  },
  {
    "objectID": "chapters/20-perceptron.html#what-next",
    "href": "chapters/20-perceptron.html#what-next",
    "title": "7  Introduction to Classification: The Perceptron",
    "section": "What Next?",
    "text": "What Next?\nWe have outlined the perceptron algorithm, which is able to find a separating hyperplane between two labeled groups of points when such a hyperplane exists.\nWe should, however, be troubled by the fact that the perceptron algorithm doesn’t converge when the data isn’t linearly separable. Maybe we could design a different algorithm that would allow us to find a parameter vector \\(\\mathbf{w}\\) that makes the empirical risk \\(R(\\mathbf{w}) = 1 - A(\\mathbf{w})\\) as small as possible?\nUnfortunately, we have a grave problem here:\n\n\n\n\n\n\n\nTheorem 7.2 (0-1 Minimization for Linear Classifiers is NP Hard (Kearns, Schapire, and Sellie (1992))) Unless P = NP, there is no polynomial-time algorithm that can find a \\(\\mathbf{w}\\) that solves Equation 11.1 when \\(R(\\mathbf{w}) = 1 - A(\\mathbf{w})\\) is the rate of incorrect classifications.\n\n\n\n\nSo, if our data is not linearly separable, not only will the perceptron algorithm not converge, but no classical algorithm will solve the minimization problem in polynomial time.\nIn order to make progress towards practical algorithms, we will need to slightly change our approach. This will be the subject of our next several chapters.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Classification: The Perceptron</span>"
    ]
  },
  {
    "objectID": "chapters/20-perceptron.html#references",
    "href": "chapters/20-perceptron.html#references",
    "title": "7  Introduction to Classification: The Perceptron",
    "section": "References",
    "text": "References\n\n\n\n\nHardt, Moritz, and Benjamin Recht. 2022. Patterns, Predictions, and Actions. Princeton University Press. https://mlstory.org/pdf/patterns.pdf.\n\n\nKearns, Michael J., Robert E. Schapire, and Linda M. Sellie. 1992. “Toward Efficient Agnostic Learning.” In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, 341–52. Pittsburgh Pennsylvania USA: ACM. https://doi.org/10.1145/130385.130424.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Classification: The Perceptron</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html",
    "href": "chapters/22-convex-erm.html",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "",
    "text": "Modeling Choices\nOnce we have chosen linear models as our tool, we can specify a model for the binary classification task by making two additional choices:\nWhat choices did we make in the context of the perceptron?\n\\[\nL(\\mathbf{w}) = \\frac{1}{n}\\sum_{i = 1}^n \\mathbb{1}[s_i (2y_i-1) &lt; 0]\n\\]\nThe optimizer we used to minimize the loss was the perceptron update, in which we picked a random point \\(i\\) and then performed the update\nHowever, as we saw, this doesn’t actually work that well. There are two problems:\nSo, how could we choose a better loss function that would allow us to create efficient algorithms?",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html#modeling-choices",
    "href": "chapters/22-convex-erm.html#modeling-choices",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "",
    "text": "Loss: How will we measure the success of the model in distinguishing the two classes?\nOptimizer: What algorithm will we use in order to minimize the loss?\n\n\nThe loss function was the misclassification rate. If we let \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i\\rangle\\), then we can write the loss like this: Here, the term \\(2y_i - 1\\) transforms a \\(y_i\\) with values in \\(\\{0,1\\}\\) into one with values in \\(\\{-1,1\\}\\).\n\n\n \\[\n\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\mathbb{1}[s_i (2y_i-1) &lt; 0]y_i \\mathbf{x}_i\n\\]If \\(i\\) is correctly classified (i.e. if \\(s_i(2 y_i - 1) &gt; 0\\)), then the second term zeros out and nothing happens.\n\n\nOur problem with the optimizer was that this update won’t actually converge if the data is not linearly separable. Maybe we could choose a better optimizer that would converge?\nUnfortunately not – as we saw last time, the very problem of minimizing \\(L(\\mathbf{w})\\) is NP-hard. This is a problem with the loss function itself.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html#convex-functions",
    "href": "chapters/22-convex-erm.html#convex-functions",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "Convex Functions",
    "text": "Convex Functions\nLet’s start by visualizing a single term of the perceptron loss function. We’ll view this as a function of the score \\(s\\) and the true target value \\(y\\):\n\\[\n\\ell(s, y) = \\mathbb{1}[s (2y-1) &lt; 0]\\;.\n\\]\nWe’ll call this the 0-1 loss function. Here’s a plot of this function for each of the two possible values of \\(y\\):\n\ndef zero_one_loss(s, y): \n    return 1*(s*(2*y-1)&lt; 0)\n\n# or \n# hinge_loss = lambda s, y: 1*(s*(2*y-1) &lt; 0)\n\n\n\nCode\nfrom matplotlib import pyplot as plt \nplt.style.use('seaborn-v0_8-whitegrid')\n# plt.rcParams[\"figure.figsize\"] = (10, 4)\n\ndef plot_loss(loss_fun, show_line = False):\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        fig, axarr = plt.subplots(1, 2, figsize = (6, 3)) \n        s = torch.linspace(-1, 1, 10001)\n\n        for j in range(2):\n            y = [0, 1][j]\n            axarr[j].set_title(f\"y = {y}\")\n            axarr[j].set(xlabel = r\"$s$\", \n                        ylabel = r\"$\\ell(s, y)$\")\n            \n            ix1 = s &lt; 0\n            axarr[j].plot(s[ix1], loss_fun(s[ix1], y), color = \"black\")\n            ix2 = s &gt; 0\n            axarr[j].plot(s[ix2], loss_fun(s[ix2], y), color = \"black\")\n\n            if show_line: \n                s1 = torch.tensor([-0.7])\n                s2 = torch.tensor([0.9])\n\n                axarr[j].plot([s1, s2], [loss_fun(s1, y), loss_fun(s2, y)], color = \"darkgrey\", linestyle = \"--\")\n\n        plt.tight_layout()\n        return fig, axarr\n\nfig, axarr = plot_loss(loss_fun = zero_one_loss, show_line = False)\n\n\n\n\n\n\n\n\nFigure 8.2: The 0-1 loss function.\n\n\n\n\n\nSurprsingly, the problem with this loss function \\(\\ell\\) is that we can “draw lines under the function.” What this means is that we can pick two points on the graph of the function, connect them with a line, and find that the line lies under the graph of the function in some regions:\n\n\nCode\nfig, axarr = plot_loss(loss_fun = zero_one_loss, show_line = True)\n\n\n\n\n\n\n\n\nFigure 8.3: The 0-1 loss function with a line demonstrating that this function is nonconvex.\n\n\n\n\n\nSurprisingly, this specific geometric property is what’s blocking us from achieving performant searchability for the problem of finding \\(\\mathbf{w}\\).",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html#convex-loss-functions",
    "href": "chapters/22-convex-erm.html#convex-loss-functions",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "Convex Loss Functions",
    "text": "Convex Loss Functions\nIn order to develop good properties of loss functions, we need to define two concepts: convex sets, which we mostly won’t need to worry about, and convex functions.\n\n\n\n\n\n\n\nDefinition 8.1 A set \\(S \\subseteq \\mathbb{R}^n\\) is convex if, for any two points \\(\\mathbf{z}_1, \\mathbf{z}_2 \\in S\\) and for any \\(\\lambda \\in [0,1]\\), the point \\(\\mathbf{z} = \\lambda \\mathbf{z}_1 + (1-\\lambda) \\mathbf{z}_2\\) is also an element of \\(S\\).\n\n\n\n\nWe also need to define convex functions:\n\n\n\n\n\n\n\nDefinition 8.2 (Convex Functions) Let \\(S \\subseteq \\mathbb{R}^n\\) be convex. A function \\(f:S \\rightarrow \\mathbb{R}\\) is convex if, for any \\(\\lambda \\in \\mathbb{R}\\) and any two points \\(\\mathbf{z}_1, \\mathbf{z}_2 \\in S\\), we have\n\\[\nf(\\lambda \\mathbf{z}_1 + (1-\\lambda)\\mathbf{z}_2) \\leq \\lambda f( \\mathbf{z}_1 ) + (1-\\lambda)f(\\mathbf{z}_2)\\;.\n\\]\nThe function \\(f\\) is strictly convex if the inequality is strict: for all \\(\\lambda\\), \\(\\mathbf{z}_1\\), and \\(\\mathbf{z}_2\\),\n\\[\nf(\\lambda \\mathbf{z}_1 + (1-\\lambda)\\mathbf{z}_2) &lt; \\lambda f( \\mathbf{z}_1 ) + (1-\\lambda)f(\\mathbf{z}_2)\\;.\n\\]\n\n\n\n\nRoughly, a convex function is “bowl-shaped,” in the sense that any line connecting two points on its graph must lie above the graph. The most familiar example of a convex function is our good friend the convex parabola:\n\n\nCode\nx = torch.linspace(-1, 1, 10001)\ny = x**2\n\nplt.plot(x, y, color = \"black\")\nlabs = plt.gca().set(xlabel = r\"$x$\", ylabel = r\"$f(x)$\")\n\n\n\n\n\n\n\n\nFigure 8.4: The convex parabola \\(f(x) = x^2\\)\n\n\n\n\n\nNote that any straight line connecting two points on this graph always stays above the graph of the parabola. As we saw above, the 0-1 loss function \\(\\ell(s, y) = \\mathbb{1}[s(2y-1)&lt;0]\\) does not have this property.\nWe can also define convex functions to replace the nonconvex 0-1 loss function from earlier. Here’s an example, which is usually called the hinge loss, which is defined by the formula\n\\[\n\\ell(s, y) = y\\max\\{0, -s\\}  + (1 - y) \\max \\{0, s\\}\\;.\n\\]\n\n\ndef hinge_loss(s, y):\n    first_term  =  y    * (torch.max(torch.zeros_like(s),  -s))  \n    second_term = (1-y) * (torch.max(torch.zeros_like(s), s))\n    return first_term + second_term\n\n# or\n# hinge_loss = lambda s, y: y * (torch.max(torch.zeros_like(s), s))  + (1-y) * (torch.max(torch.zeros_like(s), -s))\n\n\n\nCode\nfig, axarr = plot_loss(loss_fun = hinge_loss, show_line = True)\n\n\n\n\n\n\n\n\nFigure 8.5: The hinge loss function.\n\n\n\n\n\nThe hinge loss is not strictly convex and is not even everywhere differentiable! Despite this, the fact that it is convex has made it a modern workhorse of machine learning. The support vector machine (SVM) operates by minimizing the hinge loss. The “Rectified Linear Unit” (ReLU) is a mainstay of modern deep learning–and is just another name for the hinge loss.\nAn even handier loss function for our purposes is the sigmoid binary cross entropy, which is defined by the formula  \\[\n\\begin{aligned}\n\\ell(s, y) &=  -y \\log \\sigma(s) - (1-y)\\log (1-\\sigma(s))\\;,\n\\end{aligned}\n\\]In this formula, \\(\\sigma(s) = \\frac{1}{1 + e^{-s}}\\) is the logistic sigmoid function.\n\ndef sig(s):\n    return 1 / (1 + torch.exp(-s))\n\ndef binary_cross_entropy(s, y):\n    return -(y * sig(s).log() + (1 - y)*(1-sig(s)).log())\n\n# or \n# sig = lambda s: 1 / (1 + torch.exp(-s))\n# binary_cross_entropy = lambda s, y: -(y * sig(s).log() + (1 - y)*(1-sig(s)).log())\n\n\n\nCode\nsig = lambda s: 1 / (1 + torch.exp(-s))\nbinary_cross_entropy = lambda s, y: -(y * sig(s).log() + (1 - y)*(1-sig(s)).log())\nfig, axarr = plot_loss(loss_fun = binary_cross_entropy, show_line = True)\n\n\n\n\n\n\n\n\nFigure 8.6: The binary cross-entropy loss function.\n\n\n\n\n\nThis function is also convex, and has the considerable benefit of being everywhere differentiable.\nWe intentionally formulated our definition of convexity for functions of many variables. Here is a convex function \\(f:\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\).\n\n\nCode\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n\nx = torch.linspace(-1, 1, 1001)[:, None]\ny = torch.linspace(-1, 1, 1001)[None, :]\n\nz = x**2 + y**2\n\nax.plot_surface(x, y, z, cmap=\"inferno_r\",\n                       linewidth=0, antialiased=False)\n\nlabs = ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\n\n\n\n\n\n\nFigure 8.7: A convex quadratic function of two variables.\n\n\n\n\n\nYou could imagine trying to draw a straight line between two points on the graph of this function – the line would always be above the graph. When thinking about convexity in many variables, it is often sufficient to imagine a bowl-shaped function like this one.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html#convex-empirical-risk-minimization",
    "href": "chapters/22-convex-erm.html#convex-empirical-risk-minimization",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "Convex Empirical Risk Minimization",
    "text": "Convex Empirical Risk Minimization\nWe are now ready to define the primary framework in which we will conduct supervised machine learning: convex empirical risk minimization.\n\n\n\n\n\n\n\nDefinition 8.3 (Empirical Risk Minimization) Given a loss function \\(\\ell:\\mathbb{R}\\times \\{0,1\\} \\rightarrow \\mathbb{R}\\), a feature matrix \\(\\mathbb{X} \\in \\mathbb{R}^{n\\times p}\\), a target vector \\(\\mathbf{y}\\), and a parameter vector \\(\\mathbf{w} \\in \\mathbb{R}^p\\), the empirical risk of \\(\\mathbf{w}\\) is\n\\[\n\\begin{aligned}\nL(\\mathbf{w}) = \\frac{1}{n}\\sum_{i = 1}^n \\ell(s_i, y_i), \\quad&\\text{where }s_i = \\langle \\mathbf{w}, \\mathbf{x}_i\\rangle\\;.\n\\end{aligned}\n\\]\nThe empirical risk minimization problem is to find the value of \\(\\mathbf{w}\\) that makes \\(L(\\mathbf{w})\\) smallest:\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{aligned}\n\\hat{\\mathbf{w}} &= \\argmin_{\\mathbf{w}} L(\\mathbf{w})  \\\\\n                 &= \\argmin_{\\mathbf{w}} \\frac{1}{n}\\sum_{i = 1}^n \\ell(s_i, y_i) \\\\\n                 &= \\argmin_{\\mathbf{w}} \\frac{1}{n}\\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle, y_i)\\;.\n\\end{aligned}\n\\tag{8.1}\\]\n\n\n\n\n\n\n\n\n\n\n\nProposition 8.1 (Convex \\(\\ell\\) means convex \\(L\\)) If the per-observation loss function \\(\\ell:\\mathbb{R}\\times \\{0,1\\} \\rightarrow \\mathbb{R}\\) is convex in its first argument, then the empirical risk \\(L(\\mathbf{w})\\) is convex as a function of \\(\\mathbf{w}\\).\n\n\n\n\nThe proof of Proposition 8.1 involves some elementary properties of convex functions:\n\nIf \\(f(\\mathbf{z})\\) is convex as a function of \\(\\mathbf{z}\\), then \\(g(\\mathbf{z}) = f(\\mathbf{A}\\mathbf{z'})\\) is also convex as a function of \\(\\mathbf{z}'\\), provided that all the dimensions work out.\nAny finite sum of convex functions is convex.\n\nSo, we know that if we choose \\(\\ell\\) to be convex in the score function, then the entire empirical risk \\(L\\) will be convex as a function of the weight vector \\(\\mathbf{w}\\).\nWhy do we care?",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html#convex-functions-have-global-minimizers",
    "href": "chapters/22-convex-erm.html#convex-functions-have-global-minimizers",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "Convex Functions Have Global Minimizers",
    "text": "Convex Functions Have Global Minimizers\nWe want to solve the empirical risk minimization problem:\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{aligned}\n\\hat{\\mathbf{w}} &= \\argmin_{\\mathbf{w}} L(\\mathbf{w}).\n\\end{aligned}\n\\]\nWe might ask ourselves a few questions about this problem:\n\nExistence: Does there exist any choice of \\(\\mathbf{w}\\) that achieves a minimizing value for this function?\nUniqueness: Is this choice of \\(\\mathbf{w}\\) unique, or are there multiple candidates?\nSearchability: are there algorithms which are guaranteed to (a) terminate and (b) not get “trapped” at a bad solution?\n\nAnswering these questions precisely requires a bit more math:\n\n\n\n\n\n\n\nDefinition 8.4 (Local and Global Minimizers) A point \\(\\mathbf{z}\\in S\\) is a global minimizer of the function \\(f:S \\rightarrow \\mathbb{R}\\) if \\(f(\\mathbf{z}) \\leq f(\\mathbf{z}')\\) for all \\(\\mathbf{z}' \\in S\\).\nA point \\(\\mathbf{z} \\in S\\) is a local minimizer of \\(f:S \\rightarrow \\mathbb{R}\\) if there exists a neighborhood \\(T \\subseteq S\\) containing \\(\\mathbf{z}\\) such that \\(\\mathbf{z}\\) is a global minimizer of \\(f\\) on \\(T\\).\n\n\n\n\nIt’s ok if you don’t know what it means for a set to be closed – all the convex functions we will care about in this class will either be defined on sets where this theorem holds or will be otherwise defined so that the conclusions apply.\n\n\n\n\n\n\n\nTheorem 8.1 (Properties of Convex Functions) Let \\(f:S \\rightarrow \\mathbb{R}\\) be a convex function. Then:\n\nIf \\(S\\) is closed and bounded, \\(f\\) has a minimizer \\(\\mathbf{z}^*\\) in \\(S\\).\nFurthermore, if \\(\\mathbf{z}^*\\) is a local minimizer of \\(f\\), then it is also a global minimizer.\nIf in addition \\(f\\) is strictly convex, then this minimizer is unique.\n\n\n\n\n\n\nProof. The proof of item 1 needs some tools from real analysis. The short version is:\n\nEvery convex function is continuous.\nIf \\(S\\subseteq \\mathbb{R}^n\\) is closed and bounded, then it is compact.\nContinuous functions achieve minimizers and maximizers on compact sets.\n\nIt’s ok if you didn’t follow this! Fortunately the second part of the proof is one we can do together. Suppose to contradiction that \\(\\mathbf{z}^*\\) is a local minimizer of \\(f\\), but that there is also a point \\(\\mathbf{z}'\\) such that \\(f(\\mathbf{z}') &lt; f(\\mathbf{z}^*)\\). Since \\(\\mathbf{z}^*\\) is a local minimizer, we can find some neighborhood \\(T\\) containing \\(\\mathbf{z}^*\\) such that \\(\\mathbf{z}^*\\) is a minimizer of \\(f\\) on \\(T\\). Let \\(\\lambda\\) be some very small number and consider the point \\(\\mathbf{z} = \\lambda \\mathbf{z}' + (1-\\lambda)\\mathbf{z}^*\\). Specifically, choose \\(\\lambda\\) small enough so that \\(\\mathbf{z} \\in T\\) (since this makes \\(\\mathbf{z}\\) close to \\(\\mathbf{z}^*\\)). We can evaluate\n\\[\n\\begin{align}\nf(\\mathbf{z}) &= f(\\lambda \\mathbf{z}' + (1-\\lambda)\\mathbf{z}^*) &\\quad \\text{(definition of $\\mathbf{z}$)}\\\\\n       &\\leq \\lambda f(\\mathbf{z}') + (1-\\lambda)f(\\mathbf{z}^*)  &\\quad \\text{($f$ is convex)} \\\\\n       &= f(\\mathbf{z}^*) + \\lambda (f(\\mathbf{z}') - f(\\mathbf{z}^*)) &\\quad \\text{(algebra)}\\\\\n       &&lt; f(\\mathbf{z}^*)\\;. &\\quad \\text{(assumption that $f(\\mathbf{z}') &lt; f(\\mathbf{z}^*)$)}\n\\end{align}\n\\]\nBut this is a contradiction, since we constructed \\(\\mathbf{z}\\) to be in the neighborhood \\(T\\) where \\(\\mathbf{z}^*\\) is a local minimizer. We conclude that there is no \\(\\mathbf{z}'\\) such that \\(f(\\mathbf{z}') &lt; f(\\mathbf{z}^*)\\), and therefore that \\(\\mathbf{z}^*\\) is a global minimizer.\nThe proof of the third part follows a very similar argument to the proof of the second part.\n\nThese properties of convex functions have very important implications for our fundamental questions on empirical risk minimization. If we choose a strictly convex per-observation loss function \\(\\ell\\), then our empirical risk \\(L\\) will also be strictly convex, and:\nExistence. The minimizer \\(\\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}}L(\\mathbf{w})\\) will exist.\nUniqueness: The minimizer \\(\\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}}L(\\mathbf{w})\\) will be unique: if we run a minimization algorithm repeatedly, we’ll get the same answer every time.\nSearchability: When \\(L\\) is convex, there are also no local minimizers other than the global minimizer. Algorithmically, this is the most important property of convexity. It means that if I manage to find any local minimizer at all, that point must be the global minimizer.  Performance: Convexity significantly reduces the difficulty of our task: instead of trying to find “the best” solution, it’s sufficient for us to find any local optimum. This means that we can design our algorithms to be “greedy local minimizer hunters.” There are lots of fast algorithms to do this. An especially important class of algorithms are gradient descent methods, which we’ll discuss soon.If you’ve taken an algorithms class, one way of thinking of convexity is that it guarantees that greedy methods work for solving minimization problems.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html#demo-logistic-regression",
    "href": "chapters/22-convex-erm.html#demo-logistic-regression",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "Demo: Logistic Regression",
    "text": "Demo: Logistic Regression\nYou may have heard of logistic regression in a course on statistics or data science. Logistic regression is simply binary classification using a linear model and the binary cross-entropy loss function which we saw above:\n\\[\n\\begin{aligned}\n\\ell(s, y) &=  -y \\log \\sigma(s) - (1-y)\\log (1-\\sigma(s))\\;,\n\\end{aligned}\n\\]\nAs can be proven with calculus, this function is convex as a function of \\(s\\). The logistic regression problem then becomes the problem of solving:\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{w}} &= \\argmin_\\mathbf{w} \\frac{1}{n} \\sum_{i = 1}^n \\ell(s_i, y_i)  \\\\\n&= \\argmin_\\mathbf{w} \\frac{1}{n} \\sum_{i = 1}^n \\left[-y_i \\log \\sigma(s_i) - (1-y_i)\\log (1-\\sigma(s_i))\\right] \\\\\n&= \\argmin_\\mathbf{w} \\frac{1}{n} \\sum_{i = 1}^n \\left[-y_i \\log \\sigma(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle) - (1-y_i)\\log (1-\\sigma(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle)\\right]\n\\end{aligned}\n\\]\nSo, let’s do convex empirical risk minimization! We’ll use the following data set. Note that this data is not linearly separable and therefore the perceptron algorithm would not converge.\n\n\nCode\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(noise = 0.5)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nFigure 8.8: Data for logistic regression.\n\n\n\n\n\nLet’s go ahead and train a logistic regression model. For the purposes of today, we can do this in a very simple way that doesn’t even involve an explicit training loop. Next time, we’ll learn how to write an explicit training loop.\nFirst, we’ll define a complete function for calculating the empirical risk for a given value of \\(\\mathbf{w}\\). Since we already implemented binary_cross_entropy, this implementation is very quick:\n\ndef empirical_risk(w, X, y):\n    s = X@w\n    return binary_cross_entropy(s, y).mean()\n\nNow we’ll use the minimize function from scipy.optimize to find the value of \\(\\mathbf{w}\\) that makes this function smallest:\n\nfrom scipy.optimize import minimize\n\nw0 = torch.tensor([1.0, 1.0, 1.0])\nresult = minimize(lambda w: empirical_risk(w, X, y), x0 = w0)\nw = result.x\n\nprint(f\"\"\"Learned parameter vector w = {w}.\nThe empirical risk is {result.fun:.4f}.\"\"\")\n\nLearned parameter vector w = [ 4.30124525  5.0754567  -4.55870966].\nThe empirical risk is 0.1670.\n\n\nHow does it look?\n\n\nCode\ndef draw_line(w, X, y, x_min, x_max, ax, **kwargs):\n    fig, ax = plt.subplots(1, 1)\n    plot_classification_data(X, y, ax)\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    l = ax.plot(x, y, **kwargs)\n\ndraw_line(w, X, y, x_min = -0.5, x_max = 1.5, ax = ax, color = \"black\", linestyle = \"dashed\")\n\n# fig\n\n\n\n\n\n\n\n\nFigure 8.9: The separating line learned by logistic regression.\n\n\n\n\n\nPretty good! Yes, it’s as easy as that – provided that you don’t ask too many questions about how the minimize function works. Questions like that will be the topic of our next several sets of notes.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html#recap",
    "href": "chapters/22-convex-erm.html#recap",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "Recap",
    "text": "Recap\nIn this set of notes, we introduced a fundamental idea: convex empirical risk minimization. To do convex empirical risk minimization, all we need is a convex per-observation loss function. This gives us a convex empirical risk function, which is simply the mean of all the per-observation losses. Once we have that, the problem of classification reduces to the problem of finding a value of the parameter vector \\(\\mathbf{w}\\) that makes the empirical risk small. Convexity guarantees that this problem has exactly one solution. Today, we found this solution using a packaged optimizer. Starting next time, we’ll learn how to write our own optimization algorithms and explore how optimization techniques enable scalable machine learning.\n\n(Optional): A Logistic Regression Training Loop\nHow would we do this if we didn’t have access to the minimize function? We’ll soon discuss this question much more. For now, we can take a look at the code block below, which implements such a loop using a framework very similar to the one we learned for perceptron. This model also inherits from the LinearModel class that you previously started implementing. The training loop is also very similar to our training loop for the perceptron. The main difference is that the loss is calculated using the binary_cross_entropy function above, and the step function of the GradientDescentOptimizer works differently in a way that we will discuss in the following section.\nStarting with the code block below, you won’t be able to follow along in coding these notes unless you have sneakily implemented logistic regression in a hidden module.\n\nfrom hidden.logistic import LogisticRegression, GradientDescentOptimizer\n\n# instantiate a model and an optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\n\nfor _ in range(100):\n\n    # not part of the update: just for tracking our progress    \n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n\n    # only this line actually changes the parameter value\n    opt.step(X, y, lr = 0.02)\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.semilogx()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\nFigure 8.10: Evolution of the binary cross entropy loss function in the logistic regression training loop.\n\n\n\n\n\nThe loss quickly levels out to a constant value (which is the same as we learned with scipy.optimize.minimize). Because our theory tells us that the loss function is convex, we know that the value of \\(\\mathbf{w}\\) we have found is the best possible, in the sense of minimizing the loss.\nLet’s take a look at the separating line we found:\n\ndraw_line(LR.w, X, y, \n          x_min = -0.5, \n          x_max = 1.5, \n          ax = ax, \n          color = \"black\", \n          linestyle = \"dashed\")\n\n\n\n\n\n\n\n\nYep, that’s the same line as we found earlier!\nAlthough our data is not linearly separable, the separating line we have learned appears to do a reasonable job of separating the points from each other. Let’s check our accuracy:\n\n(1.0*(LR.predict(X) == y)).mean()\n\ntensor(0.9267)\n\n\nNot too bad! In the next section, we’ll learn much, much more about what’s behind that opt.step() call.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html",
    "href": "chapters/23-gradient-descent.html",
    "title": "9  Optimization with Gradient Descent",
    "section": "",
    "text": "Linear Approximations of Single-Variable Functions\nRecall the limit definition of a derivative of a single-variable function. Let \\(g:\\mathbb{R} \\rightarrow \\mathbb{R}\\). The derivative of \\(g\\) at point \\(w_0\\), if it exists, is\n\\[\n\\begin{aligned}\n\\frac{dg(w_0)}{dw} = \\lim_{\\delta w\\rightarrow 0}\\frac{g(w_0 + \\delta w) - g(w_0)}{\\delta w}\\;.\n\\end{aligned}\n\\]\nIf we imagine that \\(\\delta w\\) is very small but nonzero, we can interpret this equation a bit loosely as the statement that\n\\[\n\\begin{aligned}\n\\frac{dg(w_0)}{dw} \\approx \\frac{g(w_0 + \\delta w) - g(w_0)}{\\delta w}\\;,\n\\end{aligned}\n\\]\nwhich upon some algebraic rearrangement says that\n\\[\ng(w_0 + \\delta w) \\approx g(w_0) + \\frac{dg(w_0)}{dw} \\delta w\\;.\n\\]\nTaylor’s theorem makes this statement precise:\nHere, \\(o(\\lvert\\delta w\\rvert)\\) means “terms that grow small in comparison to \\(\\delta w\\) when \\(\\lvert\\delta w\\rvert\\) itself grows small.”\nAnother common way to write Taylor’s theorem is\n\\[\ng(w) = g(w_0) + \\frac{dg(w_0)}{dw} (w - w_0) + o(|w - w_0|)\\;,\n\\]\nwhich comes from substituting \\(\\delta w = w - w_0\\).\nTaylor’s theorem says that, in a neighborhood of \\(w_0\\), we can approximate \\(g(w)\\) with a linear function. Here’s an example of how that looks:\nimport torch \nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\nw = torch.linspace(-1, 1, 101)\n\ng = lambda w: w**2\n\nplt.plot(w, g(w), label = r\"$g(w)$\", color = \"black\")\nplt.gca().set(xlabel = r\"$w$\", ylim = (-0.2, 0.5))\n\ndef taylor(w, w0):\n    return g(w0) + 2*w0*(w-w0)\n\nw0 = 0.2\n\nplt.plot(w, taylor(w, w0), label = r\"1st-order Taylor approximation\", linestyle = \"--\")\nplt.legend()\nThis doesn’t really look like a very good approximation, but it looks better if you zoom in!\nplt.plot(w, g(w), label = r\"$g(w)$\", color = \"black\")\nplt.gca().set(xlabel = r\"$w$\", ylim = (-0.2, 0.5))\nplt.plot(w, taylor(w, w0), label = r\"1st-order Taylor approximation\", linestyle = \"--\")\nplt.gca().set(xlim = (w0 - 0.1, w0 + 0.1), ylim = (-0.00, 0.1))\nplt.legend()",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html#linear-approximations-of-single-variable-functions",
    "href": "chapters/23-gradient-descent.html#linear-approximations-of-single-variable-functions",
    "title": "9  Optimization with Gradient Descent",
    "section": "",
    "text": "Theorem 9.1 (Taylor’s Theorem: Univariate Functions) Let \\(g:\\mathbb{R}\\rightarrow \\mathbb{R}\\) be differentiable at point \\(w_0\\). Then, there exists \\(a &gt; 0\\) such that, if \\(\\lvert\\delta w\\rvert &lt; a\\), then\n\\[\ng(w_0 + \\delta w) = g(w_0) + \\frac{dg(w_0)}{dw} \\delta w + o(\\vert\\delta w\\rvert)\\;.\n\\]",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html#gradient-descent-in-1-dimension",
    "href": "chapters/23-gradient-descent.html#gradient-descent-in-1-dimension",
    "title": "9  Optimization with Gradient Descent",
    "section": "Gradient Descent in 1 dimension",
    "text": "Gradient Descent in 1 dimension\nSuppose that we have a function \\(g\\) and we would like to solve the the optimization problem\n\\[\n\\begin{aligned}\n\\hat{w} = \\argmin _w g(w) \\;.\n\\end{aligned}\n\\]\nHow do we go about doing this? You might remember from calculus that one way starts with solving the equation\n\\[\n\\begin{aligned}\n\\frac{dg(\\hat{w})}{dw} = 0\\;,\n\\end{aligned}\n\\]\nto find critical points – under certain conditions, it is guaranteed that a minimum, if it exists, will be one of these critical points. However, it is not always feasible to solve this equation exactly in practice.\nIn iterative approaches, we instead imagine that we have a current guess \\(\\hat{w}\\) which we would like to improve by adding some \\(\\delta \\hat{w}\\) to it. To this end, consider the casual Taylor approximation In the rest of these notes, we will assume that term \\(o(\\lvert \\delta w\\rvert)\\) is small enough to be negligible.\n\\[\n\\begin{aligned}\ng(\\hat{w} + \\delta \\hat{w}) \\approx g(\\hat{w}) + \\frac{dg(\\hat{w})}{dw} \\delta \\hat{w}\\;.\n\\end{aligned}\n\\]\nWe’d like to update our estimate of \\(\\hat{w}\\). Suppose we make a strategic choice: \\(\\delta \\hat{w} = -\\alpha \\frac{dg(\\hat{w})}{dw}\\) for some small \\(\\alpha &gt; 0\\). We therefore decide that we will do the update\n\\[\n\\begin{aligned}\n    \\hat{w} \\gets \\hat{w} - \\alpha \\frac{dg(\\hat{w})}{dw}\\;.\n\\end{aligned}\n\\tag{9.2}\\]\nWhat does this update do to the value of \\(g\\)? Let’s check:\n\\[\n\\begin{aligned}\n    g(\\hat{w} + \\delta \\hat{w}) &\\approx g(\\hat{w}) + \\frac{dg(\\hat{w})}{dw} \\delta \\hat{w} \\\\\n    &= g(\\hat{w}) - \\frac{dg(\\hat{w})}{dw} \\alpha \\frac{dg(\\hat{w})}{dw}\\\\\n    &= g(\\hat{w}) - \\alpha\\left(\\frac{dg(\\hat{w})}{dw}\\right)^2\\;.   \n\\end{aligned}\n\\]\nThis is the big punchline. Let’s look at the second term. If \\(\\left(\\frac{dg(\\hat{w})}{dw}\\right)^2 = 0\\) then that must mean that \\(\\frac{dg(\\hat{w})}{dw}\\) and that we are at a critical point, which we could check for being a local minimum. On the other hand, if \\(\\frac{dg(\\hat{w})}{dw} \\neq 0\\), then \\(\\left(\\frac{dg(\\hat{w})}{dw}\\right)^2 &gt; 0\\). This means that\n\\[\n\\begin{aligned}\ng(\\hat{w} + \\delta \\hat{w}) &\\approx  g(\\hat{w}) - \\alpha\\left(\\frac{dg(\\hat{w})}{dw}\\right)^2 \\\\\n                            &&lt;  g(\\hat{w})\\;,\n\\end{aligned}\n\\]\nprovided that \\(\\alpha\\) is small enough for the error terms in Taylor’s Theorem to be small. We have informally derived the following fact:\n\n\n\n\n\n\nSingle-Variable Gradient-Descent Works\n\n\n\nLet \\(g:\\mathbb{R}\\rightarrow \\mathbb{R}\\) be differentiable and assume that \\(\\frac{dg(\\hat{w})}{dw} \\neq 0\\). Then, if \\(\\alpha\\) is sufficiently small, Equation 9.2 is guaranteed to reduce the value of \\(g\\).\n\n\nLet’s see an example of single-variable gradient descent in action:\n\nw_     = -0.7\ngrad  = lambda w: 2*w\nalpha = 0.1\nw_vec = [w_]\n\nfor i in range(100):\n        w_ = w_ - alpha*grad(w_) \n        w_vec.append(w_)\n\nw_vec = torch.tensor(w_vec)\n\nplt.plot(w, g(w), label = r\"$g(w)$\")\nplt.scatter(w_vec, g(w_vec), color = \"black\", label = r\"Gradient descent updates\", s = 10, zorder = 10)\nplt.gca().set(xlabel = r\"$w$\")\nplt.legend()\n\n\n\n\n\n\n\n\nWe can see the updates from gradient descent eventually converging to the point \\(w = 0\\), which is the global minimum of this function.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html#gradient-descent-in-multiple-dimensions",
    "href": "chapters/23-gradient-descent.html#gradient-descent-in-multiple-dimensions",
    "title": "9  Optimization with Gradient Descent",
    "section": "Gradient Descent in Multiple Dimensions",
    "text": "Gradient Descent in Multiple Dimensions\nOur empirical risk function \\(L\\) is not a single-variable function; indeed, \\(L: \\mathbb{R}^p \\rightarrow \\mathbb{R}\\). So, we can’t directly apply the results above. Fortunately, these results extend in a smooth way to this setting. The main thing we need is the definition of the gradient of a multivariate function.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html#gradients",
    "href": "chapters/23-gradient-descent.html#gradients",
    "title": "9  Optimization with Gradient Descent",
    "section": "Gradients",
    "text": "Gradients\nWe’re not going to talk much about what it means for a function to be multivariate differentiable. You can assume that all the functions we will deal with in this class are unless I highlight otherwise. For a more rigorous definition, you should check out a multivariable calculus class.\n\n\n\n\n\n\n\nDefinition 9.1 (Gradient of a Multivariate Function) Let \\(g:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) be a multivariate differentiable function. The gradient of \\(g\\) evaluated at point \\(\\mathbf{w}\\in \\mathbb{R}^p\\) is written \\(\\nabla g(\\mathbf{w})\\), and has value\n\\[\n\\nabla g(\\mathbf{w}) \\triangleq\n\\left(\\begin{matrix}\n    \\frac{\\partial g(\\mathbf{w})}{\\partial w_1} \\\\\n    \\frac{\\partial g(\\mathbf{w})}{\\partial w_2} \\\\\n    \\cdots \\\\\n    \\frac{\\partial g(\\mathbf{w})}{\\partial w_p} \\\\\n\\end{matrix}\\right) \\in \\mathbb{R}^p\\;.\n\\]\nHere, \\(\\frac{\\partial g(\\mathbf{w})}{\\partial w_1}\\) is the partial derivative of \\(f\\) with respect to \\(z_1\\), evaluated at \\(\\mathbf{w}\\). To compute it:\n\nTake the derivative of \\(f\\) *with respect to variable \\(z_1\\), holding all other variables constant, and then evaluate the result at \\(\\mathbf{w}\\).\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nLet \\(p = 3\\). Let \\(g(\\mathbf{w}) = w_2\\sin w_1  + w_1e^{2w_3}\\). The partial derivatives we need are\n\\[\n\\begin{align}\n\\frac{\\partial g(\\mathbf{w})}{\\partial w_1} &= w_2 \\cos w_1 + e^{2w_3}\\\\\n\\frac{\\partial g(\\mathbf{w})}{\\partial w_2} &= \\sin w_1\\\\\n\\frac{\\partial g(\\mathbf{w})}{\\partial w_3} &= 2w_1 e^{2w_3}\\;.\n\\end{align}\n\\]\nSo, the gradient of \\(g\\) evaluated at a point \\(\\mathbf{w}\\) is\n\\[\n\\nabla g(\\mathbf{w}) =\n\\left(\\begin{matrix}\n    \\frac{\\partial g(\\mathbf{w})}{\\partial w_1} \\\\\n    \\frac{\\partial g(\\mathbf{w})}{\\partial w_2} \\\\\n    \\frac{\\partial g(\\mathbf{w})}{\\partial w_3} \\\\\n\\end{matrix}\\right) =\n\\left(\\begin{matrix}\n    w_2 \\cos w_1 + e^{2w_3}\\\\\n    \\sin w_1\\\\\n    2w_1 e^{2w_3}\n\\end{matrix}\\right)\n\\]\n\n\nTaylor’s Theorem extends smoothly to this setting.\n\n\n\n\n\n\n\nTheorem 9.2 (Taylor’s Theorem: Multivariate Functions) Let \\(g:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) be differentiable at point \\(\\mathbf{w}_0 \\in \\mathbb{R}^p\\). Then, there exists \\(a &gt; 0\\) such that, if \\(\\lVert \\delta \\mathbf{w} \\rVert &lt; a\\), then \n\\[\ng(\\mathbf{w}_0 + \\delta \\mathbf{w}) = g(\\mathbf{w}_0) + \\langle \\nabla g(\\mathbf{w}_0), \\delta \\mathbf{w} \\rangle + o(\\lVert \\delta \\mathbf{w}\\rVert)\\;.\n\\]\n\n\n\n\n\\(\\lVert \\mathbf{\\delta} \\mathbf{w}\\rVert \\triangleq \\sqrt{\\sum_{i = 1}^p (\\delta w_i)^2}\\)The vector \\(\\nabla g(\\mathbf{w}_0)\\) plays the role of the single-variable derivative \\(\\frac{d g(w_0)}{dw}\\).\n\nMultivariate Gradient Descent\nIn multiple dimensions, the gradient descent update is:\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{w}} \\gets \\hat{\\mathbf{w}} - \\alpha \\nabla g(\\hat{\\mathbf{w}})\\;.\n\\end{aligned}\n\\tag{9.3}\\]\nLet’s check that a single update of gradient descent will reduce the value of \\(g\\) provided that \\(\\alpha\\) is small enough. Here, \\(\\delta \\hat{\\mathbf{w}} = -\\alpha \\nabla g(\\hat{\\mathbf{w}})\\).\n\\[\n\\begin{aligned}\n    g(\\hat{\\mathbf{w}} - \\delta \\hat{\\mathbf{w}}) &\\approx g(\\hat{\\mathbf{w}}) + \\langle \\nabla g(\\mathbf{w}_0), \\delta \\mathbf{w} \\rangle \\\\\n    &= g(\\hat{\\mathbf{w}}) + \\langle \\nabla g(\\hat{\\mathbf{w}}), -\\alpha \\nabla g(\\hat{\\mathbf{w}}) \\rangle \\\\\n    &= g(\\hat{\\mathbf{w}}) - \\alpha \\langle \\nabla g(\\hat{\\mathbf{w}}),  \\nabla g(\\hat{\\mathbf{w}}) \\rangle \\\\\n    &= g(\\hat{\\mathbf{w}}) - \\alpha \\lVert\\nabla g(\\hat{\\mathbf{w}}) \\rVert^2\\;.\n\\end{aligned}\n\\]\nSince \\(\\lVert\\nabla g(\\hat{\\mathbf{w}}) \\rVert^2 &gt; 0\\) whenever \\(\\nabla g(\\hat{\\mathbf{w}}) \\neq\\mathbf{0}\\), we conclude that, unless \\(\\hat{w}\\) is a critical point (where the gradient is zero), then\n\\[\n\\begin{aligned}\n    g(\\hat{\\mathbf{w}} - \\alpha \\nabla g(\\hat{\\mathbf{w}})) &lt; g(\\hat{\\mathbf{w}})\\;.\n\\end{aligned}\n\\]\nIn other words, provided that \\(\\alpha\\) is small enough for the Taylor approximation to be a good one, multivariate gradient descent also always reduces the value of the objective function.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html#gradient-of-the-empirical-risk",
    "href": "chapters/23-gradient-descent.html#gradient-of-the-empirical-risk",
    "title": "9  Optimization with Gradient Descent",
    "section": "Gradient of the Empirical Risk",
    "text": "Gradient of the Empirical Risk\nRemember that our big objective here was to solve Equation 12.1 using gradient descent. To do this, we need to be able to calculate \\(\\nabla L(\\mathbf{w})\\), where the gradient is with respect to the entries of \\(\\mathbf{w}\\). Fortunately, the specific linear structure of the score function \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) makes this relatively simple: indeed, we actually only need to worry about the single variable derivatives of the per-observation loss \\(\\ell\\). To see this, we can compute\n\\[\n\\begin{align}\n\\nabla L(\\mathbf{w}) &= \\nabla \\left(\\frac{1}{n} \\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle , y_i)\\right) \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n \\nabla \\ell(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle , y_i) \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(s_i, y_i)}{ds} \\nabla \\langle \\mathbf{w}, \\mathbf{x}_i\\rangle  &\\quad \\text{(multivariate chain rule)} \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(s_i, y_i)}{ds}  \\mathbf{x}_i &\\quad \\text{(gradient of a linear function)} \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(s_i, y_i)}{ds} \\mathbf{x}_i &\\quad \\text{($s_i = \\langle \\mathbf{w}, \\mathbf{x}_i\\rangle$)} \\\\\n\\end{align}\n\\tag{9.4}\\]\nThe good news here is that for linear models, we don’t actually need to be able to compute more gradients: we just need to be able to compute derivatives of the form \\(\\frac{d\\ell(s_i, y_i)}{ds}\\) and then plug in \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i\\rangle\\).\nLet’s do an example with the logistic loss:\n\\[\\ell(s, y) = -y \\log \\sigma(s) - (1-y)\\log (1-\\sigma(s))\\;.\\]\nA useful fact to know about the logistic sigmoid function \\(\\sigma\\) is that \\(\\frac{d\\sigma(s) }{ds} = \\sigma(s) (1 - \\sigma(s))\\). So, using that and the chain rule, the derivative we need is\n\\[\n\\begin{align}\n\\frac{d\\ell(s, y)}{ds} &= -y \\frac{1}{\\sigma(s)}\\frac{d\\sigma(s) }{ds} - (1-y)\\frac{1}{1-\\sigma(s)}\\left(- \\frac{d\\sigma(s) }{ds}\\right) \\\\\n&= -y \\frac{1}{\\sigma(s)}\\sigma(s) (1 - \\sigma(s)) - (1-y)\\frac{1}{1-\\sigma(s)}\\left(- \\sigma(s) (1 - \\sigma(s))\\right) \\\\\n&= -y (1 - \\sigma(s)) + (1-y)\\sigma(s) \\\\\n&= \\sigma(s) - y\\;.\n\\end{align}\n\\]\nFinally, we need to plug this back in to our empirical risk, obtaining the gradient of the empirical risk for logistic regression:\n \\[\n\\begin{align}\n\\nabla L(\\mathbf{w}) &= \\frac{1}{n} \\sum_{i = 1}^n (\\sigma(s_i) - y_i)\\mathbf{x}_i \\\\\n              &=\\frac{1}{n} \\sum_{i = 1}^n (\\sigma(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle) - y_i)\\mathbf{x}_i\\;.\n\\end{align}\n\\tag{9.5}\\]An important note about this formula that can easily trip one up: this looks a bit like a matrix multiplication or dot product, but it isn’t!\nThis gives us all the math that we need in order to learn logistic regression by choosing a learning rate and iterating the update \\(\\mathbf{w}^{(t+1)} \\gets \\mathbf{w}^{(t)} - \\alpha \\nabla L(\\mathbf{w}^{(t)})\\) until convergence.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html#example-logistic-regression",
    "href": "chapters/23-gradient-descent.html#example-logistic-regression",
    "title": "9  Optimization with Gradient Descent",
    "section": "Example: Logistic Regression",
    "text": "Example: Logistic Regression\nLet’s see gradient-descent in action for logistic regression. Our computational approach is based on the LinearModel class that you previously started implementing. The training loop is also very similar to our training loop for the perceptron. The main difference is that the loss is calculated using the binary_cross_entropy function above, and the step function of the GradientDescentOptimizer works differently in a way that we will discuss below.\nStarting with the code block below, you won’t be able to follow along in coding these notes unless you have sneakily implemented logistic regression in a hidden module.\n\n\nCode\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\n\nThe logistic regression training loop relies on a new implementation of opt.step. For gradient descent, here’s the complete implementation: just a quick Python version of the gradient descent update Equation 9.3.\n\ndef step(self, X, y, lr = 0.01):\n    self.model.w -= lr*self.model.grad(X, y)\n\nThe method model.grad() is the challenging part of the implementation: this is where we actually need to turn Equation 9.5 into code.\nHere’s the complete training loop. This loop is very similar to our perceptron training loop – we’re just using a different loss and a different implementation of grad.\n\nfrom hidden.logistic import LogisticRegression, GradientDescentOptimizer\n\n# instantiate a model and an optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\n\nfor _ in range(100):\n\n    # not part of the update: just for tracking our progress    \n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n\n    # only this line actually changes the parameter value\n    # The whole definition is: \n    # self.model.w -= lr*self.model.grad(X, y)\n\n    opt.step(X, y, lr = 0.02)\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.semilogx()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\nFigure 9.1: Evolution of the binary cross entropy loss function in the logistic regression training loop.\n\n\n\n\n\nThe loss quickly levels out to a constant value, which is our optimized weight vector \\(\\mathbf{w}\\). Because our theory tells us that the loss function is convex, we know that the value of \\(\\mathbf{w}\\) we have found is the best possible, in the sense of minimizing the loss.\nLet’s check our training accuracy:\n\n(1.0*(LR.predict(X) == y)).mean()\n\ntensor(0.9167)\n\n\nNot too bad!",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html#recap",
    "href": "chapters/23-gradient-descent.html#recap",
    "title": "9  Optimization with Gradient Descent",
    "section": "Recap",
    "text": "Recap\nIn these lecture notes, we introduced gradient descent as a method for minimizing functions, and showed an application of gradient descent for logistic regression. Gradient descent is especially useful when working with convex functions, since in this case it is guaranteed to converge to the global minimum of the empirical risk (provided that the learning rate \\(\\alpha\\) is sufficiently low). The idea of gradient descent – incremental improvement to the weight vector \\(\\mathbf{w}\\) using information about the derivatives of the loss function–is a fundamental one which has led to many variations and improvements.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  },
  {
    "objectID": "chapters/30-features-regularization.html",
    "href": "chapters/30-features-regularization.html",
    "title": "10  Feature Maps, Regularization, and Generalization",
    "section": "",
    "text": "Quick Recap\nLast time, we considered the problem of learning a classification model via gradient descent to solve an empirical risk minimization problem. We assumed that we had data, a pair \\((\\mathbf{X}, \\mathbf{y})\\) where\nUsing this data, we defined the empirical risk minimization problem, which had the general form \\[\n\\hat{\\mathbf{w}} = \\mathop{\\mathrm{argmin\\;}}_{\\mathbf{w}} \\; L(\\mathbf{w})\\;,\n\\tag{11.1}\\] where \\[\nL(\\mathbf{w}) = \\frac{1}{n} \\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, y_i)\\;.\n\\]\nIn our last lecture, we studied how to compute the gradient of \\(L(\\mathbf{w})\\) in minimize the empirical risk and find a good value \\(\\hat{\\mathbf{w}}\\) for the parameter vector. In this lecture we’re going to assume that we can cheerfully solve the empirical risk minimization for convex linear models.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Feature Maps, Regularization, and Generalization</span>"
    ]
  },
  {
    "objectID": "chapters/30-features-regularization.html#nonlinear-decision-boundaries",
    "href": "chapters/30-features-regularization.html#nonlinear-decision-boundaries",
    "title": "10  Feature Maps, Regularization, and Generalization",
    "section": "Nonlinear Decision Boundaries",
    "text": "Nonlinear Decision Boundaries\nHowever, we are still working with an important limitation: for a long time now, we’ve focused only on linear decision boundaries. However, most of the data we care about in practice has nonlinear decision boundaries. Here’s a dramatic example. For this example and throughout today, I’m using the implementation of logistic regression from scikit-learn. The purpose is to let you follow along while still giving you the chance to implement your own logistic regression for homework. Toward the end of the notes, I’ll highlight where regularization shows up if you want to implement it in e.g. a torch model. I’m also using the plot_decision_regions function from the mlxtend package, which is a handy plotting utility for visualizing the behavior of our models.\n\n\nCode\nfrom sklearn.datasets import make_moons, make_circles\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef viz_decision_regions(model, X_train, y_train, X_test, y_test):  \n    fig, ax = plt.subplots(1, 2, figsize = (8, 3.5), sharex = True, sharey = True)\n\n    for i, data in enumerate([\"Train\", \"Test\"]):\n        X = [X_train, X_test][i]\n        y = [y_train, y_test][i]\n        plot_decision_regions(X, y, clf = model, ax = ax[i])\n        score = ax[i].set_title(f\"{data}ing accuracy = {model.score(X, y)}\")\n        ax[i].set(xlabel = r\"$x_1$\")\n        if i == 0: \n            ax[i].set(ylabel = r\"$x_2$\")\n\ndef plot_weights(plr): \n    coefs        = plr.named_steps[\"LR\"].coef_.flatten()\n    frac_zero    = np.isclose(coefs, 0).mean()\n    mean_coef    = np.abs(coefs).mean()\n    coefs_sorted = np.sort(coefs)\n    plt.scatter(np.arange(len(coefs)), coefs_sorted, s = 10, facecolors = \"none\", edgecolors = \"black\")\n    plt.gca().set(xlabel = \"weight (sorted)\", ylabel = \"weight value\", title = fr\"Mean $|w_i|$ = {mean_coef:.2f}, zero weights = {100*frac_zero:.0f}%\")\n\n\nFirst, let’s create some training and testing data:\n\nX_train, y_train = make_circles(200, shuffle = True, noise = 0.1, factor = 0.5)\nX_test, y_test   = make_circles(200, shuffle = True, noise = 0.1, factor = 0.5)\n\nNow let’s fit and visualize the the decision regions learned on both the training and test sets.\n\nLR = LogisticRegression()\nLR.fit(X_train, y_train)\nviz_decision_regions(LR, X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\nYikes!\nVisually this should be pretty easy data to classify. But the linear decision boundary clearly isn’t the way.\n\n\n\n\n\n\nGiven a point \\(\\mathbf{x}\\), what information would you find most useful about that point in determining whether it should have label \\(0\\) or \\(1\\) based on this training data?",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Feature Maps, Regularization, and Generalization</span>"
    ]
  },
  {
    "objectID": "chapters/30-features-regularization.html#feature-maps",
    "href": "chapters/30-features-regularization.html#feature-maps",
    "title": "10  Feature Maps, Regularization, and Generalization",
    "section": "Feature Maps",
    "text": "Feature Maps\nSuppose that we were able to extract from each point its distance from the origin. In 2d, we could take a point \\(\\mathbf{x}\\) and simply compute\n\\[\nr^2 = x_1^2 + x_2^2\\;.\n\\]\nWe could then make the classification based on the value of \\(r^2\\). In this data set, it looks like the classification rule that predicts \\(1\\) if \\(r^2 &lt; 1\\) and \\(0\\) otherwise would be a pretty good one. The important insight here is that this is also a linear model, with linear predictor function\n\\[\n\\hat{y} = \\langle \\mathbf{w}, \\phi(\\mathbf{x}) \\rangle\\;,\n\\]\nand predicted labels \\(\\mathbb{1}[\\hat{y} &lt; 0]\\).\nwhere \\(\\phi(\\mathbf{x}) = (r^2, 1)\\) and \\(\\mathbf{w} = (1, -1)\\). We could attempt to find a value of the weight vector close to this one using empirical risk minimization using our standard methods. This means that we can use empirical risk minimization for this problem if we just transform the features \\(\\mathbf{X}\\) first! We need to compute a matrix \\(\\mathbf{R}\\) whose \\(i\\)th row is \\(\\mathbf{r}_i = \\phi(\\mathbf{x}) = (r^2_i, 1) = (x_{i1}^2 + x_{i2}^2, 1)\\), and then use this matrix in place of \\(\\mathbf{X}\\) for our classification task.\nThe transformation \\(\\phi: (x_1, x_2) \\mapsto (x_1^2 + x_2^2, 1)\\) is an example of a feature map.\n\n\n\n\n\n\n\nDefinition 11.1 Let \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times q}\\). A feature map \\(\\phi\\) is a function \\(\\phi:\\mathbb{R}^q \\rightarrow \\mathbb{R}^q\\), We call \\(\\phi(\\mathbf{x}) in \\mathbb{R}^p\\) the feature vector corresponding to \\(\\mathbf{x}\\). For a given feature map \\(\\phi\\), we define the map \\(\\Phi:\\mathbb{R}^{n \\times q} \\rightarrow \\mathbb{R}^{n\\times p}\\) as\n\\[\n\\Phi(\\mathbf{X}) = \\left(\\begin{matrix}\n     - & \\phi(\\mathbf{x}_1) & - \\\\\n     - & \\phi(\\mathbf{x}_2) & - \\\\\n     \\vdots & \\vdots & \\vdots \\\\\n     - & \\phi(\\mathbf{x}_n) & - \\\\\n\\end{matrix}\\right)\n\\]",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Feature Maps, Regularization, and Generalization</span>"
    ]
  },
  {
    "objectID": "chapters/30-features-regularization.html#feature-maps-and-linear-separability",
    "href": "chapters/30-features-regularization.html#feature-maps-and-linear-separability",
    "title": "10  Feature Maps, Regularization, and Generalization",
    "section": "Feature Maps and Linear Separability",
    "text": "Feature Maps and Linear Separability\nWe often think of feature maps as taking us from a space in which the data is not linearly separable to a space in which it is (perhaps approximately). For example, consider the feature map\n\\[\n\\phi: (x_1, x_2) \\mapsto (x_1^2, x_2^2)\\;.\n\\]\nThis map is sufficient to express the radius information, since we can represent the radius as\n\\[\nr^2 = \\langle (1, 1), (x_1^2, x_2^2) \\rangle\\;.\n\\]\nLet’s see how this looks. We’ll again show the failed linear separator, and we’ll also show a successful separator in a transformed feature space:\n\ndef phi(X): \n    return X**2\n\n\n\nCode\nfig, axarr = plt.subplots(1, 2, figsize=(8, 4))\n\nplot_decision_regions(X_train, y_train, clf = LR, ax = axarr[0])\nscore = axarr[0].set_title(f\"Accuracy = {LR.score(X_train, y_train)}\")\n\nX_ = phi(X_train)\nLR2 = LogisticRegression()\nLR2.fit(X_, y_train)\nplot_decision_regions(X_, y_train, clf = LR2, ax = axarr[1])\nscore = axarr[1].set_title(f\"Feature space\\nAccuracy = {LR2.score(X_, y_train)}\")\n\n\n\n\n\n\n\n\n\nJust by fitting the logistic regression model in the feature space, we were able to go from essentially random accuracy to accuracy of nearly 100% on training data.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Feature Maps, Regularization, and Generalization</span>"
    ]
  },
  {
    "objectID": "chapters/30-features-regularization.html#feature-maps-in-practice",
    "href": "chapters/30-features-regularization.html#feature-maps-in-practice",
    "title": "10  Feature Maps, Regularization, and Generalization",
    "section": "Feature Maps in Practice",
    "text": "Feature Maps in Practice\nGoing back to our example of trying to classify the two nested circles, we could just compute the radius. In practice, however, we don’t really know which features are going to be most useful, and so we just compute a set of features. In our case, the square of the radius is an example of a polynomial of degree 2: \\[\nr^2 = x_1^2 + x_2^2\\;.\n\\]\nInstead of just assuming that the radius is definitely the right thing to compute, we more frequently just compute all the monomials of degree 2 or lower. If \\(\\mathbf{x} = (x_1, x_2)\\), then the vector of all monomials of degree up to 2 is\n\\[\n\\phi(\\mathbf{x}_i) = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2)\\;.\n\\]\nWe then use a linear model to solve the empirical risk minimization problem\n\\[\n\\hat{\\mathbf{w}} = \\mathop{\\mathrm{argmin\\;}}_{w} \\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\phi(\\mathbf{x}_i) \\rangle, y_i)\\;.\n\\]\nThe important point to keep track of is that the new feature matrix \\(\\mathbf{X}' = \\Phi(\\mathbf{X})\\) generally has a different number of columns from \\(\\mathbf{X}\\). In this case, for example, \\(\\mathbf{X}\\) had just 2 columns but \\(\\Phi(\\mathbf{X})\\) has 6. This means that \\(\\hat{\\mathbf{w}}\\) has 6 components, instead of 2!\nLet’s now run logistic regression with degree-2 polynomial features on this data set. The most convenient way to make this happen in the scikit-learn framework is with at Pipeline. The Pipeline first applies the feature map and then calls the model during both fitting and evaluation. We’ll wrap the pipeline in a simple function for easy reuse.\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\ndef poly_LR(degree, **kwargs):\n    plr = Pipeline([(\"poly\", PolynomialFeatures(degree = degree)),\n                    (\"LR\", LogisticRegression(**kwargs))])\n    return plr\n\nNow our decision boundary is much more successful:\n\nplr = poly_LR(degree = 2)\nplr.fit(X_train, y_train)\nviz_decision_regions(plr, X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\nLet’s check the entries of the weight vector:\n\nplot_weights(plr)\n\n\n\n\n\n\n\n\nNotice that two coefficients are much larger in magnitude than the others, and approximately equal. These are the coefficients for the features \\(x_1^2\\) and \\(x_2^2\\). The fact that these are approximately equal means that our model is very close to using the square radius \\(r^2 = x_1^2 + x_2^2\\) as a learned feature for this data, just like we’d expect. The benefit is that we didn’t have to hard-code that in; the model just detected the right pattern to find.\nPart of the reason this might be beneficial is that for some data sets, we might not really know what specific features we should try. For example, here’s another one where a linear classifier doesn’t do so great (degree 1 corresponds to no transformation of the features). We’ll keep using this example as we go, and so we’ll generate both a training and a test set.\n\nnp.random.seed(123)\nX_train, y_train = make_moons(200, shuffle = True, noise = 0.3)\nX_test, y_test   = make_moons(200, shuffle = True, noise = 0.3)\n\nplr = poly_LR(degree = 1)\nplr.fit(X_train, y_train)\n\nviz_decision_regions(plr, X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\nIt’s not as obvious that we should use the radius or any other specific feature for our feature map. Fortunately we don’t need to think too much about it – we can just increase the degree and let the model figure things out:\n\nplr = poly_LR(degree = 5)\nplr.fit(X_train, y_train)\n\nviz_decision_regions(plr, X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\nMuch nicer!",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Feature Maps, Regularization, and Generalization</span>"
    ]
  },
  {
    "objectID": "chapters/30-features-regularization.html#generalization-feature-selection-regularization",
    "href": "chapters/30-features-regularization.html#generalization-feature-selection-regularization",
    "title": "10  Feature Maps, Regularization, and Generalization",
    "section": "Generalization, Feature Selection, Regularization",
    "text": "Generalization, Feature Selection, Regularization\nSo, why don’t we just use as many features as it takes to get perfect accuracy on the training data? As usual, this would lead to overfitting, as we can observe in the following example:\n\nplr = poly_LR(degree = 15, penalty = \"none\", max_iter = 1000000)\nplr.fit(X_train, y_train)\n\nviz_decision_regions(plr, X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\nI’ve had to change some parameters to the LogisticRegression in order to ensure that it fully ran the optimization procedure for this many polynomials.\nThe problem here is that, although this classifier might achieve perfect training accuracy, it doesn’t really look like it’s captured “the right” pattern. This is why this classifier makes many more mistakes on the test data, even though it had much higher training accuracy. We have overfit: our model was so flexible that it was able to learn both some real patterns that we wanted it to learn and some noise that we didn’t. As a result, when it made a prediction on new data, the model’s predictions were imperfect, reflecting the noise it learned in the training process.\nIn machine learning practice, we don’t actually want our models to get perfect scores on the training data – we want them to generalize to new instances of unseen data. Overfitting is one way in which a model can fail to generalize.\nLet’s do an experiment in which we see what happens to the model’s generalization ability when we increase the number of polynomial features:\n\n\nCode\nimport pandas as pd\nnp.random.seed()\n\ndegs = range(0, 11)\n\ndf = pd.DataFrame({\"deg\": [], \"train\" : [], \"test\" : []})\n\nfor rep in range(10):\n    X_train_, y_train_ = make_moons(100, shuffle = True, noise = .4)\n    X_test_,  y_test_  = make_moons(100, shuffle = True, noise = .4)\n\n    for deg in degs:\n        plr_ = poly_LR(degree = deg, penalty = \"none\", max_iter = int(1e3))\n        plr_.fit(X_train_, y_train_)\n        to_add = pd.DataFrame({\"deg\" : [deg],\n                               \"train\" : [plr_.score(X_train_, y_train_)],\n                               \"test\" : [plr_.score(X_test_, y_test_)]})\n\n        df = pd.concat((df, to_add))\n\n        \nmeans = df.groupby(\"deg\").mean().reset_index()\n\nplt.plot(means[\"deg\"], means[\"train\"], label = \"training\")\nplt.plot(means[\"deg\"], means[\"test\"], label = \"validation\")\nplt.legend()\nlabs = plt.gca().set(xlabel = \"Degree of polynomial feature\",\n              ylabel = \"Accuracy (mean over 20 runs)\")\n\n\n\n\n\n\n\n\n\nWe observe that there is an optimal number of features for which the model is most able to generalize: around 3 or so. More features than that is actually harmful to the model’s predictive performance on unseen data.\nSo, one way to promote generalization is to try to find “the right” or “the right number” of features and use them for prediction. This problem is often called feature selection and can be done with cross-validation.\nAnother common approach to avoid overfitting is called regularization. Regularization proceeds from the insight that wiggly decision boundaries often come from large entries in the weight vector \\(\\mathbf{w}\\). Let’s check this for the degree-15 polynomial features model that we trained previously:\n\nplot_weights(plr)\n\n\n\n\n\n\n\n\nYikes! Not only are there many weights, but many of them are extremely large.\nCan we fix this? In regularization, we add a term to the empirical risk objective function that encourages entries of \\(\\mathbf{w}\\) to be small. We consider the modified objective function \\[\nL'(\\mathbf{w}) = L(\\mathbf{w}) + \\lambda R(\\mathbf{w})\\;,\n\\]\nwhere \\(\\lambda\\) is a regularization strength and \\(R(\\mathbf{w})\\) is a regularization function that aims to shrink the entries of \\(\\mathbf{w}\\) in some way. Common choices of regularization function include the \\(\\ell_2\\) regularizer, which is simply the square Euclidean norm \\(R(\\mathbf{w}) = \\lVert \\mathbf{w} \\rVert_2^2 = \\sum_{j = 1}^p w_i^2\\), and the \\(\\ell_1\\) norm given by \\(R(\\mathbf{w}) = \\sum_{j = 1}^p \\lvert w_j \\rvert\\). To see regularization in action, let’s go back to our logistic regression model with a large number of polynomial features. We can see the presence of overfitting in the excessive “wiggliness” of the decision boundary. If \\(\\mathbf{x}\\) is defined in such a way that it has a constant column (e.g. \\(x_{in} = 1\\) for all \\(n\\)), then it is important not to regularize the entries of \\(\\mathbf{w}\\) that correspond to the constant column. This issue can be avoided by assuming that all the entries of the feature matrix \\(\\Phi(\\mathbf{X})\\) are column-centered, so that each column mean is zero. This can be achieved simply by defining \\(\\Phi\\) that way!\nFortunately for us, we can actually use regularization directly from inside the scikit-learn implementation of LogisticRegression. Below we specify the penalty (the \\(\\ell_2\\) regularization), the strength of the penalty (in the scikit-learn implementation, you specify \\(C = \\frac{1}{\\lambda}\\) so that larger \\(C\\) means less regularization) and the optimization solver (not all solvers work with all penalties).\nThis model did much better on the test data than the overfit model.\n\nplr = poly_LR(degree = 15, penalty = \"l2\", solver = \"lbfgs\", C = 10)\nplr.fit(X_train, y_train)\nviz_decision_regions(plr, X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\nOne reason for this success is that the entries of the weight vector are now much smaller:\n\nplot_weights(plr)\n\n\n\n\n\n\n\n\nIn fact, we can even force some of the coefficients of the weight vector to be exactly 0. This is achieved through the use of the \\(\\ell_1\\) regularization term \\(R(\\mathbf{w}) = \\sum_{j = 1}^p \\lvert w_j \\rvert\\).\n\nplr = poly_LR(degree = 15, penalty = \"l1\", solver = \"liblinear\", C = 1)\nplr.fit(X_train, y_train)\nviz_decision_regions(plr, X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\nHow do the weights look now?\n\nplot_weights(plr)\n\n\n\n\n\n\n\n\nThe benefit of having many weights exactly equal to zero is that it is not necessary to even compute the relevant features in order to make a prediction – we’re just going to multiply them by zero later! We often refer to the resulting choices of the parameter vector \\(\\mathbf{w}\\) as sparse, because most of their entries are zero. Sparsity plays a major role in modern machin elearning.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Feature Maps, Regularization, and Generalization</span>"
    ]
  },
  {
    "objectID": "chapters/30-features-regularization.html#gradients-of-regularizers",
    "href": "chapters/30-features-regularization.html#gradients-of-regularizers",
    "title": "10  Feature Maps, Regularization, and Generalization",
    "section": "Gradients of Regularizers",
    "text": "Gradients of Regularizers\nSuppose that we wish to solve the regularized empirical minimization problem with features:\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{w}} &= L'(\\mathbf{w}) \\\\\n              &= \\mathop{\\mathrm{argmin\\;}}_{\\mathbf{w}} L(\\mathbf{w}) + \\lambda R(\\mathbf{w}) \\\\\n              &= \\mathop{\\mathrm{argmin\\;}}_{\\mathbf{w}} \\frac{1}{n}\\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\phi(\\mathbf{x}_i) \\rangle) + \\lambda R(\\mathbf{w})\\;,\n\\end{aligned}\n\\]\nwhere \\(R(\\mathbf{w})\\) is the regularization term. The gradient of the regularization term. The gradient of this expression is\n\\[\n\\begin{aligned}\n    \\nabla L'(\\mathbf{w}) = \\nabla L(\\mathbf{w}) + \\lambda \\nabla R(\\mathbf{w})\\;.  \n\\end{aligned}\n\\] So, to compute the gradient of the regularized empirical risk, we just need the gradient of (a) the standard unregularized empirical risk and the regularization term. Here are two examples of gradients for the regularization term. Suppose that \\(w_i\\) is the coefficient of the constant feature in \\(\\mathbf{w}\\), and let \\(\\mathbf{w}_{-i}\\) be the vector of entries of \\(\\mathbf{w}\\) excluding \\(w_i\\).  Then, the gradients for the two most common regularization terms are given by the derivatives:Usually, data processing pipelines are set up so that \\(i = 1\\) or \\(i = n\\).\n\\[\n\\begin{aligned}\n    R(\\mathbf{w}) &= \\lVert \\mathbf{w}_{-i} \\rVert_2^2 = \\sum_{j \\neq i} w_j^2\\;, &\\quad \\frac{\\partial R(\\mathbf{w})}{\\partial w_j} &= \\begin{cases}\n        0 &\\quad j = i \\\\\n        2w_j  &\\quad j \\neq i \\end{cases} &\\quad \\text{($\\ell_2$ regularization)} \\\\\n    R(\\mathbf{w}) &= \\lVert \\mathbf{w}_{-i} \\rVert_{1} = \\sum_{j \\neq i} \\lvert w_j \\rvert\\;, &\\quad \\frac{\\partial R(\\mathbf{w})}{\\partial w_j} &= \\begin{cases}\n        0 &\\quad j = i \\\\\n        \\mathrm{sign}(w_j)  &\\quad j \\neq i, w_j \\neq 0 \\\\\n        0  &\\quad j \\neq i, w_j = 0\n         \\end{cases} &\\quad \\text{($\\ell_1$ regularization)}\n\\end{aligned}\n\\]\nTechnically, the derivative of \\(\\lvert w_j \\rvert\\) is not defined when \\(w_j = 0\\). It is ok to pretend that it is and equal to zero for the purposes of optimization due to the theory of subdifferentials.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Feature Maps, Regularization, and Generalization</span>"
    ]
  },
  {
    "objectID": "chapters/30-features-regularization.html#reflecting-on-empirical-risk-minimization",
    "href": "chapters/30-features-regularization.html#reflecting-on-empirical-risk-minimization",
    "title": "10  Feature Maps, Regularization, and Generalization",
    "section": "Reflecting on Empirical Risk Minimization",
    "text": "Reflecting on Empirical Risk Minimization\nWe have now introduced all the fundamental features of modern empirical risk minimization for training machine learning models. We aim to find a weight vector \\(\\hat{\\mathbf{w}}\\) that solves the problem\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{w}} = \\mathop{\\mathrm{argmin\\;}}_{\\mathbf{w}} \\frac{1}{n} \\sum_{i = 1}^n \\ell (\\langle \\mathbf{w}, \\phi(\\mathbf{x}_i) \\rangle, y_i) + \\lambda R(\\mathbf{w})\\;.\n\\end{aligned}\n\\tag{11.2}\\]\nUntil roughly 2005 or so, Equation 11.1 was the state of the art for a wide array of classification and regression problems. Common questions would include:\n\nWhat loss functions \\(\\ell\\) should be used for model scoring?\nWhat feature maps \\(\\phi\\) should be used for extracting useful features from the data?\nWhat regularization terms should be used to guard against overfitting?\n\nIt’s not the case that all of machine learning fit into this framework; important supervised techniques that don’t fall into this category include probabilistic machine learning and tree-based methods like decision trees and random forests.\nWe’ll soon study two ways to move past this paradigm: kernel methods and deep learning.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Feature Maps, Regularization, and Generalization</span>"
    ]
  },
  {
    "objectID": "chapters/40-linear-regression.html",
    "href": "chapters/40-linear-regression.html",
    "title": "11  Linear Regression",
    "section": "",
    "text": "Linear Regression as Empirical Risk Minimization\nLast time, we studied the most general form of the empirical risk minimization problem:\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{aligned}\n    \\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}} L(\\mathbf{w}) = \\argmin_{\\mathbf{w}} \\frac{1}{n} \\sum_{i = 1}^n \\ell (\\langle\\mathbf{w}, \\phi(\\mathbf{x}_i)\\rangle, y_i) + \\lambda R(\\mathbf{w})\\;.\n\\end{aligned}\n\\tag{11.1}\\]\nHere, \\(\\ell\\) is the per-observation loss function, \\(\\phi\\) is a feature map, \\(R(\\mathbf{w})\\) is a regularizing term. When studying convexity, we introduced several different choices of \\(\\ell\\), including the 0-1 loss, logistic loss, and hinge loss.\nDoing linear regression is as simple as choosing a different loss function. The most common choice is the square loss:\n\\[\n\\begin{aligned}\n    \\ell(s, y) = (s - y)^2\\;.\n\\end{aligned}\n\\]\nWith this choice, empirical risk minimization becomes least-squares linear regression, with loss function\n\\[\n\\begin{aligned}\n    L(\\mathbf{w}) = \\underbrace{\\frac{1}{n} \\sum_{i = 1}^n (\\langle\\mathbf{w}, \\phi(\\mathbf{x}_i)\\rangle - y_i)^2}_{\\text{mean-squared error}} + \\lambda R(\\mathbf{w})\\;.\n\\end{aligned}\n\\tag{11.2}\\]\nThe first term in this expression is the mean-squared error (MSE). Motivation via the MSE is the most common way that least-squares linear regression is motivated in statistics courses.\nOne can use the second-derivative test to check that the square loss is convex in \\(s\\), which means that all our standard theory from convex risk minimization translates to this setting as well. Gradient descent is one good method to learn the model and find \\(\\hat{\\mathbf{w}}\\), although there are many other good ways as well.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/40-linear-regression.html#linear-regression-as-empirical-risk-minimization",
    "href": "chapters/40-linear-regression.html#linear-regression-as-empirical-risk-minimization",
    "title": "11  Linear Regression",
    "section": "",
    "text": "Check that this function is convex as a function of \\(s\\)!",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/40-linear-regression.html#matrix-vector-formulation",
    "href": "chapters/40-linear-regression.html#matrix-vector-formulation",
    "title": "11  Linear Regression",
    "section": "Matrix-Vector Formulation",
    "text": "Matrix-Vector Formulation\nIt is possible to write Equation 11.2 much more simply using matrix-vector notation: Here, \\(\\lVert \\mathbv{v} \\rVert_2^2 = \\sum_{i}v_i^2\\) is the squared Euclidean norm.\n\\[\n\\begin{aligned}\n    L(\\mathbf{w}) = \\frac{1}{n} \\lVert\\phi(\\mathbf{X})\\mathbf{w} - \\mathbf{y}\\rVert_2^2 + \\lambda R(\\mathbf{w})\\;.\n\\end{aligned}\n\\tag{11.3}\\]\nBy rules of multivariable differentiation, the gradient of the empirical risk \\(L(\\mathbf{w})\\) is We looked at some gradients for the regularization term \\(R(\\mathbf{w})\\) in previous notes.\n\\[\n\\nabla L(\\mathbf{w}) = \\frac{2}{n}\\phi(\\mathbf{X})^T(\\phi(\\mathbf{X})\\mathbf{w} - \\mathbf{y}) + \\lambda \\nabla R(\\mathbf{w})\\;.\n\\tag{11.4}\\]\nUsing this gradient for a gradient-descent scheme would be a perfectly reasonable way to go about solving a least-squares linear regression problem.\n\nRidge Regression\nSuppose for a moment that we choose \\(R(\\mathbf{w}) = \\lVert \\mathbf{w} \\rVert_2^2\\), the \\(\\ell_2\\) or ridge penalty. [When using this regularizing term, we usually need to assume that, for each feature \\(j\\), \\(\\sum_{i = 1}^n\\phi_{ij}(\\mathbf{X}) = 0\\). This is called column centering and can be achieved simply by subtracting the column mean from each entry of \\(\\mathbf{X}\\). The reason we need to do this is that it guarantees that the weight of the constant feature is \\(0\\), which ensures that it’s not a problem for us to include it in the regularization term \\(R(\\mathbf{w})\\). The function regression_data from above always produces column-centered data.] In order to make the math work out nicely, it’s convenient to assume that \\(\\lambda = \\Lambda / n\\) for some \\(\\Lambda\\).\nIn this case, the empirical risk objective function is\n\\[\n\\begin{aligned}\n    L(\\mathbf{w}) &= \\frac{1}{n}\\left[ \\lVert \\Phi(\\mathbf{X}) \\mathbf{w} - \\mathbf{y} \\rVert_2^2 + \\Lambda \\lVert \\mathbf{w}\\rVert_2^2 \\right] \\\\\n           &= \\underbrace{\\frac{1}{n} \\sum_{i = 1}^n (\\langle\\mathbf{w}, \\phi(\\mathbf{x}_i)\\rangle - y_i)^2} + \\frac{\\Lambda}{n} \\sum_{j = 1}^p w_{j}^2  \\;.\n\\end{aligned}\n\\]\nThe gradient of the loss function is \\[\n\\nabla L(\\mathbf{w}) = \\frac{2}{n}\\phi(\\mathbf{X})^T(\\phi(\\mathbf{X})\\mathbf{w} - \\mathbf{y}) + 2\\frac{\\Lambda}{n} \\mathbf{w}\\;.\n\\tag{11.5}\\]\nUnusually in this class, we can actually find the minimum of \\(L(\\mathbf{w})\\) using the first-derivative test: we set \\(\\nabla L(\\hat{\\mathbf{w}}) = \\mathbf{0}\\) and solve for \\(\\hat{\\mathbf{w}}\\). This equation is:\n\\[\n\\mathbf{0} = \\frac{2}{n}\\phi(\\mathbf{X})^T(\\phi(\\mathbf{X})\\mathbf{w} - \\mathbf{y}) + 2\\frac{\\Lambda}{n}  \\mathbf{w}\\;.\n\\]\nWe can cancel the \\(\\frac{2}{n}\\)s and move everything that depends on \\(\\mathbf{w}\\) to one side of the equation:\n\\[\n- \\phi(\\mathbf{X})^T\\phi(\\mathbf{X})\\hat{\\mathbf{w}} - \\Lambda \\hat{\\mathbf{w}}  = -\\phi(\\mathbf{X})^T\\mathbf{y} \\;.\n\\]\nor\n\\[\n\\left[\\phi(\\mathbf{X})^T\\phi(\\mathbf{X}) + \\Lambda \\mathbf{I} \\right]\\hat{\\mathbf{w}}  = \\phi(\\mathbf{X})^T\\mathbf{y} \\;.\n\\]\nThis is a matrix-vector equation of the familiar form \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\), where \\(\\mathbf{A} = \\phi(\\mathbf{X})^T\\phi(\\mathbf{X}) + \\Lambda \\mathbf{I} \\in \\mathbb{R}^{p \\times p}\\), \\(\\mathbf{x} = \\mathbf{w} \\in \\mathbb{R}^{p}\\), and \\(\\mathbf{b} = \\phi(\\mathbf{X})^T\\mathbf{y} \\in \\mathbb{R}^p\\).\nYou may remember from linear algebra that this equation has exactly one solution *provided that the matrix \\(\\mathbf{A} = \\phi(\\mathbf{X})^T\\phi(\\mathbf{X}) + \\Lambda \\mathbf{I}\\) has full rank (\\(p\\) linearly independent rows/columns), which implies that it is invertible. This is guaranteed to be the case provided that \\(\\Lambda &gt; 0\\). So, we can invert the matrix and find that the optimal choice of \\(\\hat{vw}\\) is\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{w}} = \\left[\\phi(\\mathbf{X})^T\\phi(\\mathbf{X}) + \\Lambda \\mathbf{I} \\right]^{-1} \\phi(\\mathbf{X})^T\\mathbf{y}\\;.\n\\end{aligned}\n\\tag{11.6}\\]\nLet’s see this formula in action. We’ll start by fitting a model to the linear trend that we saw in the beginning of these notes:\n\nplot_regression_data(X, y)\n\n\n\n\n\n\n\n\nNow we can implement Equation 11.6.\n\ndef ridge_regression(X, y, lam = 1.0):\n    p = X.size(1)\n    w_hat = torch.inverse(X.T@X + lam*torch.eye(p))@X.T@y\n    return w_hat\n\nTo assess the quality of our fit, let’s measure the MSE (the unregularized term of the empirical risk):\n\ndef mse(X, y, w):\n    return ((X@w - y)**2).mean()\n\nTraining our model is as simple as calling the function we implemented.\n\nw_hat = ridge_regression(X, y, lam = 1.0)\nplot_regression_data(X, y, w = w_hat, title = f\"MSE = {mse(X, y, w_hat):.4f}\")\n\n\n\n\n\n\n\n\nHow did we do on the test data?\n\nX_test, y_test = regression_data(100, w)\nplot_regression_data(X_test, y_test, w = w_hat, title = f\"MSE = {mse(X_test, y_test, w_hat):.4f}\")\n\n\n\n\n\n\n\n\nNot bad! The MSE is similar on the training and testing data, suggesting that we have not overfit.\nThe effect of increasing \\(\\Lambda\\) is to reduce the values of the entries \\(\\hat{\\mathbf{w}}\\). Choosing \\(\\Lambda\\) to be too large can lead to undesirable fits:\n\nw_hat = ridge_regression(X, y, lam = 100.0)\nplot_regression_data(X, y, w = w_hat, title = f\"MSE = {mse(X, y, w_hat):.4f}\")",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/40-linear-regression.html#nonlinear-feature-maps",
    "href": "chapters/40-linear-regression.html#nonlinear-feature-maps",
    "title": "11  Linear Regression",
    "section": "Nonlinear Feature Maps",
    "text": "Nonlinear Feature Maps\nSuppose now that we want to model data with a nonlinear trend:\n\nX, y = regression_data(100, w, x_max=2*torch.pi, phi = torch.sin)\nplot_regression_data(X, y, w, phi = torch.sin)\n\n\n\n\n\n\n\n\nAs usual, we can apply a nonlinear feature map in order to be able to model this nonlinear pattern using techniques from convex linear models. First we’ll implement a feature map for our data:\n\nfrom sklearn.preprocessing import PolynomialFeatures\npf = PolynomialFeatures(degree = 5)\nphi = lambda X: torch.Tensor(pf.fit_transform(X))\n\nAfter computing the matrix of features, we can use our some ridge-regression solver from before:\n\nPHI = phi(X)\nw_hat = ridge_regression(PHI, y, lam = 1)\nplot_regression_data(X, y, w = w_hat, phi = phi, title = f\"MSE = {mse(phi(X), y, w_hat):.4f}\")\n\n\n\n\n\n\n\n\nThis looks like it works fine! Choosing polynomial features of too high a degree leads to problems at the boundaries, though:\n\npf = PolynomialFeatures(degree = 15)\nphi = lambda X: torch.Tensor(pf.fit_transform(X))\nPHI = phi(X)\nw_hat = ridge_regression(PHI, y, lam = 1)\nplot_regression_data(X, y, w = w_hat, phi = phi, title = f\"MSE = {mse(phi(X), y, w_hat):.4f}\")\n\n\n\n\n\n\n\n\nAlthough we can try to address this problem with further regularization, the results can be hard to predict:\n\nw_hat = ridge_regression(PHI, y, lam = 100)\nplot_regression_data(X, y, w = w_hat, phi = phi, title = f\"MSE = {mse(phi(X), y, w_hat):.4f}\")\n\n\n\n\n\n\n\n\nHmmm, is that better?\nHere’s a more systematic sweep in which we vary the both the regularization strength \\(\\lambda\\) and the degree of the polynomial features \\(d\\). We fit the model using our ridge regression function above, and then evaluate the MSE on a test set.\n\n\nCode\n# test set\nX_test, y_test = regression_data(100, w, x_max=2*torch.pi, phi = torch.sin)\n\ndegrees = [1, 2, 5, 10]\nLAM     = [0.01, 0.1, 1, 10, 100]\n\nfig, ax = plt.subplots(4, 4, figsize = (10, 8))\n\n# main loop\nfor i in range(4):\n    for j in range(4): \n        pf = PolynomialFeatures(degree = degrees[i])\n        phi = lambda X: torch.Tensor(pf.fit_transform(X))\n        PHI = phi(X)\n        w_hat = ridge_regression(PHI, y, lam = LAM[j])\n        plot_regression_data(X_test, y_test, w = w_hat, phi = phi, ax = ax[i, j], legend = False, xlabel = i == 3, ylabel = j == 0)\n        ax[i,j].set(title = fr\"$d = ${degrees[i]}, $\\Lambda = ${LAM[j]}, MSE = {mse(phi(X_test), y_test, w_hat):.4f}\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nWe observe that there is a “happy medium” for the choice of polynomial degree and regularization strength: degree-5 polynomial features with light regularization seem to perform best on unseen data. In most applications, these hyperparameters are selected using cross-validation.\nRegression with polynomial features is not usually advised due to the strange behavior of the predictors at the boundary of the domain, which is on display in several of the plots above. We’ll soon study kernel methods, which offer a preferable alternative to polynomial regression.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/40-linear-regression.html#computational-complexity",
    "href": "chapters/40-linear-regression.html#computational-complexity",
    "title": "11  Linear Regression",
    "section": "Computational Complexity",
    "text": "Computational Complexity\nIs that all there is to least-squares linear regression? Of course not!\n\nOther Regularizers\nNot if we use different regularization terms! An especially popular regularizer is the \\(\\ell_1\\) regularizer that we discussed previous. If we use this regularizer in addition to or instead of the \\(\\ell_2\\) regularization term, then we can’t use the closed-form matrix formula above.\n\n\nKernels\nUsing polynomial feature maps is not always ideal due to poor behavior at the data boundaries, but how else can we model nonlinearities in our data? We’ll soon learn how to use kernels to introduce manageable nonlinearities.\n\n\nGradient Methods\nMore fundamentally, suppose that we have a very large number \\(p\\) of features. The matrix \\(\\Phi(\\mathbf{X})^T\\Phi(\\mathbf{X}) + \\lambda \\mathbf{I}\\) is a \\(p\\times p\\) matrix. The computational cost of inverting this matrix is \\(\\Theta(n^\\gamma)\\) for some \\(\\gamma\\) between \\(2\\) and \\(3\\). For sufficiently large \\(p\\), this may simply be infeasible. There are several approaches.\nTo perform gradient descent with least-squares linear regression, all we need is a formula for the gradient. Equation 11.5 gives this formula – we just plug in the gradient of the regularizing term and iterate to convergence.\nSometimes even this is too hard: for sufficiently large \\(p\\), even computing the matrix multiplication \\(\\Phi(\\mathbf{X})^T\\Phi(\\mathbf{X})\\) required for gradient descent is too computationally intensive. In this case, stochastic gradient methods can be used; we’ll study these in a coming lecture.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/50-kernel-methods.html",
    "href": "chapters/50-kernel-methods.html",
    "title": "12  Kernel Methods",
    "section": "",
    "text": "\\(\\hat{\\mathbf{w}}\\) Lies in the Span of the Data\nLet’s start with a theorem about a broad class of linear models. Recall that we are working with the empirical risk minimization problem\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{aligned}\n\\hat{\\mathbf{w}} &= \\argmin_{\\mathbf{w}} L(\\mathbf{w}) \\\\\n&= \\argmin_{\\mathbf{w}} \\frac{1}{n}\\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\phi(\\mathbf{x}_i)\\rangle, y_i) + \\lambda R(\\mathbf{w})\\;,\n\\end{aligned}\n\\tag{12.1}\\]\nwhere \\(R(\\mathbf{w})\\) is a regularizer such as the \\(\\ell_1\\) or \\(\\ell_2\\) norm.\nThis theorem can be proven with fundamental linear algebraic methods. We can also prove it for a special case using gradient descent when \\(R\\) is the \\(\\ell_2\\) penalty \\(R(\\mathbf{w}) = \\lVert \\mathbf{w} \\rVert\\). Assume that the learning rate \\(\\alpha\\) is small enough that gradient descent is guaranteed to converge, and that the feature matrix \\(\\Phi(\\mathbf{x})\\) is centered.The centering assumption allows us to regularize \\(\\mathbf{w}\\) without worrying about a constant feature; centering guarantees that the corresponding entry of \\(\\mathbf{w}\\) will be zero. Suppose that we initialize gradient descent with initial iterate \\(\\mathbf{w}^{(0)} = \\mathbf{0}\\). Define \\(b_i^{(j)} = -\\frac{\\alpha}{n}\\frac{d\\ell(\\langle \\mathbf{w}^{(j)}, \\phi(\\mathbf{x}_i) \\rangle, y_i)}{ds}\\) and \\(a_{i}^{(j)} = (1 + 2\\alpha)^{}\\) Then, using our formulae for the gradient of the empirical risk from Equation 9.4, we have\n\\[\n\\begin{aligned}\n    \\mathbf{w}^{(1)} &= \\mathbf{0} - \\frac{\\alpha}{n} \\sum_{i = 1}^n  \\frac{d\\ell(\\langle \\mathbf{w}^{(0)}, \\phi(\\mathbf{x}_i) \\rangle, y_i)}{ds} \\phi(\\mathbf{x}_i) + 2\\alpha \\mathbf{w}^{(0)}\\\\\n    &= \\sum_{i = 1}^n b_i^{(0)}\\phi(\\mathbf{x}_i)\\;.\n\\end{aligned}\n\\]\nThus, \\(\\mathbf{w}^{(1)}\\) is a linear combination of feature vectors $(_i)$.\nThe next iteration is\n\\[\n\\begin{aligned}\n     \\mathbf{w}^{(2)} &= \\mathbf{w}^{(1)} - \\frac{\\alpha}{n} \\sum_{i = 1}^n  \\frac{d\\ell(\\langle \\mathbf{w}^{(1)}, \\phi(\\mathbf{x}_i) \\rangle, y_i)}{ds}\\phi(\\mathbf{x}_i)  - 2\\alpha \\mathbf{w}^{(1)}\\\\\n     &= \\sum_{i = 1}^n (1 - 2\\alpha)b_i^{(0)}\\phi(\\mathbf{x}_i) - \\sum_{i = 1}^n b_i^{(1)}\\phi(\\mathbf{x}_i) \\\\\n     &= \\sum_{i = 1}^n \\left( (1 - 2\\alpha) b_i^{(0)} + b_i^{(1)}\\right)\\phi(\\mathbf{x}_i)\\;,\n\\end{aligned}\n\\]\nwhich is again a linear combination of feature vectors. We can continue in this fashion inductively, proving that\n\\[\n\\begin{aligned}\n    \\mathbf{w}^{(t)} = \\sum_{i = 1}^n a_{i}^{(t)} \\phi(\\mathbf{x}_i)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{w}} &= \\sum_{i = 1}^n \\hat{a}_i \\phi(\\mathbf{x}_i) \\\\\n                     &= \\Phi(\\mathbf{X})^T\\hat{\\mathbf{a}}\\;,\n\\end{aligned}\n\\]\nas was to be shown. In the last line, we have used the definition of matrix multiplication.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kernel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/50-kernel-methods.html#hatmathbfw-lies-in-the-span-of-the-data",
    "href": "chapters/50-kernel-methods.html#hatmathbfw-lies-in-the-span-of-the-data",
    "title": "12  Kernel Methods",
    "section": "",
    "text": "Theorem 12.1 (\\(\\hat{\\mathbf{w}}\\) is spanned by the predictor data) Let \\(\\hat{\\mathbf{w}}\\) be the solution to Equation 12.1. Then, there exists a vector \\(\\hat{\\mathbf{a}} \\in \\mathbb{R}^n\\) such that \\(\\hat{\\mathbf{w}} = \\sum_{i = 1}^{n} \\hat{a}_{i} \\phi(\\mathbf{x}_i)  = \\Phi(\\mathbf{X})^T\\hat{\\mathbf{a}}\\).\n\n\n\n\n\n\n\n\n\n\n\nfor some constants \\(\\hat{a}_i^{(t)}\\). Continuing in this fashion and passing to the limit \\(\\hat{a}_{i}^{(t)} \\rightarrow \\hat{a}_i\\) as \\(t\\rightarrow \\infty\\) ,In our argument above, \\(a_{i}^{(1)} = b_i^{(0)}\\) and \\(\\hat{a}_{i}^{(2)} = (1 - 2\\alpha) b_i^{(0)} + b_i^{(1)}\\).Passing to the limit is justified because we assume \\(\\alpha\\) to be small enough that gradient descent converges",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kernel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/50-kernel-methods.html#the-kernel-matrix",
    "href": "chapters/50-kernel-methods.html#the-kernel-matrix",
    "title": "12  Kernel Methods",
    "section": "The Kernel Matrix",
    "text": "The Kernel Matrix\nWhat does Theorem 12.1 get us? To see what it gets us, let’s use ridge regression as a running example. Ignoring factors of \\(\\frac{1}{n}\\) for convenience, the ridge-regression objective function is\n\\[\n\\begin{aligned}\n    L(\\mathbf{w}) &= \\lVert \\Phi(\\mathbf{X})\\mathbf{w} - \\mathbf{y} \\rVert_2^2 + \\lambda \\lVert \\mathbf{w} \\rVert_2^2\\;. \\\\\n    &= \\lVert \\Phi(\\mathbf{X})\\mathbf{w} - \\mathbf{y} \\rVert_2^2 + \\lambda \\mathbf{w}^T\\mathbf{w}\\;.\n\\end{aligned}\n\\]\nLet’s plug in the equation \\(\\mathbf{w} = \\Phi(\\mathbf{X})^T\\mathbf{a}\\) from Theorem 12.1. This allows us to re-express the loss as a function of the vector \\(\\mathbf{a}\\).\n\\[\n\\begin{aligned}\n    L(\\mathbf{a}) &= \\lVert\\Phi(\\mathbf{X})\\Phi(\\mathbf{X})^T\\mathbf{a} - \\mathbf{y}\\rVert_2^2+ \\lambda \\mathbf{a}^T\\Phi(\\mathbf{X})\\Phi(\\mathbf{X})^T\\mathbf{a}\\;.\n\\end{aligned}\n\\]\nLet’s define the kernel matrix \\(\\mathbf{K} = \\Phi(\\mathbf{X})\\Phi(\\mathbf{X}) ^T \\in \\mathbb{R}^{n\\times n}\\). The entries of \\(\\mathbf{K}\\) are \\(k_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j) = \\langle \\phi(\\mathbf{x}_i), \\phi(\\mathbf{x}_j) \\rangle\\). By construction, \\(\\mathbf{K}\\) is symmetric: \\(\\mathbf{K} = \\mathbf{K}^T\\).\nThen,\n\\[\n\\begin{aligned}\n    L(\\mathbf{a}) &= \\lVert\\mathbf{K}\\mathbf{a} - \\mathbf{y}\\rVert_2^2 + \\lambda \\mathbf{a}^T\\mathbf{K}\\mathbf{a}\\;.\n\\end{aligned}\n\\]\nThis looks quite similar to our old ridge-regression problem. Let’s again take the gradient, this time with respect to \\(\\mathbf{a}\\), and solve for the optimal choice \\(\\hat{\\mathbf{a}}\\):\n\\[\n\\begin{aligned}\n    \\nabla L(\\mathbf{a}) &= 2\\mathbf{K}(\\mathbf{K}\\mathbf{a} - \\mathbf{y}) + 2\\lambda K\\mathbf{a} \\\\\n    &= 2\\mathbf{K}(\\mathbf{K} + \\lambda\\mathbf{I})\\mathbf{a} - 2\\mathbf{K}\\mathbf{y}\\;.\n\\end{aligned}\n\\]\nSetting the equation \\(\\nabla L(\\hat{\\mathbf{a}}) = \\mathbf{0}\\) and solving for \\(\\hat{\\mathbf{a}}\\) yields the formula\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{a}} = (\\mathbf{K} + \\lambda \\mathbf{I})^{-1}\\mathbf{y}\\;.\n\\end{aligned}\n\\tag{12.2}\\]\nOnce we have the optimal \\(\\hat{\\mathbf{a}}\\), we could if we wanted convert back to the optimal \\(\\hat{\\mathbf{w}}\\) using Theorem 12.1.\nTo make a prediction at a data point \\(\\mathbf{x}\\), we compute\n\\[\n\\begin{aligned}\ns &= \\langle \\phi(\\mathbf{x}), \\hat{\\mathbf{w}} \\rangle \\\\\n  &= \\langle \\phi(\\mathbf{x}), \\Phi(\\mathbf{X})^T\\hat{\\mathbf{a}}\\rangle \\\\\n  &= \\sum_{i = 1}^n \\hat{a}_i k(\\mathbf{x}, \\mathbf{x}_i)\\;,\n\\end{aligned}\n\\tag{12.3}\\]\nwhere again each \\(k(\\mathbf{x}, \\mathbf{x}_i) = \\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{x}_i) \\rangle\\).\nSo far, this is all just different notation for the same ridge regression that we talked about previously. What’s the point?",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kernel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/50-kernel-methods.html#the-kernel-trick",
    "href": "chapters/50-kernel-methods.html#the-kernel-trick",
    "title": "12  Kernel Methods",
    "section": "The Kernel Trick",
    "text": "The Kernel Trick\nLet’s remind ourselves: where is the feature matrix \\(\\Phi(\\mathbf{X})\\) in Equation 12.2? It appears only through the kernel matrix \\(\\mathbf{K} = \\Phi(\\mathbf{X})\\Phi(\\mathbf{X})^T\\) with entries \\(k_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j) = \\langle \\phi(\\mathbf{x}_i), \\phi(\\mathbf{x}_j) \\rangle\\). This means that as long as we know how to calculate these inner products, we never actually have to explicitly compute any of the features \\(\\phi(\\mathbf{x})\\). This idea, though simple, is so fundamental that it has an informal name – it’s the “kernel trick.”\n\n\n\n\n\n\nKernel Trick\n\n\n\nSuppose that we can find some function \\(k:\\mathbb{R}^p\\times \\mathbb{R}^p \\rightarrow \\mathbb{R}\\) and some feature map \\(\\phi\\) with the property that \\(k(\\mathbf{x}_i, \\mathbf{x}_j) = \\langle \\phi(\\mathbf{x}_i), \\phi(\\mathbf{x}_j) \\rangle\\). Then, computing Equation 12.2 and making predictions via Equation 12.3 is equivalent to doing regular ridge regression with feature map \\(\\phi\\).",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kernel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/50-kernel-methods.html#positive-definite-kernels",
    "href": "chapters/50-kernel-methods.html#positive-definite-kernels",
    "title": "12  Kernel Methods",
    "section": "Positive-Definite Kernels",
    "text": "Positive-Definite Kernels\nWhen does a candidate function \\(k\\) have the property that \\(k(\\mathbf{x}_i, \\mathbf{x}_j) = \\langle \\phi(\\mathbf{x}_i), \\phi(\\mathbf{x}_j) \\rangle\\) for some \\(\\phi\\)? The fundamental property for which we can check is called positive-definiteness:\n\n\n\n\n\n\nPositive-Definite Kernel\n\n\n\nA matrix \\(\\mathbf{K} \\in \\mathbb{R}^{n\\times n}\\) is positive definite if, for any vector \\(\\mathbf{z} \\in \\mathbb{R}^n\\), \\(\\mathbf{z}^T\\mathbf{K}\\mathbf{z} &gt; 0\\).\nA function \\(k:\\mathbb{R}^p \\times \\mathbb{R}^p \\rightarrow \\mathbb{R}\\) is a positive definite kernel if, for any matrix \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\), the matrix \\(\\mathbf{K} \\in \\mathbb{R}^{n\\times n}\\) with entries \\(k_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\\) is a positive definite matrix.\n\n\nThere are a number of rules for deriving examples of positive-definite kernels, as well as some famous examples (for a fairly complete guide, see Section 6.2 of Bishop (2006)).\nIn fact, it’s not even necessary that \\(k\\) give inner products for finite feature spaces! For example, the Gaussian radial basis function (RBF) kernel has formula\n\\[\n\\begin{aligned}\n    k(\\mathbf{x}_i, \\mathbf{x}_j) = e^{-\\gamma\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert^2_2}\\;.\n\\end{aligned}\n\\]\nHere, \\(\\gamma\\) is a tunable parameter called the bandwidth. It may not be obvious that this is a valid positive-definite kernel, but you can check Bishop (2006) for an argument that it is. The RBF kernel does not correspond to any finite-dimensional feature space – it actually gives the inner product for an infinite-dimensional vector space.  So, if we can calculate the RBF kernel, then we can use infinitely many features with finite compute time.Technically, it describes the inner product in a certain Hilbert space.\nHere’s an implementation of the RBF kernel:\n\n\nCode\nimport torch\nfrom matplotlib import pyplot as plt\n\nfrom itertools import product\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\n\ndef rbf_kernel(X_1, X_2, gamma):\n    return torch.exp(-gamma*torch.cdist(X_1, X_2)**2)\n\nHere is a visualization of the RBF kernel for an array of values of \\(x_1\\) and \\(x_2\\).\n\n\nCode\nx1 = torch.linspace(0, 1, 101)[:, None]\nx2 = torch.linspace(0, 1, 101)[:, None]\n\nplt.imshow(rbf_kernel(x1, x2, 10), cmap = \"inferno\", extent = [0, 1, 0, 1])\nplt.gca().set(xlabel = r\"$x_2$\", ylabel = r\"$x_1$\")\n\n\n\n\n\n\n\n\n\nThere are many other kernels, and for a long time the engineering of new kernels for different machine learning domain areas was at the forefront of research.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kernel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/50-kernel-methods.html#implementing-kernel-ridge-regression",
    "href": "chapters/50-kernel-methods.html#implementing-kernel-ridge-regression",
    "title": "12  Kernel Methods",
    "section": "Implementing Kernel Ridge Regression",
    "text": "Implementing Kernel Ridge Regression\nLet’s now implement kernel ridge regression from Equation 12.2. Here’s some data with a nonlinear trend.\n\n\nCode\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef regression_data(n = 100, w = torch.Tensor([-0.7, 0.5]), phi = lambda x: x, x_max = 1):\n\n    X = torch.zeros(n, 2)\n    x = torch.rand(n)*x_max\n    x, ix = torch.sort(x)\n    X[:, 1] = x\n    X[:, 0] = torch.ones(n)\n\n    X = X - X.mean(dim = 0,keepdims = True)\n\n    y = phi(X)@w + 0.05*torch.randn(n)\n    y = y - y.mean()\n    return X, y\n\n\ndef plot_regression_data(X, y, ax = None, legend = True, xlabel = True, ylabel = True, title = None):\n    \n    if ax is None: \n        fig, ax = plt.subplots(1, 1, figsize = (6, 4))\n\n    if xlabel: \n        labels = ax.set(xlabel = \"Feature (x)\")\n    if ylabel: \n        labels = ax.set(ylabel = \"Target (y)\")\n    if title: \n        t = ax.set(title = title)\n\n\n    ax.scatter(X[:,1], y, facecolors = \"none\", edgecolors = \"steelblue\", label = \"data\")\n    if w is not None: \n        m_points = 1001\n        X_ = torch.zeros(m_points, 2)\n        x = torch.linspace(X[:,1].min().item()-0.01, X[:,1].max().item()+0.01, m_points)\n        X_[:,1] = x\n        X_[:, 0] = torch.ones(m_points)\n        X_ = X_ - X_.mean(dim = 0,keepdims = True)\n\n\n\nw = torch.Tensor([0.4, 0.5])\n\nX, y = regression_data(w = w, phi = torch.sin, x_max = 4*torch.pi)\nplot_regression_data(X, y)\n\n\n\n\n\n\n\n\nNow we’ll implement kernelized ridge regression. We’ll implement a class to hold instance variables and implement the mathematics described by Equation 12.2 and Equation 12.3. Since we don’t need to do any gradient descent with this class, we’ll implement it using the standard sklearn API.\n\nclass KernelRidgeRegression:\n\n    def __init__(self, kernel, lam, **kwargs):\n        self.lam    = lam     # regularization strength\n        self.kernel = kernel  # kernel used\n        self.kwargs = kwargs  # keyword arguments passed to kernel\n\n    def fit(self, X, y):\n        \"\"\"\n        implements eq. 12.2\n        Also saves the training data, since this is needed for prediction\n        \"\"\"\n        n = X.size(0) if isinstance(X, torch.Tensor) else len(X)\n        I = torch.eye(n)\n        \n                # compute the kernel matrix\n        K = self.kernel(X, X, **self.kwargs)                       \n\n        # perform the fit\n        self.a = torch.inverse(K + self.lam*I)@y\n\n        \n        # save the training data: we'll need it for prediction\n        self.X_train = X\n                \n    def predict(self, X):\n        \"\"\"\n        implements eq. 12.3\n        \"\"\"\n        # compute the kernel matrix of the new data with the training data\n                k = self.kernel(X, self.X_train, **self.kwargs)\n\n        # compute the predictions\n        s = k@self.a\n        \n        return s \n        \n\nWe’ll use the rbf_kernel from before to fit our model.\n\nKR = KernelRidgeRegression(rbf_kernel, lam = .1, gamma = 0.02)\nKR.fit(X, y)\n\nNow let’s visualize:\n\n\nCode\ndef plot_model(X, y, KR, **kwargs):\n\n    fig, ax = plt.subplots(1, 1)    \n\n    plot_regression_data(X, y, ax)\n    m_points = 101\n    X_ = torch.zeros(m_points, 2)\n    x = torch.linspace(X[:,1].min().item()-0.01, X[:,1].max().item()+0.01, m_points)\n    X_[:,1] = x\n    X_[:, 0] = torch.ones(m_points)\n\n    s = KR.predict(X_)\n    ax.plot(x, s, **kwargs)\n\n\n\nplot_model(X, y, KR, color = \"black\")\n\n\n\n\n\n\n\n\nHmmmm, that doesn’t look so good. Clearly “an attempt was made,” but the model is not flexible enough to model the pattern in the data. Let’s increase \\(\\gamma\\):\n\nKR = KernelRidgeRegression(rbf_kernel, lam = .1, gamma = .1)\nKR.fit(X, y)\nplot_model(X, y, KR, color = \"black\")\n\n\n\n\n\n\n\n\nMuch nicer! As usual, other combinations of \\(\\gamma\\) and \\(\\lambda\\) can result in undesirably wiggly models that may reflect overfitting or numerical issues. These issues can often be addressed via cross-validation.\n\nKR = KernelRidgeRegression(rbf_kernel, lam = 0.000001, gamma = .5)\nKR.fit(X, y)\nplot_model(X, y, KR, color = \"black\")",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kernel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/50-kernel-methods.html#other-kernelized-models",
    "href": "chapters/50-kernel-methods.html#other-kernelized-models",
    "title": "12  Kernel Methods",
    "section": "Other Kernelized Models",
    "text": "Other Kernelized Models\nIn this example we studied kernelized linear regression. It is also possible to apply the kernel trick to several other linear models, including logistic regression. The most famous kernel method for classification is the support vector machine, which has the benefit of driving most of the entries of \\(\\mathbf{a}\\) to zero. This results in sparsity, a very favorable attribute in kernel models. Training support vector machines requires some tools in optimization theory beyond the scope of this course.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kernel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/50-kernel-methods.html#application-kernelized-sentiment-analysis",
    "href": "chapters/50-kernel-methods.html#application-kernelized-sentiment-analysis",
    "title": "12  Kernel Methods",
    "section": "Application: Kernelized Sentiment Analysis",
    "text": "Application: Kernelized Sentiment Analysis\nIn addition to giving us a flexible way to model nonlinear patterns in data, the kernel trick also gives us a helpful way to represent predictor data that isn’t natively in the form of a feature matrix. For example, let’s use kernel ridge regression to perform sentiment analysis on a small data set of Yelp reviews.  Our task is to predict the score (from 0 to 4) based on the words in the review. I retrieved this data from HuggingFace.co.The first time you work with stopwords in the NLTK package, you may need to run nltk.download('stopwords').\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport string\nimport math\n\nnltk.download('stopwords')\n\nFirst let’s download the data and do a train-test split.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/yelp-reviews/reviews-subset.csv\"\n\ndf = pd.read_csv(url)\nx, y = df[\"text\"], df[\"label\"]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.6)\n\nWe have a total of 3000 reviews in the training set:\n\nx_train\n\n4979    Don't call this landscaping company unless you...\n3942    Starting Off The Night.... They forced me to r...\n4443    Thank you for making a beautiful ring! Sylvia ...\n3016    It's a bar...like the movie Coyote Ugly.  Girl...\n3852    These guys don't seem to be able to keep staff...\n                              ...                        \n465     For my birthday, the wife planned a trip to tr...\n3433    Haven't been here in like 3 years because last...\n1943    The food was brought out HOT, just what you'd ...\n4685    The kind of Thai food you want in the neighbor...\n3204    All the previous reviews on here are for dinin...\nName: text, Length: 3000, dtype: object\n\n\nEach review is accompanied by a numerical rating from 0 to 4.\n\ny_train\n\n4979    0\n3942    1\n4443    3\n3016    1\n3852    1\n       ..\n465     2\n3433    0\n1943    0\n4685    3\n3204    3\nName: label, Length: 3000, dtype: int64\n\n\nTo use kernelized ridge regression, all we need to do is define a positive-definite kernel function that accepts two strings of text as inputs. For today, we’ll simplistically model a string as a set of unique words, ignoring repetition and word order.  To compare two sets of words, we’ll use the cosine similarity kernel. If \\(A\\) and \\(B\\) are two sets, the cosine similarity of \\(A\\) and \\(B\\) is \\[\n\\begin{aligned}\n    k(A, B) = \\frac{\\lvert A\\cap B \\rvert}{\\sqrt{\\lvert A \\lvert \\times \\lvert B\\rvert}}\\;.\n\\end{aligned}\n\\]If you would like to learn to make better assumptions about how to handle text, take a course in natural language processing!\nFor example, let sentence $s_1 = $ “I love cats” and let $s_2 = $ “I love dogs”. There are two words shared between these sentences, and each has length \\(3\\). So, the cosine similarity is \\(\\frac{2}{\\sqrt{3\\times 3}} = \\frac{2}{3}\\).\nThe following functions implement cosine similarity for sets of words. There is some minimal text processing to be done: we remove common stopwords, punctuation, and casing.\n\nstopwords = stopwords.words('english')\ntranslator = str.maketrans('', '', string.punctuation)\n\n# data prep: remove stopwords and punctuation, and transform sentence into set of words. \ndef process_text(t):\n    t = t.translate(translator)\n    s = set(t.split())\n    s = {word for word in s if word not in stopwords}\n    return s\n\n# implement the kernel. \n# sadly, this one has to be done with for-loops. \ndef cosine_similarity_kernel(X_1, X_2):\n    \n    # initialize empty kernel matrix\n    K = torch.zeros((len(X_1), len(X_2)))\n\n    # lists of processed sets of words\n    S_1 = [process_text(X_1.iloc[i]) for  i in range(len(X_1))]\n    S_2 = [process_text(X_2.iloc[i]) for  i in range(len(X_2))]\n\n    # all the math is here! \n    for i, j in product(range(len(S_1)), range(len(S_2))):\n        \n        s1, s2 = S_1[i], S_2[j]\n        K[i, j] = len(s1.intersection(s2)) / (math.sqrt(len(s1)*len(s2)))\n\n    return K\n\nThis kernel accepts text as input – no feature matrix required!\n\nx = x_train.head(5)\nx\n\n4979    Don't call this landscaping company unless you...\n3942    Starting Off The Night.... They forced me to r...\n4443    Thank you for making a beautiful ring! Sylvia ...\n3016    It's a bar...like the movie Coyote Ugly.  Girl...\n3852    These guys don't seem to be able to keep staff...\nName: text, dtype: object\n\n\n\ncosine_similarity_kernel(x, x)\n\ntensor([[1.0000, 0.0473, 0.0434, 0.0280, 0.0833],\n        [0.0473, 1.0000, 0.0000, 0.0000, 0.1113],\n        [0.0434, 0.0000, 1.0000, 0.0000, 0.0383],\n        [0.0280, 0.0000, 0.0000, 1.0000, 0.0000],\n        [0.0833, 0.1113, 0.0383, 0.0000, 1.0000]])\n\n\nNow that we’ve implemented a kernel that accepts text directly, we can fit our kernel ridge regression directly on the list of reviews. This is our first and really only time in this course where we will be able to fit a model directly on a data set without first transforming it into a feature matrix.\n\nKR = KernelRidgeRegression(cosine_similarity_kernel, lam = 0.01)\nKR.fit(x_train, y_train)\n\nThis calculation is relatively slow. Why? The two computational bottlenecks are:\n\nComputing the kernel matrix, which has \\(3000^2 = 9,000,000\\) entries.\nInverting the kernel matrix, which has complexity of roughly \\(O(n^3)\\).\n\nWe can similarly extract predictions on the test data, which will again take some time.\n\npreds = KR.predict(x_test)\n\nIt can be hard to visualize these predictions because we never explicitly formed a feature matrix. One way to visually assess quality is to view the mean of the predictions as a function of the observed scores on the test set.\n\nplt.scatter(y_test, preds, alpha = 0.01)\nfor i in range(5):\n    mean_pred = preds.numpy()[y_test == i].mean()\n    plt.scatter([i], [mean_pred], color = \"black\")\nlabs = plt.gca().set(xlabel = \"True Review Rating\", ylabel = \"Predicted Review Rating\", ylim = (0, 4))\n\n\n\n\n\n\n\n\nThe predictions are pretty noisy and not perfectly lined up with the actual ratings. That said, we observe a consistent trend in which the model gives higher predicted ratings to reviews that did indeed have higher ratings.\nTo confirm, let’s look at some of the best and worst reviews.\n\n\nCode\nbottom_reviews = x_test.iloc[preds.argsort().numpy()[:5]]\ntop_reviews    = x_test.iloc[preds.argsort().numpy()[-5:]]\n\nprint(\"BEST REVIEWS\")\nprint(\"------------\")\nfor text in top_reviews: \n    print(text)\n\nprint(\"\\nWORST REVIEWS\")\nprint(\"------------\")\nfor text in bottom_reviews: \n    print(text)\n\n\nBEST REVIEWS\n------------\nThis is the best place for a couple of slices in all of AZ (period!)\nThe service was amazing and food delicious\nEverything we got was amazing, the cheese curds were perfect. I can honestly say this was the best burger I have had in Madison. The Prime Rib was delicious as well. Great atmosphere. Best bar\nReally good gyros... best I've had in Phoenix\nDelicious and everyone there is so friendly!\n\nWORST REVIEWS\n------------\nterrible.\nbland.\nWORST PIZZA EVER. really bad food. terrible service.\nPlease don't waste your time or money!  Absolutely horrible, it taste as bad as the building looks! HORRIBLE SERVICE TOO!\nZombie-like service, food mediocre. Meh.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kernel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/50-kernel-methods.html#reflecting-on-kernel-methods",
    "href": "chapters/50-kernel-methods.html#reflecting-on-kernel-methods",
    "title": "12  Kernel Methods",
    "section": "Reflecting on Kernel Methods",
    "text": "Reflecting on Kernel Methods\nKernel methods are a powerful method for working with nonlinear trends in data, and around 20 years ago they were at the forefront of machine learning research. Kernel methods, however, they also suffer from some important limitations. The major issue is the computation and manipulation of the kernel matrix \\(\\mathbf{K} \\in \\mathbb{R}^{n\\times n}\\). Because the size of this matrix scales with the size of the data set, kernel methods can struggle to train or make predictions on data sets with very large numbers of data points. The need to work with nonlinear features on very large data sets will bring us to our final major topic of the course – deep learning.\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kernel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/51-vectorization.html",
    "href": "chapters/51-vectorization.html",
    "title": "13  Vectorization and Feature Engineering",
    "section": "",
    "text": "What Data Needs Vectorization?\nMost of it!\nSome data that usually require vectorization:",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Vectorization and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "chapters/51-vectorization.html#what-data-needs-vectorization",
    "href": "chapters/51-vectorization.html#what-data-needs-vectorization",
    "title": "13  Vectorization and Feature Engineering",
    "section": "",
    "text": "If your data comes to you as a table or matrix containing only numbers, in which each row corresponds to exactly one observation, then you may not need to vectorize.\nIf your data comes to you in any other form, then you need to vectorize.\n\n\n\nImages\nText\nAudio files\nGenomic data\nEtc. etc.\n\nThere are tons of ways of vectorizing different kinds of data, and we’re not going to cover all of them. Instead, we’re going to go a little more in depth on text vectorization. We’ll discuss image vectorization much more when we get to convolutional neural networks. For your projects, depending on the data you want to work with, you may need to research vectorization schemes appropriate to your data.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Vectorization and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "chapters/51-vectorization.html#vectorization-vs.-feature-engineering",
    "href": "chapters/51-vectorization.html#vectorization-vs.-feature-engineering",
    "title": "13  Vectorization and Feature Engineering",
    "section": "Vectorization vs. Feature Engineering",
    "text": "Vectorization vs. Feature Engineering\nVectorizing data is very closely related to the problem of engineering useful features in a data set. Formally, we talk about vectorizing when we simply mean the problem of getting non-vector data into a vector format. Feature engineering then comes after: we use the vector to construct new useful features. In practice, these operations are often intermingled. For example, we might aim to go from non-vector data to a vector of useful features directly. In fact, we’ll do this below.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Vectorization and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "chapters/51-vectorization.html#case-study-sign-language-recognition",
    "href": "chapters/51-vectorization.html#case-study-sign-language-recognition",
    "title": "13  Vectorization and Feature Engineering",
    "section": "Case Study: Sign Language Recognition",
    "text": "Case Study: Sign Language Recognition\nThere is no single approach for vectorizing data. The most effective ways to vectorize a given data set depend strongly on where the data comes from and what aspects of it we expect to be useful for a prediction task. Effective vectorization methods for images can be very different from vectorization methods from text, which can in turn be very different from vectorization methods for audio files. Rather than make a superficial touch on each one, we’ll instead focus on a specific running example: vectorization of images. We’ll continue with the same example when we get to deep learning later in these notes.\nOur data for today is the Sign Language MNIST data set, which I retrieved from Kaggle. This data set poses a challenge: can we train a model to recognize a letter of American Sign Language from a hand gesture?\nFirst, let’s load our packages and retrieve the data.\n\nimport torch \nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport matplotlib.ticker as mtick\nfrom torch.nn import Conv2d, MaxPool2d, Parameter\nfrom torch.nn.functional import relu\nfrom sklearn.metrics import confusion_matrix\n\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/sign-language-mnist/sign_mnist_train.csv\"\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/sign-language-mnist/sign_mnist_test.csv\"\n\ndf_train = pd.read_csv(train_url)\ndf_test  = pd.read_csv(test_url)\n\n\nALPHABET = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\nn, p = df_train.shape[0], df_train.shape[1] - 1\n\nNatively, this data set comes to us as a data frame in which each column represents a pixel. This is actually already a kind of vectorization, and we’ll soon want to make some different choices. With this in mind, it’s better to work with the data in its native image form. The following code block reshapes the data and gets us there, on both the training and test sets.\n\ndef prep_data(df): \n    n, p = df.shape[0], df.shape[1] - 1\n    y = torch.tensor(df[\"label\"].values)\n    X = df.drop([\"label\"], axis = 1)\n    X = torch.tensor(X.values)\n    X = torch.reshape(X, (n, 1, 28, 28))\n    X = X / 255\n\n    return X, y\n\nX_train, y_train = prep_data(df_train)\nX_test, y_test = prep_data(df_test)\n\nOur data is now shaped like a “stack” of images. Let’s take a look at the size of a single slice: The reason that we have to do X[0,0] rather than simply X[0] is that X is 4-dimensional. The second dimension is for the channel of the image. An RGB image needs to represent color values for 3 distinct colors and therefore has 3 channels. Since our image is greyscale, we only have 1 channel, which is labeled 0.\n\nX_train[0, 0].size()\n\ntorch.Size([28, 28])\n\n\nLet’s visualize this:\n\nplt.imshow(X_train[10, 0], cmap = \"Greys_r\")\nplt.gca().set(title = f\"{ALPHABET[y_train[10]]}\")\nno_ax = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\nLet’s look at a handful of images and their labels:\n\ndef show_images(X, y, rows, cols, channel = 0):\n\n    fig, axarr = plt.subplots(rows, cols, figsize = (2*cols, 2*rows))\n    for i, ax in enumerate(axarr.ravel()):\n        ax.imshow(X[i, channel].detach(), cmap = \"Greys_r\")\n        ax.set(title = f\"{ALPHABET[y[i]]}\")\n        ax.axis(\"off\")\n    plt.tight_layout()\n\nshow_images(X_train, y_train, 5, 5)\n\n\n\n\n\n\n\n\nHow frequently does each letter appear in the data set?\nLet’s look at the frequencies of each letter in this data set: There are no “J”s or “Z”s in this data because these characters require motion rather than a static gesture.\n\n\nCode\nfig, ax = plt.subplots(1, 1, figsize = (6, 2))\nletters, counts = torch.unique(y_train, return_counts = True)\nproportions = counts / counts.sum()\nproportions\n\nax.scatter(letters, proportions*100, facecolor = \"none\", edgecolor = \"steelblue\")\nax.set_xticks(range(26))\nax.set_xticklabels(list(ALPHABET))\nax.set(xlabel = \"Letter\", ylabel = \"Frequency\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(decimals = 1))\n\n\n\n\n\n\n\n\n\nThe most frequent letter (“R”) in this data comprises no more than 5% of the entire data set. So, as a minimal aim, we would like a model that gets the right character at least 5% of the time.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Vectorization and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "chapters/51-vectorization.html#first-approach-pixel-by-pixel-prediction",
    "href": "chapters/51-vectorization.html#first-approach-pixel-by-pixel-prediction",
    "title": "13  Vectorization and Feature Engineering",
    "section": "First Approach: Pixel-By-Pixel Prediction",
    "text": "First Approach: Pixel-By-Pixel Prediction\nOne simple way to vectorize our data would be to simply treat the greyscale value of each pixel as a feature.  This approach gives us a feature matrix in which each column corresponds to an individual pixel, of which are there are 784 in each image.This is actually the form our data came in: we are imagining that we were instead supplied with the images in rectangular form.\n\nX_train_flat = X_train.reshape(n, p)\nprint(X_train_flat.size())\n\ntorch.Size([27455, 784])\n\n\nReshaping our square \\(28 \\times 28\\) images into long vectors of length \\(784 = 28^2\\) is the only feature engineering step we need in this case. Running this code on your laptop may lead to ConvergenceWarnings in logistic regression. For the purposes of following along with the notes it’s ok to ignore these, although it’s also possible to adjust the optimization algorithm or maximum number of iterations in LogisticRegression in order to make them go away.\n\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nf = LR.fit(X_train_flat, y_train)\n\nHow did we do on the training data?\n\nLR.score(X_train_flat, y_train)\n\n0.9581861227463121\n\n\nWow, that’s a high accuracy for just going pixel-by-pixel! It almost seems too good to be true…\n\nn_test = df_test.shape[0]\nX_test_flat = X_test.reshape(n_test, p)\nLR.score(X_test_flat, y_test)\n\n0.6740100390407139\n\n\nIndeed, we did much worse on the test data, with dramatic overfitting. That said, our model has still learned considerable pattern in the data: random guessing would lead to test accuracy in the range of 4-5%.\nThe following function wraps up this experiment for convenient reuse down the line. The only addition is the incorporation of a pipeline function, which is going to reflect our later vectorization workflows.\n\ndef vectorization_experiment(pipeline = lambda x: x, return_confusion_matrix = False):\n    X_train_transformed = pipeline(X_train)\n    X_train_flat = X_train_transformed.flatten(start_dim = 1)\n    print(f\"Number of features = {X_train_flat.size(1)}\")\n\n    LR = LogisticRegression() \n    LR.fit(X_train_flat, y_train)\n    print(f\"Training accuracy = {LR.score(X_train_flat, y_train):.2f}\")\n\n    X_test_transformed = pipeline(X_test) \n    X_test_flat = X_test_transformed.flatten(start_dim = 1)\n    print(f\"Testing accuracy  = {LR.score(X_test_flat, y_test):.2f}\")\n\n    if return_confusion_matrix: \n        y_test_pred = LR.predict(X_test_flat)\n        return confusion_matrix(y_test, y_test_pred, normalize = \"true\")\n\n\nvectorization_experiment() # same experiment as above\n\nNumber of features = 784\nTraining accuracy = 0.96\nTesting accuracy  = 0.67",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Vectorization and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "chapters/51-vectorization.html#convolutional-kernels",
    "href": "chapters/51-vectorization.html#convolutional-kernels",
    "title": "13  Vectorization and Feature Engineering",
    "section": "Convolutional Kernels",
    "text": "Convolutional Kernels\nA common approach to vectorization of images is to apply a convolutional kernel.  A convolutional kernel is a component of a vectorization pipeline which is specifically suited to the structure of images. In particular, images are fundamentally spatial. We might want to construct data features which reflect not just the value of an individual pixel, but also the values of pixels nearby that one.Convolutional kernels are not related in any way to positive-definite kernels.\nOne of the most common types of layers is a convolutional layer. The idea of an image convolution is pretty simple. We define a square kernel matrix containing some numbers, and we “slide it over” the input data. At each location, we multiply the data values by the kernel matrix values, and add them together. Here’s an illustrative diagram:\n\nImage from Dive Into Deep Learning\nIn this example, the value of 19 is computed as \\(0\\times 0 + 1\\times 1 + 3\\times 2 + 4\\times 3 = 19\\).\nLet’s create some simple \\(5\\times 5\\) kernels that we’ll apply to this image data. Our first one is designed to detect vertical lines in images:\n\nvertical = torch.tensor([0, 0, 5, 0, 0]).repeat(5, 1) - 1.0\nvertical\n\ntensor([[-1., -1.,  4., -1., -1.],\n        [-1., -1.,  4., -1., -1.],\n        [-1., -1.,  4., -1., -1.],\n        [-1., -1.,  4., -1., -1.],\n        [-1., -1.,  4., -1., -1.]])\n\n\nLet’s make some more kernels for horizontal and vertical lines:\n\ndiag1    = torch.eye(5)*5 - 1\nhorizontal = torch.transpose(vertical, 1, 0)\ndiag2    = diag1.flip(1)\n\nfig, ax = plt.subplots(1, 4)\nfor i, kernel in enumerate([vertical, horizontal, diag1, diag2]):\n    ax[i].imshow(kernel, vmin = -1.5, vmax = 2)\n    ax[i].axis(\"off\")\n    ax[i].set(title = f'{[\"Vertical\", \"Horizontal\", \"Diagonal Down\", \"Diagonal Up\"][i]}')\n\n\n\n\n\n\n\n\nNow let’s make a function which will apply these convolutional kernels to our images. The output of each kernel will be stored as a separate channel of the image.\n\ndef apply_convolutions(X): \n\n    # this is actually a neural network layer -- we'll learn how to use these\n    # in that context soon \n    conv1 = Conv2d(1, 4, 5)\n\n    # need to disable gradients for this layer\n    for p in conv1.parameters():\n        p.requires_grad = False\n\n    # replace kernels in layer with our custom ones\n    conv1.weight[0, 0] = Parameter(vertical)\n    conv1.weight[1, 0] = Parameter(horizontal)\n    conv1.weight[2, 0] = Parameter(diag1)\n    conv1.weight[3, 0] = Parameter(diag2)\n\n    # apply to input data and disable gradients\n    return conv1(X).detach()\n\nNow we’re ready to compute the result of applying our kernels to the data.\n\nX_train_convd = apply_convolutions(X_train)\n\nWe now have a training data set of diffferent size. There’s one channel (2nd index) for each of the four kernels we’ve applied.\n\nX_train_convd.size()\n\ntorch.Size([27455, 4, 24, 24])\n\n\nLet’s take a look at how each of the four kernels act on some images.\n\ndef kernel_viz(pipeline):\n\n    fig, ax = plt.subplots(5, 5, figsize = (8, 8))\n\n    X_convd = pipeline(X_train)\n\n    for i in range(5): \n        for j in range(5):\n            if i == 0: \n                ax[i,j].imshow(X_train[j, 0])\n            \n            else: \n                ax[i, j].imshow(X_convd[j,i-1])\n            \n            ax[i,j].tick_params(\n                        axis='both',      \n                        which='both',     \n                        bottom=False,     \n                        left=False,\n                        right=False,         \n                        labelbottom=False, \n                        labelleft=False)\n            ax[i,j].grid(False)\n            ax[i, 0].set(ylabel = [\"Original\", \"Vertical\", \"Horizontal\", \"Diag Down\", \"Diag Up\"][i])\n\nkernel_viz(apply_convolutions)\n\n\n\n\n\n\n\n\nWe see that the Vertical kernel, for example, tends to accentuate vertical boundaries between light and dark in the images. The horizontal kernel accentuates horizontal boundaries, and so on.\nOk, so we have Done Something to our images. Does this actually lead to better classification accuracy when we use logistic regression?\n\nvectorization_experiment(apply_convolutions)\n\nNumber of features = 2304\nTraining accuracy = 1.00\nTesting accuracy  = 0.54\n\n\nUhhhhh, well that actually seems to have made it worse. Somehow, we’ve actually overfit even more! One reason for this is that we have actually increased the number of features, without adding more data points. But shouldn’t we have gotten a bump from having these purportedly useful features in the mix?\nSurprisingly, no. The reason is that kernel convolution is a fundamentally linear operation – it’s just a matter of multiplying the data by constants and adding the results together. Logistic regression is a linear model, and the way it calculates the score is also a linear operation. As you may remember, the result of doing two linear operations back-to-back is simply a different linear operation. But since everything involved is linear, our vectorization approach hasn’t really added anything to the expressive power of logistic regression – only add more feature columns to enable even greater overfitting.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Vectorization and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "chapters/51-vectorization.html#nonlinearity",
    "href": "chapters/51-vectorization.html#nonlinearity",
    "title": "13  Vectorization and Feature Engineering",
    "section": "Nonlinearity",
    "text": "Nonlinearity\nIn order to add to the expressive power of logistic regression, we need to do something nonlinear to the result of applying our convolutions. A common choice is the rectified linear unit, or ReLU. This very simple nonlinear function looks like this:\n\nz = torch.linspace(-1, 1, 101)\nf = relu(z)\nplt.plot(z, f, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = r\"$z$\",ylabel = r\"$\\mathrm{ReLU}(z)$\")\n\n\n\n\n\n\n\n\nYes, it’s just two linear functions pasted together, but that makes it technically nonlinear! The effect of applying the relu transformation on our convolved images is that pixels with negative values in each channel are set to 0.\n\npipeline = lambda x: relu(apply_convolutions(x))\nkernel_viz(pipeline)\n\n\n\n\n\n\n\n\nDoes setting a bunch of the pixels to 0 really help that much? Surprisingly, yes! Just applying this nonlinear transformation to our convolved images already significantly improves the classification power of our model on the test set.\n\nvectorization_experiment(pipeline)\n\nNumber of features = 2304\nTraining accuracy = 1.00\nTesting accuracy  = 0.76",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Vectorization and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "chapters/51-vectorization.html#pooling",
    "href": "chapters/51-vectorization.html#pooling",
    "title": "13  Vectorization and Feature Engineering",
    "section": "Pooling",
    "text": "Pooling\nHowever, we do still have that very large number of features, which slows down model fitting and raises overfitting issues. To address this, let’s reduce the data in a nonlinear way. We’ll do this with max pooling. You can think of it as a kind of “summarization” step in which we intentionally make the current output somewhat “blockier.” Technically, it involves sliding a window over the current batch of data and picking only the largest element within that window. Here’s an example of how this looks:\n\nImage credit: Computer Science Wiki\nA useful effect of pooling is that it reduces the number of features in our data. In the image above, we reduce the number of features by a factor of \\(2\\times 2 = 4\\). We can be even more aggressive than this: we’ll reduce our data resolution by a factor of 16.\n\npipeline = lambda x: MaxPool2d(4,4)(relu(apply_convolutions(x)))\nkernel_viz(pipeline)\n\n\n\n\n\n\n\n\nOur data now looks a lot chunkier, and doesn’t really resemble recognizable hands at all. Surprisingly, however, this is another big improvement on our modeling power.\n\nvectorization_experiment(pipeline)\n\nNumber of features = 144\nTraining accuracy = 1.00\nTesting accuracy  = 0.88\n\n\nRelative to the previous experiments, we have about 6% as many features to feed into our logistic regression models, which leads to a big speed-up. Despite this, the testing accuracy has gone considerably! We have managed to extract useful, vectorized features from our image-structured data set. This level of accuracy is not enough to be practically useful. We’ll later see how to do better on this data set.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Vectorization and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "chapters/51-vectorization.html#other-kinds-of-vectorization",
    "href": "chapters/51-vectorization.html#other-kinds-of-vectorization",
    "title": "13  Vectorization and Feature Engineering",
    "section": "Other Kinds of Vectorization",
    "text": "Other Kinds of Vectorization\nOur pipeline above: convolutional kernels, pooling, and flattening, is an extremely common vectorization/feature engineering pipeline for image data. Indeed, it is at the foundation of convolutional neural networks, which are one of the standard ways for performing image classification. We’ll work with convolutional neural networks later in these notes. How do we vector other data?\nText is often vectorized in one of two paradigms.\n\nIn classical text analysis, it is common to form a document-term matrix in which the rows correspond to a given body of text (say, a single chapter of a book) and the columns correspond to words. The document-term matrix collects information on how frequently a given word appears in a given body of text. Various normalizations are possible. This kind of vectorization is common in sentiment analysis and topic modeling applications.\nIn modern generative applications, we often represent bodies of text as sequences of words. Each word in the sequence is assigned a vector which is intended to represent the semantic content of the word in some way. A sequence of words is then a sequence of vectors in this vector space. The problem of finding good vectors to represent words is known as word embedding and is fundamental to a number of text-based tasks in modern deep learning.\n\nAudio is often vectorized using classical tools from signal processing, including (discrete) Fourier transforms and wavelet transforms.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Vectorization and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "chapters/60-intro-deep.html",
    "href": "chapters/60-intro-deep.html",
    "title": "14  The Problem of Features and Deep Learning",
    "section": "",
    "text": "The Computational Graph\nIt will soon be very useful for us to start thinking about the operations underlying the computation of the loss in terms of a computational graph. In the computational graph, we break down each operation as an edge in the graph, storing the result of that operation in the nodes.\nFor example, here’s a high-level computational graph associated with the standard linear model in Equation 14.1 with no feature maps:\nflowchart LR\n  input[Data matrix X] --Matrix-vector\\nmultiplication--&gt; l3[\"Xw\"]\n\n  y[Targets y]        --Apply\\nloss--&gt; loss[\"ℓ(Xw, y)\"]\n  l3       --Apply\\nloss--&gt; loss \n  loss     --Compute\\naverage--&gt;L\nIf we wanted to apply a feature map \\(\\phi\\), we would need to incorporate one additional step in the computational graph:\nflowchart LR\n  input[Data matrix X] --Apply feature\\nmap--&gt;l1[\"Φ(X)\"]\n  l1                   --Matrix-vector\\nmultiplication--&gt; l3[\"Φ(X)w\"]\n\n  y[Targets y] --Apply\\nloss--&gt; loss[\"ℓ(Φ(X)w, y)\"]\n  l3           --Apply\\nloss--&gt; loss \n  loss         --Compute\\naverage--&gt;L",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The Problem of Features and Deep Learning</span>"
    ]
  },
  {
    "objectID": "chapters/60-intro-deep.html#optimizing-the-features",
    "href": "chapters/60-intro-deep.html#optimizing-the-features",
    "title": "14  The Problem of Features and Deep Learning",
    "section": "Optimizing the Features",
    "text": "Optimizing the Features\nHere’s a superficially simple question about Equation 14.1 and its associated computational graph: what if we could learn both the weights \\(\\mathbf{w}\\) and the feature map \\(\\phi\\) simultaneously? That is, what if we could solve a problem like\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{w}} &= \\argmin_{\\mathbf{w} \\in \\mathbb{R}^q, \\color{blue}{\\phi \\in \\mathcal{F}}} \\frac{1}{n}\\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\phi(\\mathbf{x}_i) \\rangle, y_i)\\;,\n\\end{aligned}\n\\tag{14.2}\\]\nwhere \\(\\mathcal{F}\\) is some set of possible feature maps?\nIn general, we have no hope of doing this – there are simply too many possible feature maps. However, if we restrict the set \\(\\mathcal{F}\\) somewhat, then we may have some hope.\nIn particular, let’s choose a nonlinear function \\(\\alpha: \\mathbb{R} \\rightarrow \\mathbb{R}\\) and a matrix \\(\\mathbf{U} \\in \\mathbb{R}^{p \\times q}\\). We’ll define our feature map \\(\\phi\\) by the formula \\(\\phi(\\mathbf{X}) = \\alpha(\\mathbf{X}\\mathbf{U})\\), where \\(\\alpha\\) is applied to each element of the matrix \\(\\mathbf{X}\\mathbf{U}\\). We’ll treat \\(\\alpha\\) as fixed but \\(\\mathbf{U}\\) as learnable. This turns our optimization problem into\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{w}} &= \\argmin_{\\mathbf{w} \\in \\mathbb{R}^q, \\color{blue}{\\mathbf{U} \\in \\mathbb{R}^{p \\times q}}} \\frac{1}{n}\\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\alpha (\\mathbf{x}_i \\mathbf{U}) \\rangle, y_i)\\;.\n\\end{aligned}\n\\tag{14.3}\\]\nWe now have an optimization problem with two sets of weights: the standard weights \\(\\mathbf{w}\\) and the hidden weights \\(\\mathbf{U}\\). Equation 14.3 defines the structure of a shallow neural network with a single hidden layer.\nThe computational graph for this Equation 14.3 model now looks like this:\n\n\n\n\n\nflowchart LR\n  input[Data matrix X] --Matrix\\nmultiplication--&gt; l1[XU]\n  l1       --Nonlinearity--&gt; l2[\"ɑ(XU)\"]\n  l2       --Matrix-vector\\nmultiplication--&gt; l3[\"ɑ(XU)w\"]\n\n  y[Targets y]        --Apply\\nloss--&gt; loss[\"ℓ(ɑ(XU)w, y)\"]\n  l3       --Apply\\nloss--&gt; loss \n  loss     --Compute\\naverage--&gt;L\n\n\n\n\n\n\n\nModel Layers\nIn this model, the matrix \\(\\mathbf{U}\\) is an example of a so-called hidden parameter or hidden layer. It’s hidden because it’s not involved in the final computation of scores (like \\(\\mathbf{w}\\)) and it’s also not part of the data input. If we were only looking at the inputs and final computational operations of the model, we wouldn’t see \\(\\mathbf{U}\\) at all. We only see it when we look at the complete sequence of network operations.\nImportantly, we don’t have to stop at one hidden layer. We could pick a new matrix \\(\\mathbf{V} \\in \\mathbb{R}^{q \\times r}\\) and a possibly-different nonlinearity \\(\\beta: \\mathbb{R} \\rightarrow \\mathbb{R}\\). Then, we could compute our feature map as\n\\[\n\\begin{aligned}\n    \\phi(\\mathbf{X}) = \\beta(\\alpha(\\mathbf{X}\\mathbf{U})\\mathbf{V})\\;.\n\\end{aligned}\n\\]\nThen we would have two hidden layers and a longer chain in our computational graph.\nA so-called neural network is a machine learning model that involves one or more arrays of hidden parameters. These models are also often called deep learning models – their “depth” is in the unseen layers.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The Problem of Features and Deep Learning</span>"
    ]
  },
  {
    "objectID": "chapters/60-intro-deep.html#implementation-via-torch",
    "href": "chapters/60-intro-deep.html#implementation-via-torch",
    "title": "14  The Problem of Features and Deep Learning",
    "section": "Implementation via Torch",
    "text": "Implementation via Torch\nLet’s go ahead and implement some of these models in Torch. For our first implementation, we are going to focus on the simplest linear model without feature maps. The implementation of this model using Torch’s standard neural network model is relatively similar to how we’ve implemented linear models in the past. The main difference is that instead of manually managing a weight vector self.w, instead we manage a nn.Sequential object that contains one or more layers of computation. Here, the Linear layer implements matrix multiplication and holds the analog of our vector self.w. Another difference is that we do not usually implement the loss in the model itself. Instead, we usually call a pre-implemented loss function during the training loop.\n\nfrom torch import nn \nclass LinearModel(nn.Module):\n\n    def __init__(self, num_features, num_labels):\n        \"\"\"\n        sets up the model infrastructure\n        \"\"\"\n        super().__init__()\n\n        \n        self.pipeline = nn.Sequential(\n            nn.Linear(num_features, num_labels) # implements multiplication by w\n        )\n\n    def score(self, x):\n        \"\"\"\n        computes scores for each class \n        \"\"\"\n        \n        return self.pipeline(x)\n        \n\n    # this is strictly for visualization -- not involved in the training loop\n    def predict(self, x): \n        return self.score(x) &gt; 0\n\nTo see this model in action, let’s generate some data.\n\nimport torch\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\nX_train, y_train = make_moons(100, shuffle = True, noise = 0.2)\nX_train = torch.tensor(X_train, dtype = torch.float)\ny_train = torch.tensor(y_train)\n\nX_val, y_val = make_moons(100, shuffle = True, noise = 0.2)\nX_val = torch.tensor(X_val, dtype = torch.float)\ny_val = torch.tensor(y_val)\n\n\n\nCode\ndef plot_classification_data(X, y, ax, clf = None, thresh = 0, data = \"Training\"):\n    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n\n\n    if clf is not None:\n        h = 0.01\n        x_min, x_max = X[:, 0].min() - 0.2, X[:, 0].max() + 0.2\n        y_min, y_max = X[:, 1].min() - 0.2, X[:, 1].max() + 0.2\n        xx, yy = torch.meshgrid(torch.arange(x_min, x_max, h), torch.arange(y_min, y_max, h), indexing=\"ij\")\n        XX = torch.stack([xx.ravel(), yy.ravel()], dim = 1)\n        Z = clf.score(XX)\n        Z = Z[:,1].reshape(xx.shape)\n\n        v = Z.abs().max()\n        ax.contourf(xx, yy, Z.detach(), cmap = \"BrBG\", alpha = 0.3, vmin = -v, vmax = v)\n        ax.contour(xx, yy, Z.detach() &gt; thresh, zorder = -10)\n\n        preds = clf.score(X)[:,1] &gt; 0\n        ax.set(title = f\"{data} Accuracy: {torch.mean((preds == y).float()).item():.2f}\")\n        \n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"black\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_classification_data(X_train,y_train, ax)\n\n\n\n\n\n\n\n\nThe training loop for Torch models looks similar to training loops that we have seen for the perceptron and for logistic regression. There are a few important differences:\n\nWe generally use a pre-implemented optimizer from torch.optim. Today we’re using gradient descent.\nWe also use a pre-implemented loss-function. Today we’re using the cross-entropy loss. As you may remember, the combination of a linear model with the binary cross-entropy loss means that we are working with our good friend, logistic regression.\nInstead of manually computing a gradient and writing out gradient descent, instead we make the rather cryptic calls loss.backward() followed by opt.step(). As we’ll soon discuss, loss.backward() actually handles all the gradient calculations for us (no more calculus by hand!), while opt.step() takes that gradient information and uses it to update the parameters.\n\nAfter each call of loss.backward(), it’s necessary to call opt.zero_grad() in order to “zero out” the gradient information computed in the previous loop.\n\n\n\ndef train_model(model, num_steps, **kwargs):\n\n    # define the loss function L for the linear model\n    loss_fn = nn.CrossEntropyLoss()\n\n    # instantiate an optimizer -- gradient descent today\n    opt = torch.optim.SGD(model.parameters(), **kwargs)\n\n    # collect the values of the loss in each step\n    loss_train_vec = []\n    loss_val_vec   = []\n\n    for i in range(num_steps): \n        \n                s = LM.score(X_train)                      # compute the scores \n        loss = loss_fn(s, y_train)                 # compute the model loss\n        loss.backward()                      # auto-compute gradient\n        opt.step()                           # optimizer updates params\n\n        opt.zero_grad()                      # zero out the gradient \n        \n        # for tracking model progress on the training set\n        loss_train_vec.append(loss.item())   \n        \n        # and on the validation set\n        s_val = LM.score(X_val)\n        loss_val = loss_fn(s_val, y_val)\n        loss_val_vec.append(loss_val.item())\n\n    return loss_train_vec, loss_val_vec\n\nNow that we have defined a model and a training loop, let’s go ahead and train the model!\n\nLM = LinearModel(2, 2)\nloss_train, loss_val = train_model(LM, num_steps = 1000, lr = 0.1)\n\nNow that we’ve trained the model, let’s see how we did on the training set:\n\nfig, ax = plt.subplots(1, 2, figsize = (6, 3))\nax[0].plot(loss_train, c = \"steelblue\", label = \"Training\")\nplot_classification_data(X_train, y_train, ax[1], clf = LM)\nplt.tight_layout()\n\n\n\n\n\n\n\n\nYep, that’s logistic regression! We’ve fit a reasonable classifying line, but haven’t captured any nonlinear trend.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The Problem of Features and Deep Learning</span>"
    ]
  },
  {
    "objectID": "chapters/60-intro-deep.html#adding-a-hidden-layer",
    "href": "chapters/60-intro-deep.html#adding-a-hidden-layer",
    "title": "14  The Problem of Features and Deep Learning",
    "section": "Adding A Hidden Layer",
    "text": "Adding A Hidden Layer\nIn order to model the nonlinearity, let’s add a single hidden layer to our model.\n\nfrom torch import nn \n\nclass SingleHiddenLayer(nn.Module):\n\n    def __init__(self, num_features, num_labels, num_hidden):\n        \"\"\"\n        sets up the model infrastructure\n        \"\"\"\n        super().__init__()\n    \n        self.pipeline = nn.Sequential(\n                        nn.Linear(num_features, num_hidden), \n            nn.ReLU(),\n            nn.Linear(num_hidden, num_labels)\n                    )\n\n    def score(self, x):\n        \"\"\"\n        computes scores for each class \n        \"\"\"\n        return self.pipeline(x)\n\n    def predict(self, x): \n        return self.score(x) &gt; 0\n\nLet’s also streamline the process of fitting the model and visualizing the results.\n\ndef plot_experiment(model, **kwargs):\n    fig, ax = plt.subplots(1, 3, figsize = (9, 3))\n    loss_train, loss_val = train_model(model, **kwargs)     \n    ax[0].plot(loss_train, c = \"steelblue\", label = \"Training\")\n    ax[0].plot(loss_val, c = \"goldenrod\", label = \"Validation\")\n    ax[0].set(xlabel = \"Iteration\", ylabel = \"Loss\")\n    ax[0].legend()\n    plot_classification_data(X_train, y_train, ax[1], clf = model)    \n    plot_classification_data(X_val, y_val, ax[2], clf = model, data = \"Validation\")    \n    plt.tight_layout()\n\n\nLM = SingleHiddenLayer(2, 2, 100)\nplot_experiment(LM, num_steps = 5000, lr = 0.2)\n\n\n\n\n\n\n\n\nWe’ve been able to model a nonlinear decision boundary without explicitly constructing a feature map or using kernels. Instead, we learned a helpful feature map as part of the training process.\nNote that omitting the ReLU layer in the hidden layer will result in a model that is equivalent to the linear model. This is because the composition of two linear functions is itself a linear function. Try it!\nWhat if we wanted Even More Layers? We can easily do this just by adding more layers inside the pipeline.\n\nfrom torch import nn \n\nclass FullyConnectedStack(nn.Module):\n\n    def __init__(self, num_features, num_labels):\n        \"\"\"\n        sets up the model infrastructure\n        \"\"\"\n        super().__init__()\n    \n        self.pipeline = nn.Sequential(\n                        nn.Linear(num_features, 50), \n            nn.ReLU(),\n            nn.Linear(50, 10), \n            nn.ReLU(),\n            nn.Linear(10, num_labels)\n                    )\n\n    def score(self, x):\n        \"\"\"\n        computes scores for each class \n        \"\"\"\n        return self.pipeline(x)\n\n    def predict(self, x): \n        return self.score(x) &gt; 0\n\nAs usual, adding more layers can help us model more complex patterns in the data, but also increases the risk of overfitting.\n\nLM = FullyConnectedStack(2, 2)\nplot_experiment(LM, num_steps = 10000, lr = 0.05, momentum = 0.8)",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The Problem of Features and Deep Learning</span>"
    ]
  },
  {
    "objectID": "chapters/60-intro-deep.html#looking-ahead",
    "href": "chapters/60-intro-deep.html#looking-ahead",
    "title": "14  The Problem of Features and Deep Learning",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nSo far, we have seen that a single hidden layer can help us learn nonlinear decision boundaries in data, without the need to construct handmade features or use kernel methods. However, we now have a wide range of both theoretical and practical questions ahead of us.\n\nAre these models still convex? Can I still use gradient descent?\nIsn’t it hard to compute gradients for these more complicated models? How does anyone keep track of all the calculus?\nHow many layers should I use in my models? How large should the matrices be?\nHow should I adapt my models when I am working with specific kinds of data, such as text, images, or audio?\nHow should I guard against overfitting as my models get progressively more complex?",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The Problem of Features and Deep Learning</span>"
    ]
  }
]