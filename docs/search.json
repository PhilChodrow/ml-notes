[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Welcome\nThis is a set of notes developed for an undergraduate course in machine learning. The target audience for these notes are undergraduates in computer science who have completed first courses in linear algebra and discrete mathematics. These notes draw on many sources, but are somewhat distinctive in the following ways:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#pedagogical-features",
    "href": "index.html#pedagogical-features",
    "title": "Machine Learning",
    "section": "Pedagogical Features",
    "text": "Pedagogical Features\nThese notes are explicitly designed for undergraduate instruction in computer science. For this reason:\n\nComputational examples are integrated into the text and shown throughout.\nLive versions of lecture notes are supplied as downloadable Jupyter Notebooks, with certain code components removed. The purpose is to facilitate live-coding in lectures.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#use-and-reuse",
    "href": "index.html#use-and-reuse",
    "title": "Machine Learning",
    "section": "Use and Reuse",
    "text": "Use and Reuse\nThese notes were written by Phil Chodrow for the course CSCI 0451: Machine Learning at Middlebury College. All are welcome to use them for educational purposes. Attribution is appreciated but not expected.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#source-texts",
    "href": "index.html#source-texts",
    "title": "Machine Learning",
    "section": "Source Texts",
    "text": "Source Texts\nThese notes draw on several source texts, most of which are available for free online. These are:\n\nHardt and Recht (2022) is the primary influence for the overall arc of the notes.\nA Course in Machine Learning by Hal Daumé III is an accessible introduction to many of the topics and serves as a useful source of supplementary readings.\n\nAdditional useful readings:\n\nAbu-Mostafa, Magdon-Ismail, and Lin (2012): Learning From Data: A Short Course\nBarocas, Hardt, and Narayanan (2023) is an advanced text on questions of fairness in automated decision-making for readers who have some background in probability theory.\nBishop (2006) and Murphy (2022) are advanced texts which are most suitable for advanced readers who have already taken at least one course in probability theory.\n\nDeisenroth, Faisal, and Ong (2020) and Kroese et al. (2020) are useful readings focusing on some of the mathematical fundamentals.\nZhang, Lipton, and Li (2023) tells a helpful story of the fundamentals of deep learning.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Machine Learning",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis site was generated using the Quarto publishing system. It is hosted on GitHub and published via GitHub Pages.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Machine Learning",
    "section": "References",
    "text": "References\n\n\n\n\nAbu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012. Learning from Data: A Short Course. S.l. https://amlbook.com/.\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press. https://fairmlbook.org/pdf/fairmlbook.pdf.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nDeisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020. Mathematics for Machine Learning. Cambridge, UK New York, NY: Cambridge University Press. https://mml-book.github.io/book/mml-book.pdf.\n\n\nHardt, Moritz, and Benjamin Recht. 2022. Patterns, Predictions, and Actions. Princeton University Press. https://mlstory.org/pdf/patterns.pdf.\n\n\nKroese, Dirk P., Zdravko I. Botev, Thomas Taimre, and Radislav Vaisman. 2020. Data Science and Machine Learning: Mathematical and Statistical Methods. Chapman & Hall/CRC Machine Learning & Pattern Recognition Series. Boca Raton London New York: CRC Press, Taylor & Francis Group.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. Adaptive Computation and Machine Learning Series. Cambridge, Massachusetts: The MIT Press. https://probml.github.io/pml-book/book1.html.\n\n\nZhang, Aston, Zachary Lipton, and Mu Li. 2023. Dive into Deep Learning. Cambridge, UK: Cambridge University Press.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/01-data-and-models.html",
    "href": "chapters/01-data-and-models.html",
    "title": "1  Data, Patterns, and Models",
    "section": "",
    "text": "Supervised Learning\nOur focus in these notes is almost exclusively on supervised learning. In supervised learning, we are able to view some attributes or features of a data point, which we call predictors. Traditionally, we collect these attributes into a vector called \\(\\mathbf{x}\\). Each data point then has a target, which could be either a scalar number or a categorical label. Traditionally, the target is named \\(y\\). We aim to predict the target based on the predictors using a model, which is a function \\(f\\). The result of applying the model \\(f\\) to the predictors \\(\\mathbf{x}\\) is our prediction or predicted target \\(f(\\mathbf{x})\\), to which we often give the name \\(\\hat{y}\\). Our goal is to choose \\(f\\) such that the predicted target \\(\\hat{y}\\) is equal to, or at least close to, the true target \\(y\\). We could summarize this with the heuristic statement:\n\\[\n\\begin{aligned}\n    \\text{``}f(\\mathbf{x}) = \\hat{y} \\approx y\\;.\\text{''}\n\\end{aligned}\n\\]\nHow we interpret this heuristic statement depends on context. In regression problems, this statement typically means “\\(\\hat{y}\\) is usually close to \\(y\\)”, while in classification problems this statement usually means that “\\(\\hat{y} = y\\) exactly most or all of the time.”\nIn our regression example from above, we can think of a function \\(f:\\mathbb{R}\\rightarrow \\mathbb{R}\\) that maps the predictor \\(x\\) to the prediction \\(\\hat{y}\\). In the case of classification, things are a little more complicated. Although the function \\(g(x_1) = 1 - x_1\\) is visually very relevant, that function is not itself the model we use for prediction. Instead, our prediction function should return one classification label for points on one side of the line defined by that function, and a different label for points on the other side. If we say that blue points are labeled \\(0\\) and brown points are labeled \\(1\\), then our predictor function can be written \\(f:\\mathbb{R}^2 \\rightarrow \\{0, 1\\}\\), and it could be written heuristically like this:\n\\[\n\\begin{aligned}\n    f(\\mathbf{x}) &= \\mathbb{1}[\\mathbf{x} \\text{ is above the line}] \\\\\n                  &= \\mathbb{1}[x_1 + x_2 \\geq 1] \\\\\n                  &= \\mathbb{1}[x_1 + x_2 - 1\\geq 0]\\;.\n\\end{aligned}\n\\]\nThis last expression looks a little clunky, but we will soon find out that it is the easiest one to generalize to an advanced setting.",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data, Patterns, and Models</span>"
    ]
  },
  {
    "objectID": "chapters/01-data-and-models.html#supervised-learning",
    "href": "chapters/01-data-and-models.html#supervised-learning",
    "title": "1  Data, Patterns, and Models",
    "section": "",
    "text": "Here, \\(\\mathbb{1}\\) is the indicator function which is equal to 1 if its argument is true and 0 otherwise. Formally,\n\\[\n\\begin{aligned}\n    \\mathbb{1}[P] = \\begin{cases}\n        1 &\\quad P \\text{ is true} \\\\\n        0 &\\quad P \\text{ is false.}\n        \\end{cases}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data, Patterns, and Models</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html",
    "href": "chapters/02-black-box-classification.html",
    "title": "2  Classification as a Black Box",
    "section": "",
    "text": "Classifying the Palmer Penguins\nOur data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins.\nLet’s go ahead and acquire the data.\nimport warnings\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\nLet’s take a look:\ndf.head() # first 5 rows\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\nIt’s always useful to get acquainted with the “basics” of the data. For example, how many rows and columns do we have?\ndf.shape # (rows, columns)\n\n(344, 17)\ndf.dtypes \n\nstudyName               object\nSample Number            int64\nSpecies                 object\nRegion                  object\nIsland                  object\nStage                   object\nIndividual ID           object\nClutch Completion       object\nDate Egg                object\nCulmen Length (mm)     float64\nCulmen Depth (mm)      float64\nFlipper Length (mm)    float64\nBody Mass (g)          float64\nSex                     object\nDelta 15 N (o/oo)      float64\nDelta 13 C (o/oo)      float64\nComments                object\ndtype: object\nHere’s the question we’ll ask today about this data set:",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#classifying-the-palmer-penguins",
    "href": "chapters/02-black-box-classification.html#classifying-the-palmer-penguins",
    "title": "2  Classification as a Black Box",
    "section": "",
    "text": "Image source: @allisonhorst\n\n\n\nThe Palmer Penguins data was originally collected by Gorman, Williams, and Fraser (2014) and was nicely packaged and released for use in the data science community by Horst, Hill, and Gorman (2020). You can find a very concise summary of the main workflow using a similar data set in Vanderplas (2016).\n\n\n\n\n The df variable holds a pandas.DataFrame object. You can think of a data frame as a table of data with a variety of useful behaviors for data manipulation and visualization.You can learn much more about the capabilities of pandas.DataFrame objects in Chapter 3 of Vanderplas (2016)\n\n\n\n\nWhat are the data types of the columns? str columns are represented with the generic object in Pandas.\n\n\n\nGiven some physiological measurements of a penguin, can we reliably infer its species?",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#data-preparation",
    "href": "chapters/02-black-box-classification.html#data-preparation",
    "title": "2  Classification as a Black Box",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe can select our desired columns from the data frame, operate on them, and make assignments to them using the data-frame-as-dictionary paradigm explored in Vanderplas (2016).\nIn applied data science, at least 80% of the work is typically spent acquiring and preparing data. Here, we’re going to do some simple data preparation directed by our question. It’s going to be convenient to shorten the Species column for each penguin. Furthermore, for visualization purposes today we are going to focus on the Culmen Length (mm) and Culmen Depth (mm) columns.\n\n# use only these three columns\ndf = df[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Species\"]] \n\n# remove any rows that have missing data in any of the selected columns. \ndf = df.dropna()\n\n# slightly advanced syntax: \n# replace the column with the first word in each entry\ndf[\"Species\"] = df[\"Species\"].str.split().str.get(0)\n\nLet’s take a look at what we’ve done so far:\n\ndf.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nSpecies\n\n\n\n\n0\n39.1\n18.7\nAdelie\n\n\n1\n39.5\n17.4\nAdelie\n\n\n2\n40.3\n18.0\nAdelie\n\n\n4\n36.7\n19.3\nAdelie\n\n\n5\n39.3\n20.6\nAdelie\n\n\n\n\n\n\n\n\nAs another preprocessing step, we are going to add transformed labels represented as integers.\n\n# for later: assign an integer to each species\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf[\"species_label\"] = le.fit_transform(df[\"Species\"])\n\nfor i, c in enumerate(le.classes_):\n    print(f\"Class number {i} represents {c} penguins.\")\n\nClass number 0 represents Adelie penguins.\nClass number 1 represents Chinstrap penguins.\nClass number 2 represents Gentoo penguins.\n\n\nNow our data looks like this:\n\ndf.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nSpecies\nspecies_label\n\n\n\n\n0\n39.1\n18.7\nAdelie\n0\n\n\n1\n39.5\n17.4\nAdelie\n0\n\n\n2\n40.3\n18.0\nAdelie\n0\n\n\n4\n36.7\n19.3\nAdelie\n0\n\n\n5\n39.3\n20.6\nAdelie\n0\n\n\n\n\n\n\n\n\n\nTrain-Test Split\nWhen designing predictive models, it’s important to evaluate them in a context that simulates the prediction application as accurately as possible. One important way we do this is by performing a train-test split. We keep most of the data as training data which we’ll use to design the model. We’ll hold out a bit of the data as testing data, which we’ll treat as unseen and only use once we are ready to evaluate our final design. The testing data simulates the idea of “new, unseen data” – exactly the kind of data on which it would be useful for us to make predictions!\n\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, test_size = 0.2)\n\nLet’s check the size of our two split data sets:\n\ndf_train.shape, df_test.shape\n\n((273, 4), (69, 4))\n\n\nNow we’re going to forget that df_test exists for a while. Instead, we’ll turn our attention to analysis, visualization and modeling.",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#data-analysis-and-visualization",
    "href": "chapters/02-black-box-classification.html#data-analysis-and-visualization",
    "title": "2  Classification as a Black Box",
    "section": "Data Analysis and Visualization",
    "text": "Data Analysis and Visualization\nAs a first step, it’s useful to understand how many of each species there are in the training data:\nThis is an example of a “split-apply-combine” operation (Wickham 2011). We split the dataframe into three groups depending on the species label, apply an operation (in this case, computing the number of rows), and then combine the results into a single object. Pandas implements split-apply-combine primarily through the groupby method and several associated functions. There are some nice examples of split-apply-combine in Pandas in Vanderplas (2016).\n\ndf_train.groupby(\"Species\").size()\n\nSpecies\nAdelie       118\nChinstrap     56\nGentoo        99\ndtype: int64\n\n\nThere are more Adelie penguins than Chintraps or Gentoos in this data set. Here are the proportions:\n\ndf_train.groupby(\"Species\").size() / df_train.shape[0] # divide by total rows\n\nSpecies\nAdelie       0.432234\nChinstrap    0.205128\nGentoo       0.362637\ndtype: float64\n\n\nSo, over 40% of the penguins in the data are Adelie penguins. One important consequence of this proportion is the base rate of the classification problem. The base rate refers to how well we could perform at prediction if we did not use any kind of predictive modeling, but instead simply predicted the most common class for every penguin. Here, if we always predicted “Adelie” for the species, we’d expect to be right more than 40% of the time. So, a minimal expectation of anything fancier we do is that it should be correct much more than 40% of the time.\nNow let’s take a look at our (training) data and see whether our chosen columns look like they have a chance of predicting the penguin species. We’ll show the plot both without and with the species labels.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\np1 = sns.scatterplot(df_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", ax = ax[0], color = \"darkgrey\")\np2 = sns.scatterplot(df_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue = \"Species\", style = \"Species\", ax = ax[1])\n\n\n\n\nThese plots are generated using the Seaborn library for Python. Seaborn is a high-level wrapper around the classical matplotlib library for data visualization. Although Matplotlib is very flexible, Seaborn is optimized for visualizing data contained in Pandas data frames. You can find many examples of creating Seaborn plots in the official gallery, and many tips and examples for matplotlib in Vanderplas (2016).\n\n\n\n\nWe can think of the lefthand side as “what the model will see:” just physiological measurements with no labels. On the right we can see the data with its species labels included. We can see that the species are divided into clusters: Adelie penguins have measurements which tend to be similar to other Adelies; Chinstraps are similar to other Chinstraps, etc.\nThis pattern is promising! The approximate separation of the species suggests that a machine learning model which predicts the species label from these measurements is likely to be able to beat the base rate.",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#modeling-and-model-selection",
    "href": "chapters/02-black-box-classification.html#modeling-and-model-selection",
    "title": "2  Classification as a Black Box",
    "section": "Modeling and Model Selection",
    "text": "Modeling and Model Selection\nLet’s go ahead and fit some models! We’re going to fit two models that are pre-implemented in the package scikit-learn. For now, you can think of these models as black-box algorithms that accept predictor variables as inputs and return a predicted target as an output. In our case, the predictor variables are the culmen length and culmen depth columns, while the target we are attempting to predict is the species. Later on, we’ll learn more about how some of these models actually work.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\nIt’s convenient to split our data into predictors \\(\\mathbf{X}\\) and targets \\(\\mathbf{y}\\). We need to do this once for each of the training and test sets.\n\npredictor_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\ntarget_col = \"species_label\"\n\nX_train = df_train[predictor_cols]\ny_train = df_train[target_col]\n\nX_test = df_test[predictor_cols]\ny_test = df_test[target_col]\n\nLet’s take a quick look at X_train\n\nX_train\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\n\n\n\n\n341\n50.4\n15.7\n\n\n245\n46.1\n15.1\n\n\n168\n50.3\n20.0\n\n\n45\n39.6\n18.8\n\n\n156\n52.7\n19.8\n\n\n...\n...\n...\n\n\n253\n59.6\n17.0\n\n\n87\n36.9\n18.6\n\n\n108\n38.1\n17.0\n\n\n249\n50.0\n15.3\n\n\n14\n34.6\n21.1\n\n\n\n\n273 rows × 2 columns\n\n\n\n\nWe’ll go in-depth on logistic regression later in this course.\nNow we’re ready to fit our first machine learning model. Let’s try logistic regression! In the Scikit-learn API, we first need to instantiate the LogisticRegression() class, and then call the fit() method of this class on the training predictors and targets.\n\nLR = LogisticRegression()\nm = LR.fit(X_train, y_train)\n\nSo, uh, did it work? The LogisticRegression() class includes a handy method to compute the accuracy of the classifier:\n\nLR.score(X_train, y_train)\n\n0.9743589743589743\n\n\nWow! Much better than the base rate. Note that this is the accuracy on the training data. In theory, accuracy on the test data could look very different.\nA useful way to visualize models with two numerical predictors is via decision regions. Each region describes the set of possible measurements that would result in a given classification.\nYou can unfold this code to see a simple implementation of a function for plotting decision regions which wraps the plot_decision_regions function of the mlxtend package.\n\n\nCode\ndef decision_regions(X, y, model, title):\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        ax = plot_decision_regions(X_train.to_numpy(), y_train.to_numpy(), clf = model, legend = 2)\n\n        handles, labels = ax.get_legend_handles_labels()\n        ax.legend(handles, \n                le.classes_, \n                framealpha=0.3, scatterpoints=1)\n\n        ax.set(xlabel = \"Culmen Length (mm)\", ylabel = \"Culmen Depth (mm)\", title = f\"{title}: Accuracy = {model.score(X, y):.3f}\")\n\ndecision_regions(X_train, y_train, LR, \"Decision Regions for Logistic Regression\")\n\n\n\n\n\n\n\n\n\nYou can learn more about how support vector machines work in Vanderplas (2016). We’ll also study these models later in the course.\nWhile we’re at it, let’s try fitting a different classifier, also supplied by Scikit-learn. This classifier is called support vector machine (SVM).\n\nSVM = SVC(gamma = 5)\nSVM.fit(X_train, y_train)\ndecision_regions(X_train, y_train, SVM, \"Decision Regions for Support Vector Machine\")\n\n\n\n\n\n\n\n\nWow! The support vector machine classifier achieved even higher accuracy on the training data. This is enabled by the greater flexibility of the SVM. Flexibility comes from a lot of places in machine learning, and generally refers to the ability of models to learn complicated decision boundaries like the ones shown here.\nBut is this increased flexibility a good thing? You might look at this predictor and think that something funny is going on. For example, shouldn’t a point on the bottom right be more likely to be a Gentoo penguin than an Adelie?…\n\nSimulating Evaluation: Cross-Validation\nNow we have two competing classification models: logistic regression and support vector machine. Which one is going to do the best job of prediction on totally new, unseen data? We could go ahead and evaluate on our test set, but for statistical reasons we need to avoid doing this until we’ve made a final choice of classifier.\nVanderplas (2016) has more on cross-validation and overfitting. We’ll confront overfitting agian many times in this course.\nIn order to make an assessment, we can simulate the process of fitting the model and evaluating on “test” data by witholding parts of our training data to use as testing. We split the data into chunks and withold each chunk, using the other chunks to train the data. This is called cross-validation, and it is illustrated in this figure:\n\n\n\nImage source: scikit-learn\n\n\nWe could do this with a janky for-loop, but the nice scikit-learn developers have implemented this for us. Here’s an example of cross-validation with 5 folds. This can take a little while, as there are actually 5 calls to model.fit() happening under the hood each time.\n\nfrom sklearn.model_selection import cross_val_score\n\nFirst let’s compute the cross-validation accuracies for logistic regression:\n\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv=5)\ncv_scores_LR\n\n/Users/philchodrow/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n\n\narray([0.982, 0.945, 0.982, 0.963, 0.981])\n\n\nA convenient way to summarize these results is by computing the average:\n\ncv_scores_LR.mean()\n\n0.9707070707070707\n\n\nLet’s compare to SVM:\n\ncv_scores_SVM = cross_val_score(SVM, X_train, y_train, cv=5)\ncv_scores_SVM.mean()\n\n0.8462626262626263\n\n\nAh! It looks like our SVM classifier was indeed too flexible to do well in predicting data that it hasn’t seen before. Although the SVM had better training accuracy than the logistic regression model, it failed to generalize to the task of unseen prediction. This phenomenon is called overfitting. Dealing with overfitting is one of the fundamental modeling challenges in applied machine learning.",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#model-evaluation",
    "href": "chapters/02-black-box-classification.html#model-evaluation",
    "title": "2  Classification as a Black Box",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nSo far, we’ve fit a logistic regression model and a support vector machine model; compared the two on a cross-validation task; and determined that the logistic regression model is most likely to generalize. Let’s now retrain the logistic regression model on the complete training data and finally evaluate it on the test set:\n\nLR.fit(X_train,y_train) \nLR.score(X_test, y_test)\n\n0.927536231884058\n\n\nNot bad! This is our final estimate for the accuracy of our model as a classification tool on unseen penguin data.\n\nBeyond Accuracy\nAccuracy is a simple measure of how many errors a model makes. In many applications, it’s important to understand what kind of errors the model makes, a topic which we’ll study much more when we come to decision theory in the near future. We can get a quick overview of the kinds of mistakes that a model makes by computing the confusion matrix between the true labels and predictions. This matrix cross-tabulates all the true labels with all the predicted ones.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[33,  0,  0],\n       [ 4,  7,  1],\n       [ 0,  0, 24]])\n\n\nThe entry in the ith row and jth column of the confusion matrix gives the number of data points that have true label i and predicted label j from our model.\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 33 Adelie penguin(s) who were classified as Adelie.\nThere were 0 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 4 Chinstrap penguin(s) who were classified as Adelie.\nThere were 7 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 1 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 24 Gentoo penguin(s) who were classified as Gentoo.",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#recap",
    "href": "chapters/02-black-box-classification.html#recap",
    "title": "2  Classification as a Black Box",
    "section": "Recap",
    "text": "Recap\nIn these notes, we took a very quick tour of the core data science workflow. We considered a simple classification problem in which we acquired some data, cleaned it up a bit, visualized several of its features, used those features to make a predictive classification model, visualized that model, and evaluated its accuracy. Along the way, we encountered the phenomenon of overfitting: models that are too flexible will achieve remarkable accuracy on the training set but will generalize poorly to unseen data. The problem of designing models that are “flexible enough” and “in the right way” is a fundamental driving force in modern machine learning, and the deep learning revolution can be viewed as the latest paradigm for seeking appropriately flexible models.\nSo far, we haven’t attempted to understand how any of these predictive models actually work. We’ll dive into this topic soon.",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#references",
    "href": "chapters/02-black-box-classification.html#references",
    "title": "2  Classification as a Black Box",
    "section": "References",
    "text": "References\n\n\n\n\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” Edited by André Chiaradia. PLoS ONE 9 (3): e90081. https://doi.org/10.1371/journal.pone.0090081.\n\n\nHorst, Allison M, Alison Presmanes Hill, and Kristen B Gorman. 2020. “Allisonhorst/Palmerpenguins: V0.1.0.” Zenodo. https://doi.org/10.5281/ZENODO.3960218.\n\n\nVanderplas, Jacob T. 2016. Python Data Science Handbook: Essential Tools for Working with Data. First edition. Sebastopol, CA: O’Reilly Media, Inc.\n\n\nWickham, Hadley. 2011. “The Split-Apply-Combine Strategy for Data Analysis.” Journal of Statistical Software 40 (1). https://doi.org/10.18637/jss.v040.i01.",
    "crumbs": [
      "Introducing Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/03-score-based-classification.html",
    "href": "chapters/03-score-based-classification.html",
    "title": "3  Score-Based Classification",
    "section": "",
    "text": "To Lend Or Not To Lend?\nBanks are in the business of lending money, and they must often decide when to loan whom how much money and under what terms. When deciding whether to loan a sum of money, there are two major competing questions:\nBanks can try to balance these risks by controlling interest rates. Higher interest rates increase prospective profit if the loan is repaid in full, but also increase the risk that an individual may be unable to keep up with payments.\nThe judgment of whether to extend an individual a loan is handled by human experts. Recently, human experts have been seeking assistance from machine learning algorithms. As in most predictive modeling, the idea is to use the past to predict the future. Here, we’ll consider simple modeling problem in which we aim to learn patterns in when individuals are able to pay off loans, and use these patterns to make predictions.\nWe’ll first load in the pandas package and use the read_csv command to acquire our data for this problem as a pd.DataFrame. This data set was produced by Kaggle; it is a simulated data set based on patterns in real world data, which, of course, is sensitive and confidential. For today, we are only going to focus on the first 1,000 rows of data.\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np \nplt.style.use('seaborn-v0_8-whitegrid')\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/credit_risk_dataset.csv\"\ndf = pd.read_csv(url)\ndf = df.head(1000).copy()\nLet’s take a look at an excerpt of the data.\ndf\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n22\n59000\nRENT\n123.0\nPERSONAL\nD\n35000\n16.02\n1\n0.59\nY\n3\n\n\n1\n21\n9600\nOWN\n5.0\nEDUCATION\nB\n1000\n11.14\n0\n0.10\nN\n2\n\n\n2\n25\n9600\nMORTGAGE\n1.0\nMEDICAL\nC\n5500\n12.87\n1\n0.57\nN\n3\n\n\n3\n23\n65500\nRENT\n4.0\nMEDICAL\nC\n35000\n15.23\n1\n0.53\nN\n2\n\n\n4\n24\n54400\nRENT\n8.0\nMEDICAL\nC\n35000\n14.27\n1\n0.55\nY\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n22\n46610\nRENT\n6.0\nDEBTCONSOLIDATION\nB\n18000\n10.71\n1\n0.39\nN\n3\n\n\n996\n24\n48000\nRENT\n5.0\nPERSONAL\nA\n18000\n6.54\n1\n0.38\nN\n2\n\n\n997\n23\n24000\nOWN\n0.0\nPERSONAL\nA\n8000\n5.79\n0\n0.33\nN\n3\n\n\n998\n25\n55000\nRENT\n2.0\nEDUCATION\nC\n18000\n12.84\n1\n0.33\nN\n2\n\n\n999\n25\n55344\nRENT\n2.0\nHOMEIMPROVEMENT\nB\n18000\n10.99\n1\n0.33\nN\n4\n\n\n\n\n1000 rows × 12 columns\nEach row of this data set describes a single loan and the attributes of the borrower. For visualization purposes, today we are going to focus on just three of the columns:\nOur primary predictive interest is whether or not a borrower is likely to default on a loan. How common is this in our data set?\ndf[\"loan_status\"].mean()\n\n0.553\nIn this data, roughly 55% of borrowers default on their loan. An important aspect of this learning is the base rate for prediction. If we predicted that every borrower would default on a loan, we would be right 55% of the time. So, if we want to find patterns in our data set and use those patterns to make predictions, we should aim for accuracy greater than 55%.\nSo, can we find some patterns? Here is a labeled scatterplot of our simplified data set.\ndef scatter_data(ax, df):\n\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        to_plot = df[df[\"loan_status\"] == i]\n        ax.scatter(to_plot[\"loan_int_rate\"], to_plot[\"loan_percent_income\"], c = to_plot[\"loan_status\"], vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['repaid', 'defaulted'][i]}\", cmap = \"BrBG\", marker = markers[i])\n        ax.legend()\n        ax.set(xlabel = \"Loan interest rate\", ylabel = \"Loan / income ratio\")\n\ndf = df.head(1000)\nfig, ax = plt.subplots(1, 1)\nscatter_data(ax, df)\nAlthough it looks difficult to completely separate the defaulted loans from the loans which were repaid in full, it does look like there is some pattern to find. Loans which were repaid in full concentrate in the lower right corner of the visualization. This makes sense – these are loans which have low interest rates and which are relatively small sums relative to the annual resources of the borrower.\nA very common approach in problems like this one is to assign, to each loan applicant \\(i\\), a score \\(s_i\\) which predicts their likelihood to default on a loan. Higher scores indicate greater reliability. Let’s formulate a linear score function:\n\\[\n\\begin{aligned}\n    s_i = w_1 \\times (\\text{loan interest rate}_i) + w_2 \\times (\\text{loan percent income}_i)\\;.\n\\end{aligned}\n\\]\nWe can write this score function much more compactly by defining a data point\n\\[\n\\begin{aligned}\n    \\mathbf{x}_{i} = \\left(\\text{loan interest rate}_i, \\text{loan percent income}_i\\right)\n\\end{aligned}\n\\]\nand weight vector\n\\[\n\\begin{aligned}\n    \\mathbf{w} = \\left(w_1, w_2\\right)\\;.\n\\end{aligned}\n\\]\nThen, we can compactly write our score function as\n\\[\n\\begin{aligned}\n    s_i = \\langle \\mathbf{w}, \\mathbf{x}_i\\rangle\\;.\n\\end{aligned}\n\\tag{3.1}\\]\nLet’s implement this score in Python.\ndef linear_score(w, x0, x1):\n    return w[0]*x0 + w[1]*x1\nNow we can plot this score function in the data space.\nShow code\ndef plot_score(ax, score_fun, w, df):\n    \"\"\"\n    Plot a given score function on axis ax with weights w and data df. \n    \"\"\"\n\n    x0_col = \"loan_int_rate\"\n    x1_col = \"loan_percent_income\"\n\n    x0_min, x0_max = df[x0_col].min(), df[x0_col].max()\n    x1_min, x1_max = df[x1_col].min(), df[x1_col].max()\n\n    x0 = np.linspace(x0_min, x0_max, 101)\n    x1 = np.linspace(x1_min, x1_max, 101)\n\n    X0, X1 = np.meshgrid(x0, x1)\n    S = score_fun(w, X0, X1)\n\n    ticks = np.linspace(0, 101, 6)\n\n    im = ax.contourf(X0, X1, S, origin = \"lower\", extent = (x0_min, x0_max, x1_min, x1_max),  cmap = \"BrBG\", vmin = 2*S.min() - S.max(), vmax = 2*S.max() - S.min())\n    \n    ax.set(xlabel = \"Loan interest rate\", ylabel = \"Loan / income ratio\")\n    \n    cbar = plt.colorbar(im, )\n    cbar.set_label(\"Predicted score\")\n\ndef score_viz(score_fun, w, df):\n    fig, ax = plt.subplots(1, 2, figsize = (7, 2.7)) \n    plot_score(ax[0], score_fun, w, df)\n    plot_score(ax[1], score_fun, w, df)\n    scatter_data(ax[1], df)\n    plt.tight_layout()\nTo see the scores, we need to make an initial choice about the weight vector \\(\\mathbf{w}\\).\nw = np.array([0.4, -2])\nscore_viz(linear_score, w, df)\nHmmm, that doesn’t look so good. Ideally, we’d like the higher scores to line up with the borrowers who defaulted, and the lower scores to line up with the borrowers who fully repaid their loans. Can we find a better choice?\nw = np.array([0.01, 1.1])\nscore_viz(linear_score, w, df)",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Score-Based Classification</span>"
    ]
  },
  {
    "objectID": "chapters/03-score-based-classification.html#what-about-nonlinear-scores",
    "href": "chapters/03-score-based-classification.html#what-about-nonlinear-scores",
    "title": "3  Score-Based Classification",
    "section": "What About Nonlinear Scores?",
    "text": "What About Nonlinear Scores?\nYou’ll notice in Figure 3.1 that the decision boundary is a straight line. This is due to the way that we chose to compute scores. Recall that the score function we used is \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\). Since we imposed a threshold \\(t\\), the decision boundary is defined by the equation \\(t = s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\). Generically, this is the equation of a hyperplane (technically, an affine subspace). The dimension of this space is \\(p-1\\), where \\(p\\) is the number of features. Here we have two features, so the decision boundary is a \\(2-1=1\\)-dimensional subspace–i.e. a line.\nWhat if we think a curved decision boundary would be more appropriate? In that case, we need to define a score function that factors in the features in a nonlinear way.\nWe started by representing each point as a 2-vector of predictors \\(\\mathbf{x} = \\left(\\text{loan interest rate}, \\text{loan percent income}\\right)\\). Let’s now add a feature map \\(\\phi\\) that accepts this vector and adds three nonlinear functions of the predictors:\n\\[\n\\begin{aligned}\n    \\phi(\\mathbf{x}) =\n        \\left(\\begin{matrix}\n            \\text{loan interest rate} \\\\\n            \\text{loan percent income} \\\\\n            \\left(\\text{loan interest rate}\\right)^2 \\\\  \n            \\left(\\text{loan percent income}\\right)^2 \\\\\n            \\text{loan interest rate} \\times \\text{loan percent income}\n        \\end{matrix}\\right)\n\\end{aligned}\n\\]\nBecause the new features are order-2 polynomials in the predictors, this feature map is often called the quadratic feature map.\nWe’ll still use an inner product to compute our score but now the formula will be  \\[\n\\begin{aligned}\n    s_i = \\langle \\mathbf{w}, \\phi(\\mathbf{x}_i) \\rangle\\;.\n\\end{aligned}\n\\]In order for this formula to make sense, we now need \\(\\mathbf{w}\\in \\mathbb{R}^5\\).\nHere’s an implementation of a score function with quadratic features:\n\ndef quadratic_score(w, X0, X1):\n    return w[0]*X0 + w[1]*X1 + w[2]*X0**2 + w[3]*X1**2 + w[4]*X0*X1\n\nNow we can set a new vector of weights \\(\\mathbf{w}\\in \\mathbb{R}^5\\) and a threshold \\(t\\).\n\nw = np.array([0.01, 1, 0.0005, 0.6, 0.001])\nthreshold = 0.5\n\nOur classification now looks like this:\n\nfig, ax = plt.subplots(1, 1)\nplot_score(ax, quadratic_score, w, df)\nscatter_data(ax, df)\nplot_threshold(ax, quadratic_score, w,  df, threshold)\n\n\n\n\n\n\n\nFigure 3.2: quadratic score-based classification.\n\n\n\n\n\nHow accurate were we?\n\ndf[\"decision\"] = predict(quadratic_score, w, threshold, df)\n(df[\"decision\"] == df[\"loan_status\"]).mean()\n\n0.777\n\n\nOur nonlinear score function was very slightly more accurate than our linear score function on training data. A few things to keep in mind:\n\nPerformance on training data is not always a reliable indicator of performance on unseen data.\nAdding nonlinear features is one way of adding flexibility to a model, allowing that model to learn complicated, “wiggly” decision patterns. As we saw with the Palmer penguins case study, too much model flexibility can lead to worse predictive performance. We’ll regularly revisit the problem of balancing flexibility/features against predictive generalization throughout these notes.\n\n\nRecap\nSo, we looked at a simplified data set in which we were able to observe some features of each prospective borrower \\(i\\) in the form of a vector \\(\\mathbf{x}_i\\). We then computed a score for each borrower \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) and used a threshold to decide whether or not to make a loan: the loan is approved if \\(s_i \\leq t\\) for a chosen threshold \\(t\\). We can think of this as a decision-making model for the loan approval problem.\nIs that the end of the story? Of course not! There are many questions remaining.\n\nModel Evaluation: How do we actually measure whether our decision-making model is good or not? Is accuracy the right measure? Is computing accuracy on the training data reliable? How would the model perform on unseen data that wasn’t used to decide \\(\\mathbf{w}\\) or \\(t\\)? What other ways could we measure the performance of models?\nLegitimacy: Is it morally and politically appropriate to use algorithmic decision-making in the context of loan applications? What is the potential for disparate harm? What is the potential for contributing to the reinforcement of historically disparity? In what cases could algorithmic loan-making be appropriate in a democratic society? In what cases could it constitute a violation of personal political or moral rights?\nTask Choice: How was the data collected? Is it complete? Why did I choose a certain set of predictors and targets? Are my predictors and targets reliable measurements of what they claim to represent? Whose interests are served by the existence of a machine learning model that completes this task?\nAlgorithm Design: What algorithm was used to find the model (i.e. the separating line)? Is that algorithm guaranteed to converge? Will it converge quickly? Would a different algorithm find a better model? Or would it find a model that is equally good more quickly?\nVectorization: Instead of classifying points in a measurement space, how could I instead classify images, videos, or bodies of text?\n\nWe’ll discuss all of these questions – in approximately this order – later in these notes.",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Score-Based Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04-decision-theory.html",
    "href": "chapters/04-decision-theory.html",
    "title": "4  Decision Theory in Classification",
    "section": "",
    "text": "Last time…\n…we considered a prediction problem in which we observed \\(p\\) attributes of prospective borrower \\(i\\) in the form of a vector \\(\\mathbf{x}_i \\in \\mathbb{R}^p\\). We then formed a score for prospective borrower \\(i\\) using a weight vector \\(\\mathbf{w}\\in \\mathbb{R}^p\\) and an inner product:\n\\[\n\\begin{aligned}\n    s_i = \\langle \\mathbf{x}_i, \\mathbf{w}  \\rangle\\;.\n\\end{aligned}\n\\tag{4.1}\\]\nThen, we classified prospective borrowers into two categories based on a threshold \\(t \\in \\mathbb{R}\\):\nEquation 4.1 says that the score should be computed as a linear function of the features \\(\\mathbf{x}_i\\). Models with this property are called linear models and are fundamental in both classification and regression tasks.",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Theory in Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04-decision-theory.html#last-time",
    "href": "chapters/04-decision-theory.html#last-time",
    "title": "4  Decision Theory in Classification",
    "section": "",
    "text": "We also developed the ability to compute nonlinear scores by instead computing the score as \\(s_i = \\langle \\mathbf{w},\\phi(\\mathbf{x}_i) \\rangle\\), where \\(\\phi\\) was a feature map that computed nonlinear functions of the entries of \\(\\mathbf{x}_i\\). For reasons that we’ll learn about when we study the theory of machine learning, this is still called a linear model, due to the fact that the score is a linear function of the vector \\(\\mathbf{w}\\). In this set of notes, we’ll always assume that \\(\\mathbf{x}\\) has already had a feature map applied to it, so that we can just focus on the simpler form of Equation 4.1.\n\n\n\nBorrowers who receive a loan had the property \\(s_i \\leq t\\).\nBorrowers who do not receive a loan have the property \\(s_i &gt; t\\).\n\n\nIn this set of notes, we are going to focus on one of the many questions we might ask about this framework: how do we choose the threshold \\(t\\)?  As we’ll see, this is a surprisingly tricky question that depends heavily on context.We’ll study later how to find \\(\\mathbf{w}\\).",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Theory in Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04-decision-theory.html#lending-data-set",
    "href": "chapters/04-decision-theory.html#lending-data-set",
    "title": "4  Decision Theory in Classification",
    "section": "Lending Data Set",
    "text": "Lending Data Set\nTo illustrate our discussion, we are going to pull up the lending data set from the previous section.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/credit_risk_dataset.csv\"\n\n\n\ndf_all = pd.read_csv(url)\ndf = df_all[[\"loan_int_rate\", \"loan_percent_income\", \"loan_status\"]]\ndf = df.dropna()\n\nFollowing the usual paradigm in machine learning, we’re going to incorporate two elements which we previously saw when studying the Palmer penguins. First, we are going to hold off a part of our data set that we will not use for making any choices about how we design our decision algorithm. This held-off part of the data is called the test set. We’ll use it for a final evaluation of our model’s performance.\n\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size = 0.2, random_state = 123) # 20% test set\n\nNext, we’ll distinguish our predictor and target variables in each of the train and test sets.\n\nX_train = df_train[[\"loan_int_rate\", \"loan_percent_income\"]]\ny_train = df_train[\"loan_status\"]\n\nX_test = df_test[[\"loan_int_rate\", \"loan_percent_income\"]]\ny_test = df_test[\"loan_status\"]",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Theory in Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04-decision-theory.html#vectorized-computation-of-scores",
    "href": "chapters/04-decision-theory.html#vectorized-computation-of-scores",
    "title": "4  Decision Theory in Classification",
    "section": "Vectorized Computation of Scores",
    "text": "Vectorized Computation of Scores\nSuppose that we have a weight vector \\(\\mathbf{w}\\) and that we’d like to choose a threshold \\(t\\). To do this, we will compute all the scores on the training data and do some experiments. How should we compute training scores? As we know, the \\(i\\)th score is given by Equation 4.1. To compute scores for all \\(n\\) of our training points, we could write a loop like this: In our case, \\(n =\\) 23572, the number of rows in the training data.\n\ns = [] # vector of scores\nfor i in range(n):\n    s.append(compute_score(X[i], w))\n\nwhere X[i] is the ith data point \\(\\mathbf{x}_i\\) and compute_score is a function that computes the score according to Equation 4.1. However, there’s a better way to do this if we step back from code into math for a moment. If \\(\\mathbf{s} \\in \\mathbb{R}^n\\) is a vector whose \\(i\\)th entry is the score \\(s_i\\), then we have\n\\[\n\\begin{aligned}\n    \\mathbf{s} = \\left(\n        \\begin{matrix}\n            \\langle \\mathbf{x}_1, \\mathbf{w} \\rangle \\\\\n            \\langle \\mathbf{x}_2, \\mathbf{w} \\rangle \\\\\n            \\vdots \\\\\n            \\langle \\mathbf{x}_n, \\mathbf{w} \\rangle\n        \\end{matrix}\n        \\right) = \\mathbf{X}\\mathbf{w}\\;,\n\\end{aligned}\n\\]\nwhere we have defined the predictor matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\)\n\\[\n\\begin{aligned}\n    \\mathbf{X} = \\left[\n        \\begin{matrix}\n            - \\mathbf{x}_1 -  \\\\\n            -\\mathbf{x}_2-  \\\\\n            \\vdots \\\\\n            -\\mathbf{x}_n -\n        \\end{matrix}\n        \\right] =\n        \\left[\n        \\begin{matrix}\n            x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n            x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n            \\vdots \\\\\n            x_{n1} & x_{n2} & \\cdots & x_{np}\n        \\end{matrix}\n        \\right]\\;.\n\\end{aligned}\n\\]\nThis is good news because it simplifies our life both mathematically and in code: the Numpy package supplies very fast matrix multiplication:\n\ndef linear_score(X, w):\n    return X@w\n\nNow, given \\(\\mathbf{w}\\), we can compute all the scores at once.\n\nw = np.array([0.01, 1.0])\ns = linear_score(X_train, w)\n\nHere is a histogram of the scores we just computed:\n\nhist = plt.hist(s)\nlabs = plt.gca().set(xlabel = r\"Score $s$\", ylabel = \"Frequency\")",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Theory in Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04-decision-theory.html#types-of-error",
    "href": "chapters/04-decision-theory.html#types-of-error",
    "title": "4  Decision Theory in Classification",
    "section": "Types of Error",
    "text": "Types of Error\nNow that we have the scores, we can easily simulate decision-making with a given threshold. For example, the proportion predicted to default on their loan with a given threshold \\(t\\) can be computed like this:\n\nt = 0.4\npreds = s &gt;= t\npreds.mean()\n\n0.15386899711522145\n\n\nSo, how should we choose the threshold \\(t\\)? One possibility would be to try to choose the threshold in a way that maximizes the training accuracy, the number of times that the prediction agrees with the actual outcome (repaid or default) on the training data. Here’s an example of a quick grid search:\n\nfor t in np.linspace(0, 1, 11):\n    y_pred = s &gt;= t\n    acc = (y_pred == y_train).mean()\n    print(f\"A threshold of {t:.1f} gives an accuracy of {acc:.2f}.\")\n\nA threshold of 0.0 gives an accuracy of 0.22.\nA threshold of 0.1 gives an accuracy of 0.23.\nA threshold of 0.2 gives an accuracy of 0.46.\nA threshold of 0.3 gives an accuracy of 0.71.\nA threshold of 0.4 gives an accuracy of 0.82.\nA threshold of 0.5 gives an accuracy of 0.80.\nA threshold of 0.6 gives an accuracy of 0.79.\nA threshold of 0.7 gives an accuracy of 0.78.\nA threshold of 0.8 gives an accuracy of 0.78.\nA threshold of 0.9 gives an accuracy of 0.78.\nA threshold of 1.0 gives an accuracy of 0.78.\n\n\nHowever, accuracy is not always the most relevant measure. For example, Field and Stream estimates that there are, globally, approximately 70 unprovoked shark attacks each year. Since the population of the world is currently around \\(8.1\\times 10^9\\) people, the average probability that a specific individual will suffer an unprovoked shark attack in a year is approximately \\(70 / (8.1 \\times 10^9) \\approx 8.6 \\times 10^{-9}\\). So, if we created a shark attack predictor which always predicted “no shark attack,” our model would be correct approximately 99.999999% of the time. However, this model wouldn’t be very useful, and wouldn’t have anything to tell us about the activities that increase or reduce the risk of experience an attack.\nA second reason we may wish to measure something other than accuracy has to do with asymmetrical costs of error. If we incorrectly predict that an individual will suffer a shark attack but no attack occurs, this is not that big a problem. Yes, we were wrong, but no one got hurt. In contrast, if we incorrectly predict that an individual will not suffer a shark attach, then this is a big problem which potentially involves grievous bodily injury, death, trauma, legal liability, etc. So, in designing our predictor, we might want to prioritizing avoiding the second kind of error, even if that leads us to make more of the first kind of error.\nWhat are the types of error? For a binary outcome with a binary predictor, there are four possibilities:\n\n\n\nTable 4.1: Types of correct classifications and errors in a binary classification problem.\n\n\n\n\n\n\nAbbreviation\nTrue Outcome\nPredicted Outcome\n\n\n\n\nTrue positive\nTP\n1\n1\n\n\nFalse negative\nFN\n1\n0\n\n\nFalse positive\nFP\n0\n1\n\n\nTrue negative\nTN\n0\n0\n\n\n\n\n\n\nGiven a vector of true outcomes \\(\\mathbf{y}\\) and a vector of predictions \\(\\hat{\\mathbf{y}}\\), we can calculate frequencies of each outcome. For example, here are the false positives associated with a given threshold value:\n\nt = 0.5\ny_pred = s &gt;= t \n\n# number where outcome == 0 and prediction == 1\n((y_train == 0)*(y_pred == 1)).sum()\n\n286\n\n\nIn practice, it’s more convenient to compute all the error rates at once using the confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_train, y_pred)\n\narray([[18062,   286],\n       [ 4319,   905]])\n\n\nThe layout of the confusion matrix is:\ntrue negative,  false positive \nfalse negative, true positive\nIt is common to normalize these counts into rates:\n\n\n\n\n\n\n\n\n\n\nAbbreviation\nFormula\n\n\n\n\n\nTrue negative rate\nTNR\n\\(\\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}}\\)\n\n\n\nFalse positive rate\nFPR\n\\(\\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}}\\)\n\n\n\nFalse negative rate\nFNR\n\\(\\frac{\\mathrm{FN}}{\\mathrm{TP} + \\mathrm{FN}}\\)\n\n\n\nTrue positive rate\nTPR\n\\(\\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}\\)\n\n\n\n\nIntuitively, the TPR measures the proportion of the time that the classifier predicts the correct (positive) label when the true outcome was positive. Similarly, the FPR measures the proportion of the time that the classifier predicts the incorrect (positive) label when the true outcome was negative. Because \\(\\mathrm{TPR} = 1 - \\mathrm{FNR}\\) and \\(\\mathrm{FPR} = 1 - \\mathrm{TNR}\\), folks usually only bother remembering and using \\(\\mathrm{TPR}\\) and \\(\\mathrm{FNR}\\).\nRather than computing these by hand, Scikit-learn offers a handy argument to confusion_matrix for computing these automatically and simultaneously:\n\nconfusion_matrix(y_train, y_pred, normalize = \"true\")\n\narray([[0.984, 0.016],\n       [0.827, 0.173]])\n\n\nLet’s do a quick check against the FPR using manual vectorized code. Cases where y_pred == 1 correspond to positive predictions, while cases where y_train == 0 correspond to true negative outcomes.\n\n# agrees with the top right corner of the normalized confusion matrix\n((y_pred == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n\n0.015587529976019185\n\n\n\nThe ROC Curve\nFor any given value of the threshold \\(t\\), we can compute the TPR and FPR. We can think of this process as defining a parametrized function, a curve in TPR-FPR space. This curve is the ROC curve ROC stands for “receiver operating characteristic,” a term that reflects the origin of the curve in detection of objects by radar.\nTo compute an ROC curve, we simply need to compute the TPR and FPR for many different values of the threshold \\(t\\) and plot them.\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\n\nnum_thresholds = 101\n\nFPR = np.zeros(num_thresholds)\nTPR = np.zeros(num_thresholds)\nT = np.linspace(s.min()-0.1, s.max()+0.1, num_thresholds)\ns    = linear_score(X_train, w)\n\nfor i in range(num_thresholds):\n    t = T[i]\n    preds    = s &gt;= t\n    FPR[i]   = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR[i]   = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\nax.plot(FPR, TPR, color = \"black\")\nax.plot([0,1], [0,1], linestyle=\"--\", color = \"grey\")\nax.set_aspect('equal')\n\nlabs = ax.set(xlabel = \"False Positive Rate\", ylabel = \"True Positive Rate\", title = \"ROC Curve\")\n\n\n\n\n\n\n\n\nWe can think of the ROC curve as a description of all the possible tradeoffs between the TPR and FPR that are achievable for a given score as we vary the threshold \\(t\\). For example, the curve tells us that if we are willing to tolerate a false positive rate of 0.40, then the best true positive rate we can achieve is approximately 0.77.\nROC curves are often used as a measure of the ability of a score function to classify data into two groups. Curves that bend farther towards the upper left corner of the plot are generally viewed as more effective classifiers. The area under the curve (AUC) is sometimes used as a single quantitative measure describing the classification quality.\n\n\nCost of Errors and Optimal Thresholding\nHow do we choose the tradeoff that works best for us? To answer this kind of question, we need to reflect back on the purpose for which we are building a classifier. According to Table 4.1, there are two ways to be correct (true positive, true negative) and two ways to make an error (false positive, false negative). In order to choose an appropriate tradeoff, we need to think about the benefit of being right in relation to the cost of being wrong.\nA logical way for a bank to approach this problem would be from the perspective of profit-maximization. In the lending business, a bank can make money when loans are fully repaid with interest, but lose money (usually much more) when an individual defaults on the loan. To keep the problem simple, suppose that the bank gains $1 every time they make a loan which is successfully paid back, and that the bank loses $2 every time they make a loan which ends in default. The first scenario happens when the bank makes a true positive identification, while the second case happens when the bank makes a false negative classification.  For a given threshold, the expected gain for the bank when making a loan is thenRemember that the “positive” outcome in this data set is default.\n\\[\n\\begin{aligned}\n    \\mathbb{E}[\\mathrm{gain}] = 1\\times \\text{TN} - 2\\times \\text{FN}\\;.\n\\end{aligned}\n\\]\nLet’s plot the expected gain as a function of the threshold:\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\ncost_of_FN = -2.0\ngain_of_TN = 1.0\n\ngain =  gain_of_TN*TNR  + cost_of_FN*FNR \n\nplt.plot(T, gain)\nplt.gca().set(ylim = (-0.2, 0.2), xlim = (0, 0.5))\nlabs = plt.gca().set(xlabel = r\"Threshold $t$\", ylabel = \"Expected profit per loan\")\n\n\n\n\n\n\n\n\nFor these costs, we observe that the bank can make a small expected profit (roughly 17 cents per loan) by using the given score function with threshold of roughly \\(t \\approx 0.21\\). Note that this is very different from the value of the thresold \\(t \\approx 0.4\\) which maximized the unweighted accuracy of the predictor.\nAt this stage, we could go on to estimate the profit gained by using this predictor and threshold on the test data set instead of the training data set. The code below simply consolidates the many steps that we have walked through in these notes, applied to the test data.\n\nt = 0.21\n\n# compute the scores\ns     = linear_score(X_test, w)\npreds = s &gt;= t\n\n# compute error rates\nFPR   = ((preds == 1) & (y_test == 0)).sum() / (y_test == 0).sum()\nTPR   = ((preds == 1) & (y_test == 1)).sum() / (y_test == 1).sum()\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\n# compute the expected gain\ngain = gain_of_TN*TNR  + cost_of_FN*FNR \ngain\n\n0.17641653321131712\n\n\nOur performance on the test data is very slightly worse than our performance on the training data, which is to be expected.",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Theory in Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04-decision-theory.html#recap",
    "href": "chapters/04-decision-theory.html#recap",
    "title": "4  Decision Theory in Classification",
    "section": "Recap",
    "text": "Recap\nIn these notes, we studied a simple question: given a score \\(s_i = \\langle \\mathbf{x}_i, \\mathbf{w}\\rangle\\), how should we convert that score into a yes/no decision? We found that adjusting the threshold can have major consequences for the accuracy of the resulting classification algorithm, but also that pure accuracy may not be the most relevant metric to measure or optimize. We computed the ROC curve of the score, which is a visual indicator of the overall ability of the score function to balance the false positive rate against the true positive rate. Finally, we explored the possible tradeoffs between different kinds of errors by considering a simplified scenario in which different kinds of errors have different costs associated with them. We found that the threshold that optimizes expected gain under this setting can be very different from the threshold that optimizes unweighted accuracy.",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Theory in Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04-decision-theory.html#who-sets-the-cost-who-pays-the-cost",
    "href": "chapters/04-decision-theory.html#who-sets-the-cost-who-pays-the-cost",
    "title": "4  Decision Theory in Classification",
    "section": "Who Sets The Cost? Who Pays the Cost?",
    "text": "Who Sets The Cost? Who Pays the Cost?\nIn our analysis above, we assumed a simple optimization objective: the bank is going to maximize its net profit. In formulating this objective, we made assumptions about the costs of different outcomes – to the bank. It’s important to note that the costs of errors to the bank may look very different from the costs of those errors to individuals. For example, if the bank’s prediction system recommends that an individual be denied a loan and the bank acts on this recommendation, then the bank pays no cost. On the other hand, the individual may experience major costs, depending on the purpose for which the loan was requested.\nThis data set includes a coarse description of the purpose of each loan:\n\ndf_all.groupby(\"loan_intent\").size()\n\nloan_intent\nDEBTCONSOLIDATION    5212\nEDUCATION            6453\nHOMEIMPROVEMENT      3605\nMEDICAL              6071\nPERSONAL             5521\nVENTURE              5719\ndtype: int64\n\n\nWhat are the costs of being denied access to borrowed funds to pursue education? What about for medical care?\nIt is of fundamental importance to remember that machine learning systems are embedded in social context; that they are generally developed and implemented by people and organizations that occupy positions of power; and that the costs of these systems are often unequally shared by the people they impact. We will discuss these considerations in much greater detail soon.",
    "crumbs": [
      "Fundamentals of Prediction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Theory in Classification</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html",
    "href": "chapters/10-compas.html",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Data Preparation\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nsns.set_style(\"whitegrid\")\nnp.set_printoptions(precision = 3)\npd.set_option('display.precision', 3)\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/compas/compas.csv\"\ncompas = pd.read_csv(url)\nFor today we are only going to consider a subset of columns.\ncols = [\"sex\", \"race\", \"decile_score\", \"two_year_recid\"]\ncompas = compas[cols]\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\nis_white = compas[\"race\"] == \"Caucasian\"\nis_black = compas[\"race\"] == \"African-American\"\n\ncompas = compas[is_white | is_black]\ncompas = compas.copy()\nOur data now looks like this:\ncompas.head()\n\n\n\n\n\n\n\n\n\nsex\nrace\ndecile_score\ntwo_year_recid\n\n\n\n\n1\nMale\nAfrican-American\n3\n1\n\n\n2\nMale\nAfrican-American\n4\n1\n\n\n3\nMale\nAfrican-American\n8\n0\n\n\n6\nMale\nCaucasian\n6\n1\n\n\n8\nFemale\nCaucasian\n1\n0",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html#data-preparation",
    "href": "chapters/10-compas.html#data-preparation",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.This data set was obtained by Angwin et al. (2022) through a public records request. The data comprises two years worth of COMPAS scoring in Broward County, Florida.",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html#preliminary-explorations",
    "href": "chapters/10-compas.html#preliminary-explorations",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Preliminary Explorations",
    "text": "Preliminary Explorations\nLet’s do some quick exploration of our data. How many defendants are present in this data of each sex?\n\ncompas.groupby(\"sex\").size()\n\nsex\nFemale    1219\nMale      4931\ndtype: int64\n\n\nWhat about race?\n\ncompas.groupby(\"race\").size()\n\nrace\nAfrican-American    3696\nCaucasian           2454\ndtype: int64\n\n\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as being produced by computing a score like \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) for each defendant \\(i\\), and then dividing these into the lowest 10% (decile score 1), the next 10% (decile score 2), the next 10% (decile score 3) and so on.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\np = sns.barplot(data = counts, \n                x = \"decile_score\", \n                y = \"n\", \n                hue = \"race\", \n                palette = \"BuPu\", \n                saturation = 0.5)\n\n\n\n\n\n\n\n\nYou may notice that the number of white defendants who receive a given decile score tends to decrease as the score increases, whereas the number of Black defendants remains relatively constant.\nLet’s also take a look at the recidivism rate in the data:\n\ncompas[\"two_year_recid\"].mean()\n\n0.4661788617886179\n\n\nSo, in these data, approximately 47% of all defendants went on to be charged of another crime within the next two years. This is sometimes called the prevalence of the outcome. Although this is not a “good” outcome, it is labeled 1 in the target data and so we refer to this as the “positive” outcome. Prevalence without further specification usually refers to prevalence of the positive outcome.\nThe base rate of prediction accuracy in this problem is 53%: if we always guessed that the defendant was not arrested within two years, we would be right 53% of the time.\nWe can also compute the prevalence broken down by race of the defendant:\n\ncompas.groupby(\"race\")[\"two_year_recid\"].mean()\n\nrace\nAfrican-American    0.514\nCaucasian           0.394\nName: two_year_recid, dtype: float64\n\n\n\n\nWhen interpreting these different prevalences, it is important to remember that\n\nRace is itself a socially-constructed system of human categorization invented by humans with political and economic motives to describe other humans as property (Bonilla-Silva 2018).\n\nThe relation between arrest and actual criminal offense can display racial bias, with effects varying by geography (Fogliato et al. 2021).\nDecisions about which behaviors are criminal are contingent political decisions which have, historically, fallen hardest on Black Americans (Yusef and Yusef 2017).\n\nThe prevalences between the two groups are substantially different. This difference will have major consequences later on for the possibility of different kinds of fairness in classifiers.\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of Angwin et al. (2022), we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\n\ncompas[\"predicted_high_risk\"] = (compas[\"decile_score\"] &gt; 4)\n\nNow that we’ve done that, we can ask: how likely are Black and white defendants to receive positive predictions in this data?\n\ncompas.groupby(\"race\")[\"two_year_recid\", \"predicted_high_risk\"].mean()\n\n/var/folders/xn/wvbwvw0d6dx46h9_2bkrknnw0000gn/T/ipykernel_34470/3539224628.py:1: FutureWarning:\n\nIndexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n\n\n\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\nrace\n\n\n\n\n\n\nAfrican-American\n0.514\n0.588\n\n\nCaucasian\n0.394\n0.348\n\n\n\n\n\n\n\n\nBlack defendants are substantially more likely to receive a positive prediction than white defendants, and the disparity is larger than the observed prevalence of the positive outcome.\n\n\n\n\n\n\nFairness (Part 1)\n\n\n\nIs this fair? What is your gut telling you? Yes, no, possibly? What information would you need in order to make a judgment? What is the principle on which your judgment rests?",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html#the-propublica-findings",
    "href": "chapters/10-compas.html#the-propublica-findings",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "The ProPublica Findings",
    "text": "The ProPublica Findings\nLet’s now ask a few questions about the the predictive accuracy of this algorithm. First, how accurate it is it overall?\n\ncompas[\"correct_prediction\"] = (compas[\"predicted_high_risk\"] == compas[\"two_year_recid\"])\ncompas[\"correct_prediction\"].mean()\n\n0.6508943089430894\n\n\nRecall that the base rate in this problem is 53%, so our accuracy is somewhat better than random guessing.\nWhat about the accuracy on Black and white defendants separately?\n\ncompas.groupby([\"race\"])[\"correct_prediction\"].mean()\n\nrace\nAfrican-American    0.638\nCaucasian           0.670\nName: correct_prediction, dtype: float64\n\n\nThe overall accuracies for Black and white defendants are comparable, and both are somewhat higher than the base rate of 53%.\nWhat about the error rates? Here is a simple calculation which computes the false positive rate (FPR) in the first row and the true positive rate (TPR) on the bottom row:\n\ncompas.groupby([\"two_year_recid\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\n\n\n0\n0\n0.352\n\n\n1\n1\n0.654\n\n\n\n\n\n\n\n\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race:\n\ncompas.groupby([\"two_year_recid\", \"race\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\n\ntwo_year_recid\nrace\npredicted_high_risk\n\n\n\n\n0\n0\nAfrican-American\n0.448\n\n\n1\n0\nCaucasian\n0.235\n\n\n2\n1\nAfrican-American\n0.720\n\n\n3\n1\nCaucasian\n0.523\n\n\n\n\n\n\n\n\nThe false positive rate for Black defendants is much higher than the false positive rate for white defendants. This was the main finding of Angwin et al. (2022). The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to the idea of representational bias, in which algorithms learn and reproduce toxic stereotypes about certain groups of people.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. This is sometimes called allocative or distributional bias: bias in how resources or opportunities (in this case, freedom) are allocated or distributed between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well.\nWe can think about the argument of Angwin et al. (2022) as a two-step argument:\n\n\nThe COMPAS algorithm has disparate error rates by race.\nTherefore, the COMPAS algorithm is unjustly biased with respect to race.\n\n\nThis argument implicitly equates equality of error rates with lack of bias.\n\n\n\n\n\n\nFairness (Part 2)\n\n\n\n\nSuppose that we developed an alternative algorithm in which the false positive rates were equal, but there were still more positive predictions for Black defendants overall. Would that be enough to ensure fairness?\nSuppose that we developed an alternative prediction algorithm in which the rate of positive prediction was the same across racial groups, but the false positive rates were different. Would that be to ensure fairness?",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html#the-rebuttal",
    "href": "chapters/10-compas.html#the-rebuttal",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "The Rebuttal",
    "text": "The Rebuttal\nAngwin et al. (2022) kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. In particular, Northpointe, the company that developed COMPAS, issued a report Flores, Bechtel, and Lowenkamp (2016) in which they argued that their algorithm was fair. Their argument is based on an idea of fairness which is sometimes called sufficiency Corbett-Davies et al. (2017).\nHere’s the intuition expressed by sufficiency. Imagine that you and your friend both received an A- in Data Structures. Suppose, however, that the instructor says different things to each of you:\n\nTo you, the instructor says: “You did fine in this class, but I don’t think that you are prepared to take Computer Architecture. I gave you a higher grade than I would normally because you wear cool hats in class.”\nTo your friend, the instructor says: “*You did fine in this class and I think you are prepared to take Computer Architecture. Some students got a bump in their grade because they are cool-hat-wearers, but you didn’t get that benefit.”\n\nFeels unfair, right? The instructor is saying that:\n\nWhat a grade means for you in terms of your future success depends on your identity group.\n\n\n\n\n\n\n\nSuppose that you heard this, but instead of cool hats it was because you are a member of an identity group that “needs some help” in order to achieve equitable representation in the CS major. How would you feel? Would that feel fair to you?\n\n\n\nWe’ll formally define sufficiency in a future lecture. For now, let’s use an informal definition:\n\nSufficiency means that a positive prediction means the same thing for future outcomes for each racial group.\n\nTo operationalize this idea, we are looking for the rate of re-arrest to be the same between (a) Black defendants who received a positive prediction and (b) white defendants who received a positive prediction.\nLet’s check this:\n\ncompas.groupby([\"predicted_high_risk\", \"race\"])[\"two_year_recid\"].mean().reset_index()\n\n\n\n\n\n\n\n\n\npredicted_high_risk\nrace\ntwo_year_recid\n\n\n\n\n0\nFalse\nAfrican-American\n0.350\n\n\n1\nFalse\nCaucasian\n0.288\n\n\n2\nTrue\nAfrican-American\n0.630\n\n\n3\nTrue\nCaucasian\n0.591\n\n\n\n\n\n\n\n\nThe rates of rearrest are relatively similar between groups when controlling for the predictions they collectively received. Formal statistical hypothesis tests are typically used to determine whether this difference is sufficiently “real” to warrant correction. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be relatively close to satisfying sufficiency.\nIndeed, in a rejoinder article published by affiliates of the company Northpointe which produced COMPAS, the fact that COMPAS satisfies sufficiency is one of the primary arguments (Flores, Bechtel, and Lowenkamp 2016).",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html#recap",
    "href": "chapters/10-compas.html#recap",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Recap",
    "text": "Recap\nIn these notes, we replicated the data analysis of Angwin et al. (2022), finding that the COMPAS algorithm has disparate error rates between Black and white defendants. We introduced the idea that fairness actually has several different facets in our moral intuitions, and found that the COMPAS algorithm satisfies one of them (sufficiency: equal scores mean the same thing regardless of your group membership) but not the others (equal prediction rates and equal error rates).",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html#some-questions-moving-forward",
    "href": "chapters/10-compas.html#some-questions-moving-forward",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Some Questions Moving Forward",
    "text": "Some Questions Moving Forward\n\nCan we have it all? Could we modify the COMPAS algorithm in such a way that it satisfies all the ideas of fairness that we discussed above? Could we then call it “fair” or “unbiased?”\nAre there other ways to define fairness? Which ones are most compelling to us? Does the right idea of fairness depend on the context in which we apply it?\nHow did this happen? The COMPAS algorithm was never trained on race data about the defendant. How did it happen that this algorithm nevertheless made recommendations at different rates across groups?\nIs automated decision-making legitimate in this setting? Can it be legitimate (just, fair) to use an automated decision-system for making recommendations about parole and sentencing decisions at all? What safeguards and forms of recourse are necessary for the legitimate use of automated decision-making in criminal justice?\nWhat are the systemic impacts? Disparate sentencing decisions can have downstream impacts on communities and institutions. How could application of the COMPAS algorithm exacerbate systemic inequalities?",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/10-compas.html#references",
    "href": "chapters/10-compas.html#references",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "References",
    "text": "References\n\n\n\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2022. “Machine Bias.” In Ethics of Data and Analytics, 254–64. Auerbach Publications.\n\n\nBonilla-Silva, Eduardo. 2018. Racism Without Racists: Color-Blind Racism and the Persistence of Racial Inequality in America. Fifth edition. Lanham: Rowman & Littlefield.\n\n\nCorbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. “Algorithmic Decision Making and the Cost of Fairness.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 797–806. Halifax NS Canada: ACM. https://doi.org/10.1145/3097983.3098095.\n\n\nFlores, Anthony W, Kristin Bechtel, and Christopher T Lowenkamp. 2016. “False Positives, False Negatives, and False Analyses: A Rejoinder to Machine Bias: There’s Software Used Across the Country to Predict Future Criminals. And It’s Biased Against Blacks.” Federal Probation 80: 38.\n\n\nFogliato, Riccardo, Alice Xiang, Zachary Lipton, Daniel Nagin, and Alexandra Chouldechova. 2021. “On the Validity of Arrest as a Proxy for Offense: Race and the Likelihood of Arrest for Violent Crimes.” In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 100–111. Virtual Event USA: ACM. https://doi.org/10.1145/3461702.3462538.\n\n\nYusef, Kideste Wilder, and Tseleq Yusef. 2017. “Criminalizing Race, Racializing Crime: Assessing the Discipline of Criminology Through a Historical Lens.” In The Handbook of the History and Philosophy of Criminology, edited by Ruth Ann Triplett, 1st ed., 272–88. Wiley. https://doi.org/10.1002/9781119011385.ch16.",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Algorithmic Disparity: COMPAS</span>"
    ]
  },
  {
    "objectID": "chapters/12-statistical-fairness.html",
    "href": "chapters/12-statistical-fairness.html",
    "title": "6  Statistical Definitions of Fairness in Decision-Making",
    "section": "",
    "text": "Three Statistical Definitions of Fairness\nLast time, we introduced the idea that fairness in decision-making could be defined formally, and models could be audited to determine the extent to which those models conformed to a given definition. In this section, we’ll discuss some of the definitions in Chapter 3 of Barocas, Hardt, and Narayanan (2023) and implement Python functions to measure the extent to which the COMPAS risk score conforms to those definitions.\nTo line ourselves up with the notation of Barocas, Hardt, and Narayanan (2023), let’s define the following random variables: Let \\(A\\) be a random variable that describes the group membership of an individual. Let \\(Y\\) be the outcome we want to predict. Let \\(R\\) be the value of our risk score. Let \\(\\hat{Y}\\) be our model’s prediction about whether \\(Y\\) occurs.\nIn the case of COMPAS:",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Definitions of Fairness in Decision-Making</span>"
    ]
  },
  {
    "objectID": "chapters/12-statistical-fairness.html#three-statistical-definitions-of-fairness",
    "href": "chapters/12-statistical-fairness.html#three-statistical-definitions-of-fairness",
    "title": "6  Statistical Definitions of Fairness in Decision-Making",
    "section": "",
    "text": "\\(A\\) is the race of the individual, with possible values \\(A = a\\) and \\(A = b\\).\n\\(Y = 1\\) if the individual was arrested within two years after release, and \\(Y = 0\\) if not.\n\\(R\\) is the decile risk score.\n\\(\\hat{Y} = 1\\) if \\(R \\geq 4\\) and \\(\\hat{Y} = 0\\) otherwise.\n\n\nStatistical Independence\nHere’s our first concept of fairness: independence. For our present purposes, we focus on the definition of independence for binary classifiers as given by Barocas, Hardt, and Narayanan (2023).\n\n\n\n\n\n\n\nDefinition 6.1 (Statistical Independence For Binary Classifiers)  The model predictions \\(\\hat{Y}\\) satisfy statistical independence if \\(\\mathbb{P}(\\hat{Y} = 1 | A = a) = {P}(\\hat{Y} = 1 | A = b)\\).\n\n\n\n\nRecall that \\(\\mathbb{P}(Y = 1|A = a)\\) is the probability that \\(Y = 1\\) given that \\(A=a\\). It can be computed using the formula \\(\\mathbb{P}(Y = 1|A = a) = \\frac{\\mathbb{P}(Y = 1, A = a)}{\\mathbb{P}(A = a)}\\).Colloquially, Definition 6.1 says that the probability of a positive prediction \\(\\hat{Y} = 1\\) does not depend on the group membership \\(A\\). In the COMPAS data, independence would require that the probability of the model predicting that an individual will be arrested within two years be the same for Black and white defendants.\nLet’s write a Python function to empirically check independence that will accept a data frame df and three additional arguments:\nFor independence, we don’t actually need the target column, but this approach will let us keep a consistent API for our more complicated implementations below.\n\ngroup_col, the name of the column describing group memberships.\ntarget, the name of the column holding the binary outcomes.\npred, the name of the column holding the predicted binary outcomes.\n\n\ndef test_independence(df, group_col, target, pred):\n    return df.groupby(group_col)[pred].aggregate([np.mean, len])\n\nLet’s run our function to check for independence:\n\ntest_independence(compas, \"race\", \"two_year_recid\", \"predicted_high_risk\")\n\n\n\n\n\n\n\n\n\nmean\nlen\n\n\nrace\n\n\n\n\n\n\nAfrican-American\n0.588\n3696\n\n\nCaucasian\n0.348\n2454\n\n\n\n\n\n\n\n\nThe mean column gives the proportion of the time in which the predictor \\(\\hat{Y}\\) had value equal to 1, for each of the two groups. This is an empirical estimate of the probability \\(\\mathbb{P}(\\hat{Y} = 1 | A = a)\\). We can see that the two proportions are substantially different between groups, strongly suggesting that this model violates the independence criterion. Formally, statistical tests beyond the scope of this course would be needed to reject the hypothesis that the two proportions are different. In this case, you can take my word for it that the relevant test provides strong support for rejecting the null.\nAs discussed in Barocas, Hardt, and Narayanan (2023), independence is a very strong expression of the idea that predictions, and therefore automated decisions, should be the same in aggregate across all groups present in the data. This idea sometimes accompanies another idea, that all groups are equally worthy, meritorious, or deserving of a given decision outcome.\n\n\nError-Rate Balance\n The primary finding of Angwin et al. (2022) was, famously, that the COMPAS algorithm makes very different kinds of errors on Black and white defendants.This definition can be generalized from binary classifiers to score functions via the concept of separation, which is discussed in Barocas, Hardt, and Narayanan (2023).\n\n\n\n\n\n\n\nDefinition 6.2 (Error Rate Balance for Binary Classifiers) The model predictions \\(\\hat{Y}\\) satisfy error-rate balance if the following conditions both hold:\n\\[\n\\begin{aligned}\n    \\mathbb{P}(\\hat{Y} = 1 | Y = 1, A = a) &= \\mathbb{P}(\\hat{Y} =1  | Y = 1, A = b) & \\text{(balanced true positives)} \\\\\n    \\mathbb{P}(\\hat{Y} = 1 | Y = 0, A = a) &= \\mathbb{P}(\\hat{Y} =1  | Y = 0, A = b)\\;. & \\text{(balanced false positives)}\n\\end{aligned}\n\\]\n\n\n\n\nError rate balance requires that the true positive rate and false positive rates be equal on the two groups. Given some data in which we have \\(\\mathrm{TP}\\) instances of true positives, \\(\\mathrm{FP}\\) instances of false positives, \\(\\mathrm{TN}\\) instances of true negatives, and \\(\\mathrm{FN}\\) instances of false negatives, we can estimate the TPR and FPR via the formulas\n\\[\n\\begin{aligned}\n    \\mathrm{TPR} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} \\\\  \n    \\mathrm{FPR} &= \\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}}\\;.\n\\end{aligned}\n\\]\nLet’s write another function with the same API to give a summary of error rates between two groups using these formulas. As we know, it’s pretty convenient to do this with confusion matrices. It’s not much more difficult to do it “by hand” using vectorized Pandas computations:\n\ndef test_error_rate_balance(df, group_col, target, pred):\n    return df.groupby([group_col, target])[pred].mean().reset_index()\n\nWe can use this function to do an empirical test for error rate balance:\n\ntest_error_rate_balance(compas, \"race\", \"two_year_recid\", \"predicted_high_risk\")\n\n\n\n\n\n\n\n\n\nrace\ntwo_year_recid\npredicted_high_risk\n\n\n\n\n0\nAfrican-American\n0\n0.448\n\n\n1\nAfrican-American\n1\n0.720\n\n\n2\nCaucasian\n0\n0.235\n\n\n3\nCaucasian\n1\n0.523\n\n\n\n\n\n\n\n\n The false positive rates are in the rows in which two_year_recid == 0, and the true positive rates are in the rows in which two_year_recid == 1.As before, before concluding that the COMPAS algorithm violates error rate balance as in Definition 6.2, it is technically necessary to perform a statistical test to reject the null hypothesis that the true population error rates are the same.\n\n\nSufficiency\nFinally, as we mentioned last time, the analysis of Angwin et al. (2022) received heavy pushback from Flores, Bechtel, and Lowenkamp (2016) and others, who argued that error rate balance wasn’t really the right thing to measure. Instead, we should check sufficiency, which we’ll define here for binary classifiers:\n\n\n\n\n\n\n\nDefinition 6.3 (Sufficiency) Model predictions \\(\\hat{Y}\\) satisfy sufficiency if the following two conditions hold: \\[\n\\begin{aligned}\n    \\mathbb{P}(Y = 1 | \\hat{Y} = 1, A = a) &= \\mathbb{P}(Y = 1 | \\hat{Y} = 1, A = b) \\\\\n    \\mathbb{P}(Y = 0 | \\hat{Y} = 0, A = a) &= \\mathbb{P}(Y = 0 | \\hat{Y} = 0, A = b) \\;.\n\\end{aligned}\n\\]\n\n\n\n\nThe quantity \\(\\mathbb{P}(Y = 1 | \\hat{Y} = 1, A = a)\\) is sometimes called the positive predictive value (PPV) of \\(\\hat{Y}\\) for group \\(a\\). You can think of it as the “value” of a positive prediction: given that the prediction is positive (\\(\\hat{Y} = 1\\)) for a member of group \\(a\\), how likely is it that the prediction is accurate? Similarly, \\(\\mathbb{P}(Y = 0 | \\hat{Y} = 0, A = a)\\) is sometimes called the negative predictive value (NPV) of \\(\\hat{Y}\\) for group \\(a\\). So, the sufficiency criterion demands that the positive and negative predictive values be equal across groups.\nGiven some data in which we have \\(\\mathrm{TP}\\) instances of true positives, \\(\\mathrm{FP}\\) instances of false positives, \\(\\mathrm{TN}\\) instances of true negatives, and \\(\\mathrm{FN}\\) instances of false negatives, we can estimate the PPV and NPV via the formulas\n\\[\n\\begin{aligned}\n    \\mathrm{PPV} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}} \\\\  \n    \\mathrm{NPV} &= \\frac{\\mathrm{TN}}{\\mathrm{TN} + \\mathrm{FN}}\\;.\n\\end{aligned}\n\\]\nLet’s write a function to check for sufficiency in the COMPAS predictions. This function will compute the positive and negative predictive values by group:\n\ndef test_sufficiency(df, group_col, target, pred):\n    df_ = df.copy()\n    df_[\"correct\"] = df_[pred] == df_[target]\n    return df_.groupby([pred, group_col])[\"correct\"].mean().reset_index()\n\n\ntest_sufficiency(compas, \"race\", \"two_year_recid\", \"predicted_high_risk\")\n\n\n\n\n\n\n\n\n\npredicted_high_risk\nrace\ncorrect\n\n\n\n\n0\n0\nAfrican-American\n0.650\n\n\n1\n0\nCaucasian\n0.712\n\n\n2\n1\nAfrican-American\n0.630\n\n\n3\n1\nCaucasian\n0.591\n\n\n\n\n\n\n\n\nThe negative predictive values are in the rows in which predicted_high_risk == 0 and the positive predictive values are in the rows in which predicted_high_risk == 1. We observe that the negative predictive value is slightly higher for white defendants, while the positive predictive value is slightly higher for Black defendants. These differences, however, are much lower than the error rate disparity noted above.",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Definitions of Fairness in Decision-Making</span>"
    ]
  },
  {
    "objectID": "chapters/12-statistical-fairness.html#can-we-have-it-all",
    "href": "chapters/12-statistical-fairness.html#can-we-have-it-all",
    "title": "6  Statistical Definitions of Fairness in Decision-Making",
    "section": "Can We Have It All?",
    "text": "Can We Have It All?\nOk, well COMPAS isn’t an ideal algorithm by any means. But couldn’t we just define some more conceptions of fairness, pick the ones that we wanted to use, and then design an algorithm that satisfied all of them?\nSadly, no: we can’t even have error rate balance and sufficiency simultaneously.\n\n\n\n\n\n\n\nTheorem 6.1 (Incompatibility of Error Rate Balance and Sufficiency (Chouldechova 2017)) If the true rates \\(p_a\\) and \\(p_b\\) of positive outcomes in the groups \\(a\\) and \\(b\\) are not equal (\\(p_a \\neq p_b\\)), then there does not exist a model that produces predictions which satisfy both error rate balance and sufficiency.\n\n\n\n\n\nProof. Our big-picture approach is proof by contrapositive. We’ll show that if there were a model that satisfied error rate balance and sufficiency, then \\(p_a = p_b\\).\nLet’s briefly forget about group labels – we’ll reintroduce them in a moment.\nFirst, the prevalence of positive outcomes is the fraction of positive outcomes. There are \\(\\mathrm{TP} + \\mathrm{FN}\\) total positive outcomes, and \\(\\mathrm{TP} + \\mathrm{FP} + \\mathrm{TN} + \\mathrm{FN}\\) outcomes overal, so we can write the prevalence as\n\\[\n\\begin{aligned}\n    p = \\frac{\\mathrm{TP} + \\mathrm{FN}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{TN} + \\mathrm{FN}};.\n\\end{aligned}\n\\]\nFrom above, the true and false positive rates are:\n\\[\n\\begin{aligned}\n    \\mathrm{TPR} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} \\\\\n    \\mathrm{FPR} &= \\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}}\\;.  \n\\end{aligned}\n\\]\nThe PPV is: \\[\n\\begin{aligned}\n    \\mathrm{PPV} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}\\;.\n\\end{aligned}\n\\]\nOk, now it’s time to do some algebra. Let’s start with the \\(\\mathrm{TPR}\\) and see if we can find an equation that relates it to the \\(\\mathrm{FPR}\\). First, we’ll multiply by \\(\\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}}\\). If we do this and insert the definitions of these quantities, we’ll get\n\\[\n\\begin{aligned}\n    \\mathrm{TPR} \\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} \\frac{\\mathrm{FP}}{\\mathrm{TP} + \\mathrm{FP}} \\frac{\\mathrm{TP} + \\mathrm{FP}}{\\mathrm{TP}} \\\\\n    &= \\frac{\\mathrm{FP}}{\\mathrm{TP} + \\mathrm{FN}}\\;.\n\\end{aligned}\n\\]\nLet’s now also multiply by a factor of \\(\\frac{p}{1-p}\\):\n\\[\n\\begin{aligned}\n    \\mathrm{TPR} \\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}}\\frac{p}{1-p} &= \\frac{\\mathrm{FP}}{\\mathrm{TP} + \\mathrm{FN}}\\frac{p}{1-p} \\\\\n    &= \\frac{\\mathrm{FP}}{\\mathrm{TP} + \\mathrm{FN}} \\frac{\\mathrm{TP} + \\mathrm{FN}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{TN} + \\mathrm{FN}} \\frac{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{TN} + \\mathrm{FN}}{\\mathrm{FP} + \\mathrm{TN}} \\\\\n    &= \\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}} \\\\\n    &= \\mathrm{FPR}\\;.\n\\end{aligned}\n\\]\nSo, with some algebra, we have proven an equation for any classifier:\n\\[\n\\begin{aligned}\n    \\mathrm{TPR} \\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}}\\frac{p}{1-p} = \\mathrm{FPR}\\;.\n\\end{aligned}\n\\]\nIt’s convenient to rearrange this equation slightly:\n\\[\n\\begin{aligned}\n    \\frac{\\mathrm{TPR}}{\\mathrm{FPR}} \\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}} = \\frac{1-p}{p}\\;.\n\\end{aligned}\n\\]\nor\n\\[\n\\begin{aligned}\n    p = \\left(1 + \\frac{\\mathrm{TPR}}{\\mathrm{FPR}} \\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}}\\right)^{-1}\\;.\n\\end{aligned}\n\\tag{6.1}\\]\nNow suppose that I want to enforce error rate balance and sufficiency for two groups \\(a\\) and \\(b\\), where \\(p_a \\neq p_b\\). So, from error rate balance I am going to require that \\(\\mathrm{TPR}_a = \\mathrm{TPR}_b\\), \\(\\mathrm{FPR}_a = \\mathrm{FPR}_b\\), and from sufficiency I am going to enforce that \\(\\mathrm{PPV}_a = \\mathrm{PPV}_b\\). Now, however, we have a problem: by Equation 6.1, it must also be the case that \\(p_a = p_b\\). This contradicts our assumption from the theorem. We cannot mathematically satisfy both error rate balance and sufficiency. This completes the proof.\n\n\n\n\n\n\n\nDiscussion\n\n\n\nDo you feel that it is more important for a recidivism prediction algorithm like COMPAS to satisfy error rate balance or sufficiency? Why?",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Definitions of Fairness in Decision-Making</span>"
    ]
  },
  {
    "objectID": "chapters/12-statistical-fairness.html#fairness-context-and-legitimacy",
    "href": "chapters/12-statistical-fairness.html#fairness-context-and-legitimacy",
    "title": "6  Statistical Definitions of Fairness in Decision-Making",
    "section": "Fairness, Context, and Legitimacy",
    "text": "Fairness, Context, and Legitimacy\nThe proof above shows that, when the prevalences of positive outcomes differ between groups, we have no hope of being able to have both balanced error rates and sufficiency. In Chapter 3, Barocas, Hardt, and Narayanan (2023) give a few other examples of fairness definitions, as well as proofs that some of these definitions are incompatible with each other. We can’t just have it all – we have to choose.\nThe quantitative story of fairness in automated decision-making is not cut-and-dried – we need to make choices, which may be subject to politics. Let’s close this discussion with three increasingly difficult questions:\n\nWhat is the right definition of fairness by which to judge the operation of a decision-making algorithm?\nIs “fairness” even the right rubric for assessing the impact of a given algorithm?\nIs it legitimate to use automated decision-making at all for a given application context?\n\nWe’ll consider each of these questions soon.",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Definitions of Fairness in Decision-Making</span>"
    ]
  },
  {
    "objectID": "chapters/12-statistical-fairness.html#references",
    "href": "chapters/12-statistical-fairness.html#references",
    "title": "6  Statistical Definitions of Fairness in Decision-Making",
    "section": "References",
    "text": "References\n\n\n\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2022. “Machine Bias.” In Ethics of Data and Analytics, 254–64. Auerbach Publications.\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press. https://fairmlbook.org/pdf/fairmlbook.pdf.\n\n\nChouldechova, Alexandra. 2017. “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.” Big Data 5 (2): 153–63. https://doi.org/10.1089/big.2016.0047.\n\n\nFlores, Anthony W, Kristin Bechtel, and Christopher T Lowenkamp. 2016. “False Positives, False Negatives, and False Analyses: A Rejoinder to Machine Bias: There’s Software Used Across the Country to Predict Future Criminals. And It’s Biased Against Blacks.” Federal Probation 80: 38.",
    "crumbs": [
      "Discrimination, Disparity, Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Definitions of Fairness in Decision-Making</span>"
    ]
  },
  {
    "objectID": "chapters/20-perceptron.html",
    "href": "chapters/20-perceptron.html",
    "title": "7  Introduction to Classification: The Perceptron",
    "section": "",
    "text": "Our Data\nThe perceptron algorithm aims to find a rule for separating two distinct groups in some data. Here’s an example of some data on which we might aim to apply the perceptron:\nCode\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nFigure 7.1: 300 data points in the 2d plane, each of which has one of two labels.\nThere are \\(n = 300\\) points of data. Each data point \\(i\\) has three pieces of information associated with it:\nMore generally, supervised prediction problems with \\(n\\) data points and \\(k\\) features can be summarized in terms of a feature matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) and a target vector \\(\\mathbf{y} \\in \\mathbb{R}^n\\).",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Classification: The Perceptron</span>"
    ]
  },
  {
    "objectID": "chapters/20-perceptron.html#illustration",
    "href": "chapters/20-perceptron.html#illustration",
    "title": "7  Introduction to Classification: The Perceptron",
    "section": "Illustration",
    "text": "Illustration\nThe following figure illustrates the perceptron algorithm in action over several iterations.\n\n\nCode\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    i, local_loss = opt.step(X, y)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nFigure 7.3: Several iterations of the perceptron algorithm. In each panel, the dashed line is the hyperplane corresponding to the previous weight vector \\(\\mathbf{w}^{(t)}\\), while the solid line is the hyperplane for the updated weight vector \\(\\mathbf{w}^{t+1}\\). The empty circle is the point \\(i\\) used in the update; only iterations in which \\(i\\) was a mistake are shown. The loss is computed as in Equation 7.4.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Classification: The Perceptron</span>"
    ]
  },
  {
    "objectID": "chapters/20-perceptron.html#theory-of-the-perceptron-algorithm",
    "href": "chapters/20-perceptron.html#theory-of-the-perceptron-algorithm",
    "title": "7  Introduction to Classification: The Perceptron",
    "section": "Theory of the Perceptron Algorithm",
    "text": "Theory of the Perceptron Algorithm\nDoes the perceptron algorithm always improve our accuracy? Is it guaranteed to terminate? If it does terminate, is the result guaranteed to be a weight vector \\(\\mathbf{w}\\) that perfectly separates the two data classes?\n\nLocal Improvement on Data Point \\(i\\)\nLet’s first check that the perceptron update in Equation 7.6 actually improves the prediction on data point \\(i\\) if there is currently a mistake on that point (i.e. if \\(s^{(t)}_i y_i &lt; 0\\)). We can do this by computing the new \\(s_i^{(t+1)}\\). Remember, what we want is for the sign of \\(s_i^{(t+1)}\\) to match \\(y_i\\).\n\\[\n\\begin{align}\ns_i^{(t+1)} &= \\langle \\mathbf{w}^{(t+1)}, \\mathbf{x}_i\\rangle  &\\text{(definition of $s_i^{(t+1)}$)}\\\\\n               &= \\langle \\mathbf{w}^{(t)} + y_i\\mathbf{x}_i, \\mathbf{x}_i\\rangle &\\text{(perceptron update)} \\\\\n               &= \\langle \\mathbf{w}^{(t)},\\mathbf{x}_i\\rangle + y_i\\langle \\mathbf{x}_i, \\mathbf{x}_i\\rangle &\\text{(linearity of $\\langle \\cdot\\rangle$)}\\\\\n               &= s_i^{(t)} + y_i \\lVert \\mathbf{x}_i\\rVert_2^2\\;. &\\text{(definition of $s_i^{(t)}$ and $\\lVert \\mathbf{x}_i\\rVert$)}\n\\end{align}\n\\]\n Since \\(\\lVert\\mathbf{x}_i\\rVert &gt; 0\\), we conclude that \\(s_i\\) always moves in the right direction: if \\(y_i = 1\\) then \\(s_i^{(t+1)} &gt; s_i^{(t)}\\), while if \\(y_i = -1\\) then \\(s_i^{(t+1)} &lt; s_i^{(t)}\\).Again, this is only if \\(s^{(t)}_i y_i &lt; 0\\); otherwise there is no change in \\(s_i^{(t)}\\) in the current iteration.\n\n\nGlobal Properties\n\n\n\n\n\n\n\nDefinition 7.1 (Linear Separability) A data set with feature matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n\\times k}\\) and target vector \\(y\\in \\{-1, 1\\}\\) is linearly separable if there exists a weight vector \\(\\mathbf{w}\\) such that, for all \\(i \\in [n]\\),\n\\[\n\\langle\\mathbf{w}, \\mathbf{x}_i\\rangle &gt; 0 \\Leftrightarrow y = 1\\;.\n\\]\n\n\n\n\nTake a moment to convince yourself of the following:\n\n\n\n\n\n\n\nProposition 7.1 (Nonconvergence of perceptron for nonseparable data) Suppose that \\((\\mathbf{X}, \\mathbf{y})\\) is not linearly separable. Then, the perceptron update does not converge. Furthermore, at no iteration of the algorithm is it the case that \\(A(\\mathbf{w}) = 1\\).\n\n\n\n\nIt’s not as obvious that, if the data is linearly separable, then the perceptron algorithm will converge to a correct answer. Perhaps surprisingly, this is also true:\n\n\n\n\n\n\n\nTheorem 7.1 (Convergence of perceptron for separable data) Suppose that \\((\\mathbf{X}, \\mathbf{y})\\) is linearly separable. Then:\n\nThe perceptron algorithm converges in a finite number of iterations to a vector \\(\\mathbf{w}\\) that separates the data, so that \\(A(\\mathbf{w}) = 1\\).\n\nDuring the running of the perceptron algorithm, the total number of updates made is no more than \\[\\frac{2 + r(\\mathbf{X})^2}{\\gamma(\\mathbf{X}, \\mathbf{y})}\\;,\\]\n\nwhere \\(r(\\mathbf{X}) = \\max_{i \\in [n]} \\lVert \\mathbf{x}_i \\rVert\\) and \\(\\gamma(\\mathbf{X}, \\mathbf{y})\\) is a geometric measure called the margin of how far apart the two label classes are.\n\n\n\n\nFor a proof of Theorem 7.1, see p. 37-44 of Hardt and Recht (2022).",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Classification: The Perceptron</span>"
    ]
  },
  {
    "objectID": "chapters/20-perceptron.html#what-next",
    "href": "chapters/20-perceptron.html#what-next",
    "title": "7  Introduction to Classification: The Perceptron",
    "section": "What Next?",
    "text": "What Next?\nWe have outlined the perceptron algorithm, which is able to find a separating hyperplane between two labeled groups of points when such a hyperplane exists.\nWe should, however, be troubled by the fact that the perceptron algorithm doesn’t converge when the data isn’t linearly separable. Maybe we could design a different algorithm that would allow us to find a parameter vector \\(\\mathbf{w}\\) that makes the empirical risk \\(R(\\mathbf{w}) = 1 - A(\\mathbf{w})\\) as small as possible?\nUnfortunately, we have a grave problem here:\n\n\n\n\n\n\n\nTheorem 7.2 (0-1 Minimization for Linear Classifiers is NP Hard (Kearns, Schapire, and Sellie (1992))) Unless P = NP, there is no polynomial-time algorithm that can find a \\(\\mathbf{w}\\) that solves Equation 7.5 when \\(R(\\mathbf{w}) = 1 - A(\\mathbf{w})\\) is the rate of incorrect classifications.\n\n\n\n\nSo, if our data is not linearly separable, not only will the perceptron algorithm not converge, but no classical algorithm will solve the minimization problem in polynomial time.\nIn order to make progress towards practical algorithms, we will need to slightly change our approach. This will be the subject of our next several chapters.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Classification: The Perceptron</span>"
    ]
  },
  {
    "objectID": "chapters/20-perceptron.html#references",
    "href": "chapters/20-perceptron.html#references",
    "title": "7  Introduction to Classification: The Perceptron",
    "section": "References",
    "text": "References\n\n\n\n\nHardt, Moritz, and Benjamin Recht. 2022. Patterns, Predictions, and Actions. Princeton University Press. https://mlstory.org/pdf/patterns.pdf.\n\n\nKearns, Michael J., Robert E. Schapire, and Linda M. Sellie. 1992. “Toward Efficient Agnostic Learning.” In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, 341–52. Pittsburgh Pennsylvania USA: ACM. https://doi.org/10.1145/130385.130424.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Classification: The Perceptron</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html",
    "href": "chapters/22-convex-erm.html",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "",
    "text": "Modeling Choices\nOnce we have chosen linear models as our tool, we can specify a model for the binary classification task by making two additional choices:\nWhat choices did we make in the context of the perceptron?\n\\[\nL(\\mathbf{w}) = \\frac{1}{n}\\sum_{i = 1}^n \\mathbb{1}[s_i (2y_i-1) &lt; 0]\n\\]\nThe optimizer we used to minimize the loss was the perceptron update, in which we picked a random point \\(i\\) and then performed the update\nHowever, as we saw, this doesn’t actually work that well. There are two problems:\nSo, how could we choose a better loss function that would allow us to create efficient algorithms?",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html#modeling-choices",
    "href": "chapters/22-convex-erm.html#modeling-choices",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "",
    "text": "Loss: How will we measure the success of the model in distinguishing the two classes?\nOptimizer: What algorithm will we use in order to minimize the loss?\n\n\nThe loss function was the misclassification rate. If we let \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i\\rangle\\), then we can write the loss like this: Here, the term \\(2y_i - 1\\) transforms a \\(y_i\\) with values in \\(\\{0,1\\}\\) into one with values in \\(\\{-1,1\\}\\).\n\n\n \\[\n\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\mathbb{1}[s_i (2y_i-1) &lt; 0]y_i \\mathbf{x}_i\n\\]If \\(i\\) is correctly classified (i.e. if \\(s_i(2 y_i - 1) &gt; 0\\)), then the second term zeros out and nothing happens.\n\n\nOur problem with the optimizer was that this update won’t actually converge if the data is not linearly separable. Maybe we could choose a better optimizer that would converge?\nUnfortunately not – as we saw last time, the very problem of minimizing \\(L(\\mathbf{w})\\) is NP-hard. This is a problem with the loss function itself.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html#convex-functions",
    "href": "chapters/22-convex-erm.html#convex-functions",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "Convex Functions",
    "text": "Convex Functions\nLet’s start by visualizing a single term of the perceptron loss function. We’ll view this as a function of the score \\(s\\) and the true target value \\(y\\):\n\\[\n\\ell(s, y) = \\mathbb{1}[s (2y-1) &lt; 0]\\;.\n\\]\nWe’ll call this the 0-1 loss function. Here’s a plot of this function for each of the two possible values of \\(y\\):\n\ndef zero_one_loss(s, y): \n    return 1*(s*(2*y-1)&lt; 0)\n\n# or \n# hinge_loss = lambda s, y: 1*(s*(2*y-1) &lt; 0)\n\n\n\nCode\nfrom matplotlib import pyplot as plt \nplt.style.use('seaborn-v0_8-whitegrid')\n# plt.rcParams[\"figure.figsize\"] = (10, 4)\n\ndef plot_loss(loss_fun, show_line = False):\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        fig, axarr = plt.subplots(1, 2, figsize = (6, 3)) \n        s = torch.linspace(-1, 1, 10001)\n\n        for j in range(2):\n            y = [0, 1][j]\n            axarr[j].set_title(f\"y = {y}\")\n            axarr[j].set(xlabel = r\"$s$\", \n                        ylabel = r\"$\\ell(s, y)$\")\n            \n            ix1 = s &lt; 0\n            axarr[j].plot(s[ix1], loss_fun(s[ix1], y), color = \"black\")\n            ix2 = s &gt; 0\n            axarr[j].plot(s[ix2], loss_fun(s[ix2], y), color = \"black\")\n\n            if show_line: \n                s1 = torch.tensor([-0.7])\n                s2 = torch.tensor([0.9])\n\n                axarr[j].plot([s1, s2], [loss_fun(s1, y), loss_fun(s2, y)], color = \"darkgrey\", linestyle = \"--\")\n\n        plt.tight_layout()\n        return fig, axarr\n\nfig, axarr = plot_loss(loss_fun = zero_one_loss, show_line = False)\n\n\n\n\n\n\n\n\nFigure 8.2: The 0-1 loss function.\n\n\n\n\n\nSurprsingly, the problem with this loss function \\(\\ell\\) is that we can “draw lines under the function.” What this means is that we can pick two points on the graph of the function, connect them with a line, and find that the line lies under the graph of the function in some regions:\n\n\nCode\nfig, axarr = plot_loss(loss_fun = zero_one_loss, show_line = True)\n\n\n\n\n\n\n\n\nFigure 8.3: The 0-1 loss function with a line demonstrating that this function is nonconvex.\n\n\n\n\n\nSurprisingly, this specific geometric property is what’s blocking us from achieving performant searchability for the problem of finding \\(\\mathbf{w}\\).",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html#convex-loss-functions",
    "href": "chapters/22-convex-erm.html#convex-loss-functions",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "Convex Loss Functions",
    "text": "Convex Loss Functions\nIn order to develop convex loss functions, we need to define two concepts:\n\n\n\n\n\n\n\nDefinition 8.1 A set \\(S \\subseteq \\mathbb{R}^n\\) is convex if, for any two points \\(\\mathbf{z}_1, \\mathbf{z}_2 \\in S\\) and for any \\(\\lambda \\in [0,1]\\), the point \\(\\mathbf{z} = \\lambda \\mathbf{z}_1 + (1-\\lambda) \\mathbf{z}_2\\) is also an element of \\(S\\).\n\n\n\n\nWe also need to define convex functions:\n\n\n\n\n\n\n\nDefinition 8.2 (Convex Functions) Let \\(S \\subseteq \\mathbb{R}^n\\) be convex. A function \\(f:S \\rightarrow \\mathbb{R}\\) is convex if, for any \\(\\lambda \\in \\mathbb{R}\\) and any two points \\(\\mathbf{z}_1, \\mathbf{z}_2 \\in S\\), we have\n\\[\nf(\\lambda \\mathbf{z}_1 + (1-\\lambda)\\mathbf{z}_2) \\leq \\lambda f( \\mathbf{z}_1 ) + (1-\\lambda)f(\\mathbf{z}_2)\\;.\n\\]\nThe function \\(f\\) is strictly convex if the inequality is strict: for all \\(\\lambda\\), \\(\\mathbf{z}_1\\), and \\(\\mathbf{z}_2\\),\n\\[\nf(\\lambda \\mathbf{z}_1 + (1-\\lambda)\\mathbf{z}_2) &lt; \\lambda f( \\mathbf{z}_1 ) + (1-\\lambda)f(\\mathbf{z}_2)\\;.\n\\]\n\n\n\n\nRoughly, a convex function is “bowl-shaped,” in the sense that any line connecting two points on its graph must lie above the graph. The most familiar example of a convex function is our good friend the convex parabola:\n\n\nCode\nx = torch.linspace(-1, 1, 10001)\ny = x**2\n\nplt.plot(x, y, color = \"black\")\nlabs = plt.gca().set(xlabel = r\"$x$\", ylabel = r\"$f(x)$\")\n\n\n\n\n\n\n\n\nFigure 8.4: The convex parabola \\(f(x) = x^2\\)\n\n\n\n\n\nNote that any straight line connecting two points on this graph always stays above the graph of the parabola. As we saw above, the 0-1 loss function \\(\\ell(s, y) = \\mathbb{1}[s(2y-1)&lt;0]\\) does not have this property.\nWe can also define convex functions to replace the nonconvex 0-1 loss function from earlier. Here’s an example, which is usually called the hinge loss, which is defined by the formula\n\\[\n\\ell(s, y) = y \\max \\{0, s\\} + (1 - y) \\max\\{0, -s\\}\\;.\n\\]\n\n\ndef hinge_loss(s, y):\n    first_term  =  y    * (torch.max(torch.zeros_like(s),  s))  \n    second_term = (1-y) * (torch.max(torch.zeros_like(s), -s))\n    return first_term + second_term\n\n# or\n# hinge_loss = lambda s, y: y * (torch.max(torch.zeros_like(s), s))  + (1-y) * (torch.max(torch.zeros_like(s), -s))\n\n\n\nCode\nfig, axarr = plot_loss(loss_fun = hinge_loss, show_line = True)\n\n\n\n\n\n\n\n\nFigure 8.5: The hinge loss function.\n\n\n\n\n\nThe hinge loss is not strictly convex and is not even everywhere differentiable! Despite this, the fact that it is convex has made it a modern workhorse of machine learning. The support vector machine (SVM) operates by minimizing the hinge loss. The “Rectified Linear Unit” (ReLU) is a mainstay of modern deep learning–and is just another name for the hinge loss.\nAn even handier loss function for our purposes is the sigmoid binary cross entropy, which is defined by the formula  \\[\n\\begin{aligned}\n\\ell(s, y) &=  -y \\log \\sigma(s) - (1-y)\\log (1-\\sigma(s))\\;,\n\\end{aligned}\n\\]In this formula, \\(\\sigma(s) = \\frac{1}{1 + e^{-s}}\\) is the logistic sigmoid function.\n\ndef sig(s):\n    return 1 / (1 + torch.exp(-s))\n\ndef binary_cross_entropy(s, y):\n    return -(y * sig(s).log() + (1 - y)*(1-sig(s)).log())\n\n# or \n# sig = lambda s: 1 / (1 + torch.exp(-s))\n# binary_cross_entropy = lambda s, y: -(y * sig(s).log() + (1 - y)*(1-sig(s)).log())\n\n\n\nCode\nsig = lambda s: 1 / (1 + torch.exp(-s))\nbinary_cross_entropy = lambda s, y: -(y * sig(s).log() + (1 - y)*(1-sig(s)).log())\nfig, axarr = plot_loss(loss_fun = binary_cross_entropy, show_line = True)\n\n\n\n\n\n\n\n\nFigure 8.6: The binary cross-entropy loss function.\n\n\n\n\n\nThis function is also convex, and has the considerable benefit of being everywhere differentiable.\nWe intentionally formulated our definition of convexity for functions of many variables. Here is a convex function \\(f:\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\).\n\n\nCode\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n\nx = torch.linspace(-1, 1, 1001)[:, None]\ny = torch.linspace(-1, 1, 1001)[None, :]\n\nz = x**2 + y**2\n\nax.plot_surface(x, y, z, cmap=\"inferno_r\",\n                       linewidth=0, antialiased=False)\n\nlabs = ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\n\n\n\n\n\n\nFigure 8.7: A convex quadratic function of two variables.\n\n\n\n\n\nYou could imagine trying to draw a straight line between two points on the graph of this function – the line would always be above the graph. When thinking about convexity in many variables, it is often sufficient to imagine a bowl-shaped function like this one.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html#convex-empirical-risk-minimization",
    "href": "chapters/22-convex-erm.html#convex-empirical-risk-minimization",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "Convex Empirical Risk Minimization",
    "text": "Convex Empirical Risk Minimization\nWe are now ready to define the primary framework in which we will conduct supervised machine learning: convex empirical risk minimization.\n\n\n\n\n\n\n\nDefinition 8.3 (Empirical Risk Minimization) Given a loss function \\(\\ell:\\mathbb{R}\\times \\{0,1\\} \\rightarrow \\mathbb{R}\\), a feature matrix \\(\\mathbb{X} \\in \\mathbb{R}^{n\\times p}\\), a target vector \\(\\mathbf{y}\\), and a parameter vector \\(\\mathbf{w} \\in \\mathbb{R}^p\\), the empirical risk of \\(\\mathbf{w}\\) is\n\\[\n\\begin{aligned}\nL(\\mathbf{w}) = \\frac{1}{n}\\sum_{i = 1}^n \\ell(s_i, y_i), \\quad&\\text{where }s_i = \\langle \\mathbf{w}, \\mathbf{x}_i\\rangle\\;.\n\\end{aligned}\n\\]\nThe empirical risk minimization problem is to find the value of \\(\\mathbf{w}\\) that makes \\(L(\\mathbf{w})\\) smallest:\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{aligned}\n\\hat{\\mathbf{w}} &= \\argmin_{\\mathbf{w}} L(\\mathbf{w})  \\\\\n                 &= \\argmin_{\\mathbf{w}} \\frac{1}{n}\\sum_{i = 1}^n \\ell(s_i, y_i) \\\\\n                 &= \\argmin_{\\mathbf{w}} \\frac{1}{n}\\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle, y_i)\\;.\n\\end{aligned}\n\\tag{8.1}\\]\n\n\n\n\n\n\n\n\n\n\n\nProposition 8.1 (Convex \\(\\ell\\) means convex \\(L\\)) If the per-observation loss function \\(\\ell:\\mathbb{R}\\times \\{0,1\\} \\rightarrow \\mathbb{R}\\) is convex in its first argument, then the empirical risk \\(L(\\mathbf{w})\\) is convex as a function of \\(\\mathbf{w}\\).\n\n\n\n\nThe proof of Proposition 8.1 involves some elementary properties of convex functions:\n\nIf \\(f(\\mathbf{z})\\) is convex as a function of \\(\\mathbf{z}\\), then \\(g(\\mathbf{z}) = f(\\mathbf{A}\\mathbf{z'})\\) is also convex as a function of \\(\\mathbf{z}'\\), provided that all the dimensions work out.\nAny finite sum of convex functions is convex.\n\nSo, we know that if we choose \\(\\ell\\) to be convex in the score function, then the entire empirical risk \\(L\\) will be convex as a function of the weight vector \\(\\mathbf{w}\\).\nWhy do we care?",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html#convex-functions-have-global-minimizers",
    "href": "chapters/22-convex-erm.html#convex-functions-have-global-minimizers",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "Convex Functions Have Global Minimizers",
    "text": "Convex Functions Have Global Minimizers\nWe want to solve the empirical risk minimization problem:\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{aligned}\n\\hat{\\mathbf{w}} &= \\argmin_{\\mathbf{w}} L(\\mathbf{w}).\n\\end{aligned}\n\\]\nWe might ask ourselves a few questions about this problem:\n\nExistence: Does there exist any choice of \\(\\mathbf{w}\\) that achieves a minimizing value for this function?\nUniqueness: Is this choice of \\(\\mathbf{w}\\) unique, or are there multiple candidates?\nSearchability: are there algorithms which are guaranteed to (a) terminate and (b) not get “trapped” at a bad solution?\n\nAnswering these questions precisely requires a bit more math:\n\n\n\n\n\n\n\nDefinition 8.4 (Local and Global Minimizers) A point \\(\\mathbf{z}\\in S\\) is a global minimizer of the function \\(f:S \\rightarrow \\mathbb{R}\\) if \\(f(\\mathbf{z}) \\leq f(\\mathbf{z}')\\) for all \\(\\mathbf{z}' \\in S\\).\nA point \\(\\mathbf{z} \\in S\\) is a local minimizer of \\(f:S \\rightarrow \\mathbb{R}\\) if there exists a neighborhood \\(T \\subseteq S\\) containing \\(\\mathbf{z}\\) such that \\(\\mathbf{z}\\) is a global minimizer of \\(f\\) on \\(T\\).\n\n\n\n\nIt’s ok if you don’t know what it means for a set to be closed – all the convex functions we will care about in this class will either be defined on sets where this theorem holds or will be otherwise defined so that the conclusions apply.\n\n\n\n\n\n\n\nTheorem 8.1 (Properties of Convex Functions) Let \\(f:S \\rightarrow \\mathbb{R}\\) be a convex function. Then:\n\nIf \\(S\\) is closed and bounded, \\(f\\) has a minimizer \\(\\mathbf{z}^*\\) in \\(S\\).\nFurthermore, if \\(\\mathbf{z}^*\\) is a local minimizer of \\(f\\), then it is also a global minimizer.\nIf in addition \\(f\\) is strictly convex, then this minimizer is unique.\n\n\n\n\n\n\nProof. The proof of item 1 needs some tools from real analysis. The short version is:\n\nEvery convex function is continuous.\nIf \\(S\\subseteq \\mathbb{R}^n\\) is closed and bounded, then it is compact.\nContinuous functions achieve minimizers and maximizers on compact sets.\n\nIt’s ok if you didn’t follow this! Fortunately the second part of the proof is one we can do together. Suppose to contradiction that \\(\\mathbf{z}^*\\) is a local minimizer of \\(f\\), but that there is also a point \\(\\mathbf{z}'\\) such that \\(f(\\mathbf{z}') &lt; f(\\mathbf{z}^*)\\). Since \\(\\mathbf{z}^*\\) is a local minimizer, we can find some neighborhood \\(T\\) containing \\(\\mathbf{z}^*\\) such that \\(\\mathbf{z}^*\\) is a minimizer of \\(f\\) on \\(T\\). Let \\(\\lambda\\) be some very small number and consider the point \\(\\mathbf{z} = \\lambda \\mathbf{z}' + (1-\\lambda)\\mathbf{z}^*\\). Specifically, choose \\(\\lambda\\) small enough so that \\(\\mathbf{z} \\in T\\) (since this makes \\(\\mathbf{z}\\) close to \\(\\mathbf{z}^*\\)). We can evaluate\n\\[\n\\begin{align}\nf(\\mathbf{z}) &= f(\\lambda \\mathbf{z}' + (1-\\lambda)\\mathbf{z}^*) &\\quad \\text{(definition of $\\mathbf{z}$)}\\\\\n       &\\leq \\lambda f(\\mathbf{z}') + (1-\\lambda)f(\\mathbf{z}^*)  &\\quad \\text{($f$ is convex)} \\\\\n       &= f(\\mathbf{z}^*) + \\lambda (f(\\mathbf{z}') - f(\\mathbf{z}^*)) &\\quad \\text{(algebra)}\\\\\n       &&lt; f(\\mathbf{z}^*)\\;. &\\quad \\text{(assumption that $f(\\mathbf{z}') &lt; f(\\mathbf{z}^*)$)}\n\\end{align}\n\\]\nBut this is a contradiction, since we constructed \\(\\mathbf{z}\\) to be in the neighborhood \\(T\\) where \\(\\mathbf{z}^*\\) is a local minimizer. We conclude that there is no \\(\\mathbf{z}'\\) such that \\(f(\\mathbf{z}') &lt; f(\\mathbf{z}^*)\\), and therefore that \\(\\mathbf{z}^*\\) is a global minimizer.\nThe proof of the third part follows a very similar argument to the proof of the second part.\n\nThese properties of convex functions have very important implications for our fundamental questions on empirical risk minimization. If we choose a convex per-observation loss function \\(\\ell\\), then our empirical risk \\(L\\) will also be convex, and:\nExistence. The minimizer \\(\\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}}L(\\mathbf{w})\\) will exist.\nUniqueness: The minimizer \\(\\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}}L(\\mathbf{w})\\) will be unique: if we run a minimization algorithm repeatedly, we’ll get the same answer every time.\nSearchability: When \\(L\\) is convex, there are also no local minimizers other than the global minimizer. Algorithmically, this is the most important property of convexity. It means that if I manage to find any local minimizer at all, that point must be the global minimizer.  Performance: Convexity significantly reduces the difficulty of our task: instead of trying to find “the best” solution, it’s sufficient for us to find any local optimum. This means that we can design our algorithms to be “greedy local minimizer hunters.” There are lots of fast algorithms to do this. An especially important class of algorithms are gradient descent methods, which we’ll discuss soon.If you’ve taken an algorithms class, one way of thinking of convexity is that it guarantees that greedy methods work for solving minimization problems.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html#demo-logistic-regression",
    "href": "chapters/22-convex-erm.html#demo-logistic-regression",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "Demo: Logistic Regression",
    "text": "Demo: Logistic Regression\nYou may have heard of logistic regression in a course on statistics or data science. Logistic regression is simply binary classification using the binary cross-entropy loss function which we saw above:\n\\[\n\\begin{aligned}\n\\ell(s, y) &=  -y \\log \\sigma(s) - (1-y)\\log (1-\\sigma(s))\\;,\n\\end{aligned}\n\\]\nAs can be proven with calculus, this function is convex as a function of \\(s\\). The logistic regression problem then becomes the problem of solving:\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{w}} &= \\argmin_\\mathbf{w} \\frac{1}{n} \\sum_{i = 1}^n \\ell(s_i, y_i)  \\\\\n&= \\argmin_\\mathbf{w} \\frac{1}{n} \\sum_{i = 1}^n \\left[-y_i \\log \\sigma(s_i) - (1-y_i)\\log (1-\\sigma(s_i))\\right] \\\\\n&= \\argmin_\\mathbf{w} \\frac{1}{n} \\sum_{i = 1}^n \\left[-y_i \\log \\sigma(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle) - (1-y_i)\\log (1-\\sigma(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle)\\right]\n\\end{aligned}\n\\]\nSo, let’s do convex empirical risk minimization! We’ll use the following data set. Note that this data is not linearly separable and therefore the perceptron algorithm would not converge.\n\n\nCode\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(noise = 0.5)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nFigure 8.8: Data for logistic regression.\n\n\n\n\n\nLet’s go ahead and train a logistic regression model. For the purposes of today, we can do this in a very simple way that doesn’t even involve an explicit training loop. Next time, we’ll learn how to write an explicit training loop.\nFirst, we’ll define a complete function for calculating the empirical risk for a given value of \\(\\mathbf{w}\\). Since we already implemented binary_cross_entropy, this implementation is very quick:\n\ndef empirical_risk(w, X, y):\n    s = X@w\n    return binary_cross_entropy(s, y).mean()\n\nNow we’ll use the minimize function from scipy.optimize to find the value of \\(\\mathbf{w}\\) that makes this function smallest:\n\nfrom scipy.optimize import minimize\n\nw0 = torch.tensor([1.0, 1.0, 1.0])\nresult = minimize(lambda w: empirical_risk(w, X, y), x0 = w0)\nw = result.x\n\nprint(f\"Learned parameter vector w = {w}.\\nThe empirical risk is {result.fun:.4f}.\")\n\nLearned parameter vector w = [ 3.88016032  4.915439   -4.37486882].\nThe empirical risk is 0.1712.\n\n\nHow does it look?\n\n\nCode\ndef draw_line(w, X, y, x_min, x_max, ax, **kwargs):\n    fig, ax = plt.subplots(1, 1)\n    plot_classification_data(X, y, ax)\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    l = ax.plot(x, y, **kwargs)\n\ndraw_line(w, X, y, x_min = -0.5, x_max = 1.5, ax = ax, color = \"black\", linestyle = \"dashed\")\n\n# fig\n\n\n\n\n\n\n\n\nFigure 8.9: The separating line learned by logistic regression.\n\n\n\n\n\nPretty good! Yes, it’s as easy as that – provided that you don’t ask too many questions about how the minimize function works. Questions like that will be the topic of our next several sets of notes.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/22-convex-erm.html#recap",
    "href": "chapters/22-convex-erm.html#recap",
    "title": "8  Convex Empirical Risk Minimization",
    "section": "Recap",
    "text": "Recap\nIn this set of notes, we introduced a fundamental idea: convex empirical risk minimization. To do convex empirical risk minimization, all we need is a convex per-observation loss function. This gives us a convex empirical risk function, which is simply the mean of all the per-observation losses. Once we have that, the problem of classification reduces to the problem of finding a value of the parameter vector \\(\\mathbf{w}\\) that makes the empirical risk small. Convexity guarantees that this problem has exactly one solution. Today, we found this solution using a packaged optimizer. Starting next time, we’ll learn how to write our own optimization algorithms and explore how optimization techniques enable scalable machine learning.\n\n(Optional): A Logistic Regression Training Loop\nHow would we do this if we didn’t have access to the minimize function? We’ll soon discuss this question much more. For now, we can take a look at the code block below, which implements such a loop using a framework very similar to the one we learned for perceptron. This model also inherits from the LinearModel class that you previously started implementing. The training loop is also very similar to our training loop for the perceptron. The main difference is that the loss is calculated using the binary_cross_entropy function above, and the step function of the GradientDescentOptimizer works differently in a way that we will discuss in the following section.\nStarting with the code block below, you won’t be able to follow along in coding these notes unless you have sneakily implemented logistic regression in a hidden module.\n\nfrom hidden.logistic import LogisticRegression, GradientDescentOptimizer\n\n# instantiate a model and an optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\n\nfor _ in range(100):\n\n    # not part of the update: just for tracking our progress    \n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n\n    # only this line actually changes the parameter value\n    opt.step(X, y, lr = 0.02)\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.semilogx()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\nFigure 8.10: Evolution of the binary cross entropy loss function in the logistic regression training loop.\n\n\n\n\n\nThe loss quickly levels out to a constant value (which is the same as we learned with scipy.optimize.minimize). Because our theory tells us that the loss function is convex, we know that the value of \\(\\mathbf{w}\\) we have found is the best possible, in the sense of minimizing the loss.\nLet’s take a look at the separating line we found:\n\ndraw_line(LR.w, X, y, x_min = -0.5, x_max = 1.5, ax = ax, color = \"black\", linestyle = \"dashed\")\n\n\n\n\n\n\n\n\nYep, that’s the same line as we found earlier!\nAlthough our data is not linearly separable, the separating line we have learned appears to do a reasonable job of separating the points from each other. Let’s check our accuracy:\n\n(1.0*(LR.predict(X) == y)).mean()\n\ntensor(0.9133)\n\n\nNot too bad! In the next section, we’ll learn much, much more about what’s behind that opt.step() call.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convex Empirical Risk Minimization</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html",
    "href": "chapters/23-gradient-descent.html",
    "title": "9  Optimization with Gradient Descent",
    "section": "",
    "text": "Linear Approximations of Single-Variable Functions\nRecall the limit definition of a derivative of a single-variable function. Let \\(g:\\mathbb{R} \\rightarrow \\mathbb{R}\\). The derivative of \\(g\\) at point \\(w_0\\), if it exists, is\n\\[\n\\begin{aligned}\n\\frac{dg(w_0)}{dw} = \\lim_{\\delta w\\rightarrow 0}\\frac{g(w_0 + \\delta w) - g(w_0)}{\\delta w}\\;.\n\\end{aligned}\n\\]\nIf we imagine that \\(\\delta w\\) is very small but nonzero, we can interpret this equation a bit loosely as the statement that\n\\[\n\\begin{aligned}\n\\frac{dg(w_0)}{dw} \\approx \\frac{g(w_0 + \\delta w) - g(w_0)}{\\delta w}\\;,\n\\end{aligned}\n\\]\nwhich upon some algebraic rearrangement says that\n\\[\ng(w_0 + \\delta w) \\approx g(w_0) + \\frac{dg(w_0)}{dw} \\delta w\\;.\n\\]\nTaylor’s theorem makes this statement precise:\nHere, \\(o(\\delta w)\\) means “terms that grow small in comparison to \\(\\delta w\\) when \\(\\delta w\\) itself grows small.”\nAnother common way to write Taylor’s theorem is\n\\[\ng(w) = g(w_0) + \\frac{dg(w_0)}{dw} (w - w_0) + o(|w - w_0|)\\;,\n\\]\nwhich comes from substituting \\(\\delta w = t - w_0\\).\nTaylor’s theorem says that, in a neighborhood of \\(w_0\\), we can approximate \\(g(w)\\) with a linear function. Here’s an example of how that looks:\nimport torch \nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\nw = torch.linspace(-1, 1, 101)\n\ng = lambda w: w**2\n\nplt.plot(w, g(w), label = r\"$g(w)$\", color = \"black\")\nplt.gca().set(xlabel = r\"$w$\", ylim = (-0.2, 0.5))\n\ndef taylor(w, w0):\n    return g(w0) + 2*w0*(w-w0)\nplt.plot(w, taylor(w, .2), label = r\"1st-order Taylor approximation\", linestyle = \"--\")\nplt.legend()",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html#linear-approximations-of-single-variable-functions",
    "href": "chapters/23-gradient-descent.html#linear-approximations-of-single-variable-functions",
    "title": "9  Optimization with Gradient Descent",
    "section": "",
    "text": "Theorem 9.1 (Taylor’s Theorem: Univariate Functions) Let \\(g:\\mathbb{R}\\rightarrow \\mathbb{R}\\) be differentiable at point \\(w_0\\). Then, there exists \\(a &gt; 0\\) such that, if \\(\\delta w &lt; a\\), then\n\\[\ng(w_0 + \\delta w) = g(w_0) + \\frac{dg(w_0)}{dw} \\delta w + o(\\delta w)\\;.\n\\]",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html#gradient-descent-in-1-dimension",
    "href": "chapters/23-gradient-descent.html#gradient-descent-in-1-dimension",
    "title": "9  Optimization with Gradient Descent",
    "section": "Gradient Descent in 1 dimension",
    "text": "Gradient Descent in 1 dimension\nSuppose that we have a function \\(g\\) and we would like to solve the the optimization problem\n\\[\n\\begin{aligned}\n\\hat{w} = \\argmin _w g(w) \\;.\n\\end{aligned}\n\\]\nHow do we go about doing this? You might remember from calculus that one way starts with solving the equation\n\\[\n\\begin{aligned}\n\\frac{dg(\\hat{w})}{dw} = 0\\;,\n\\end{aligned}\n\\]\nbut it is not always feasible to solve this equation exactly in practice.\nIn iterative approaches, we instead imagine that we have a current guess \\(\\hat{w}\\) which we would like to improve. To this end, consider the casual Taylor approximation In the rest of these notes, we will assume that term \\(o(\\delta w)\\) is small enough to be negligible.\n\\[\n\\begin{aligned}\ng(\\hat{w} + \\delta \\hat{w}) \\approx g(\\hat{w}) + \\frac{dg(\\hat{w})}{dw} \\delta \\hat{w}\\;.\n\\end{aligned}\n\\]\nWe’d like to update our estimate of \\(\\hat{w}\\). Suppose we make a strategic choice: \\(\\delta hat{w} = -\\alpha \\frac{dg(\\hat{w})}{dw}\\) for some small \\(\\alpha &gt; 0\\). We therefore decide that we will do the update\n\\[\n\\begin{aligned}\n    \\hat{w} \\gets \\hat{w} - \\alpha \\frac{dg(\\hat{w})}{dw}\\;.\n\\end{aligned}\n\\tag{9.2}\\]\nWhat does this update do to the value of \\(g\\)? Let’s check:\n\\[\n\\begin{aligned}\n    g(\\hat{w} + \\delta \\hat{w}) &\\approx g(\\hat{w}) + \\frac{dg(\\hat{w})}{dw} \\delta \\hat{w} \\\\\n    &= g(\\hat{w}) - \\frac{dg(\\hat{w})}{dw} \\alpha \\frac{dg(\\hat{w})}{dw}\\\\\n    &= g(\\hat{w}) - \\alpha\\left(\\frac{dg(\\hat{w})}{dw}\\right)^2\\;.   \n\\end{aligned}\n\\]\nThis is the big punchline. Let’s look at the second term. If \\(\\left(\\frac{dg(\\hat{w})}{dw}\\right)^2 = 0\\) then that must mean that \\(\\frac{dg(\\hat{w})}{dw}\\) and that we are at a critical point, which we could check for being a local minimum. On the other hand, if \\(\\frac{dg(\\hat{w})}{dw} \\neq 0\\), then \\(\\left(\\frac{dg(\\hat{w})}{dw}\\right)^2 &gt; 0\\). This means that\n\\[\n\\begin{aligned}\ng(\\hat{w} + \\delta \\hat{w}) &\\approx  g(\\hat{w}) - \\alpha\\left(\\frac{dg(\\hat{w})}{dw}\\right)^2 \\\\\n                            &&lt;  g(\\hat{w})\\;,\n\\end{aligned}\n\\]\nprovided that \\(\\alpha\\) is small enough for the error terms in Taylor’s Theorem to be small. We have informally derived the following fact:\n\n\n\n\n\n\nSingle-Variable Gradient-Descent Works\n\n\n\nLet \\(g:\\mathbb{R}\\rightarrow \\mathbb{R}\\) be differentiable and assume that \\(\\frac{dg(\\hat{w})}{dw} \\neq 0\\). Then, if \\(\\alpha\\) is sufficiently small, Equation 9.2 is guaranteed to reduce the value of \\(g\\).\n\n\nLet’s see an example of single-variable gradient descent in action:\n\nw_     = -0.7\ngrad  = lambda w: 2*w\nalpha = 0.1\nw_vec = [w_]\n\nfor i in range(100):\n        w_ = w_ - alpha*grad(w_) \n        w_vec.append(w_)\n\nw_vec = torch.tensor(w_vec)\n\nplt.plot(w, g(w), label = r\"$g(w)$\")\nplt.scatter(w_vec, g(w_vec), color = \"black\", label = r\"Gradient descent updates\", s = 10, zorder = 10)\nplt.gca().set(xlabel = r\"$w$\")\nplt.legend()\n\n\n\n\n\n\n\n\nWe can see the updates from gradient descent eventually converging to the point \\(w = 0\\), which is the global minimum of this function.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html#gradient-descent-in-multiple-dimensions",
    "href": "chapters/23-gradient-descent.html#gradient-descent-in-multiple-dimensions",
    "title": "9  Optimization with Gradient Descent",
    "section": "Gradient Descent in Multiple Dimensions",
    "text": "Gradient Descent in Multiple Dimensions\nOur empirical risk function \\(L\\) is not a single-variable function; indeed, \\(L: \\mathbb{R}^p \\rightarrow \\mathbb{R}\\). So, we can’t directly apply the results above. Fortunately, these results extend in a smooth way to this setting. The main thing we need is the definition of the gradient of a multivariate function.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html#gradients",
    "href": "chapters/23-gradient-descent.html#gradients",
    "title": "9  Optimization with Gradient Descent",
    "section": "Gradients",
    "text": "Gradients\nWe’re not going to talk much about what it means for a function to be multivariate differentiable. You can assume that all the functions we will deal with in this class are unless I highlight otherwise. For a more rigorous definition, you should check out a multivariable calculus class.\n\n\n\n\n\n\n\nDefinition 9.1 (Gradient of a Multivariate Function) Let \\(g:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) be a multivariate differentiable function. The gradient of \\(g\\) evaluated at point \\(\\mathbf{w}\\in \\mathbb{R}^p\\) is written \\(\\nabla g(\\mathbf{w})\\), and has value\n\\[\n\\nabla g(\\mathbf{w}) \\triangleq\n\\left(\\begin{matrix}\n    \\frac{\\partial g(\\mathbf{w})}{\\partial w_1} \\\\\n    \\frac{\\partial g(\\mathbf{w})}{\\partial w_2} \\\\\n    \\cdots \\\\\n    \\frac{\\partial g(\\mathbf{w})}{\\partial w_p} \\\\\n\\end{matrix}\\right) \\in \\mathbb{R}^p\\;.\n\\]\nHere, \\(\\frac{\\partial g(\\mathbf{w})}{\\partial w_1}\\) is the partial derivative of \\(f\\) with respect to \\(z_1\\), evaluated at \\(\\mathbf{w}\\). To compute it:\n\nTake the derivative of \\(f\\) *with respect to variable \\(z_1\\), holding all other variables constant, and then evaluate the result at \\(\\mathbf{w}\\).\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nLet \\(p = 3\\). Let \\(g(\\mathbf{w}) = w_2\\sin w_1  + w_1e^{2w_3}\\). The partial derivatives we need are\n\\[\n\\begin{align}\n\\frac{\\partial g(\\mathbf{w})}{\\partial w_1} &= w_2 \\cos w_1 + e^{2w_3}\\\\\n\\frac{\\partial g(\\mathbf{w})}{\\partial w_2} &= \\sin w_1\\\\\n\\frac{\\partial g(\\mathbf{w})}{\\partial w_3} &= 2w_1 e^{2w_3}\\;.\n\\end{align}\n\\]\nSo, the gradient of \\(g\\) evaluated at a point \\(\\mathbf{w}\\) is\n\\[\n\\nabla g(\\mathbf{w}) =\n\\left(\\begin{matrix}\n    \\frac{\\partial g(\\mathbf{w})}{\\partial w_1} \\\\\n    \\frac{\\partial g(\\mathbf{w})}{\\partial w_2} \\\\\n    \\frac{\\partial g(\\mathbf{w})}{\\partial w_3} \\\\\n\\end{matrix}\\right) =\n\\left(\\begin{matrix}\n    w_2 \\cos w_1 + e^{2w_3}\\\\\n    \\sin w_1\\\\\n    2w_1 e^{2w_3}\n\\end{matrix}\\right)\n\\]\n\n\nTaylor’s Theorem extends smoothly to this setting.\n\n\n\n\n\n\n\nTheorem 9.2 (Taylor’s Theorem: Multivariate Functions) Let \\(g:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) be differentiable at point \\(\\mathbf{w}_0 \\in \\mathbb{R}^p\\). Then, there exists \\(a &gt; 0\\) such that, if \\(\\lVert \\delta \\mathbf{w} \\rVert &lt; a\\), then \n\\[\ng(\\mathbf{w}_0 + \\delta \\mathbf{w}) = g(\\mathbf{w}_0) + \\langle \\nabla g(\\mathbf{w}_0), \\delta \\mathbf{w} \\rangle + o(\\lVert \\delta \\mathbf{w}\\rVert)\\;.\n\\]\n\n\n\n\n\\(\\lVert \\mathbf{\\delta} \\mathbf{w}\\rVert \\triangleq \\sqrt{\\sum_{i = 1}^p (\\delta w_i)^2}\\)The vector \\(\\nabla g(\\mathbf{w}_0)\\) plays the role of the single-variable derivative \\(\\frac{d g(w_0)}{dw}\\).\n\nMultivariate Gradient Descent\nIn multiple dimensions, the gradient descent update is:\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{w}} \\gets \\hat{\\mathbf{w}} - \\alpha \\nabla g(\\hat{\\mathbf{w}})\\;.\n\\end{aligned}\n\\tag{9.3}\\]\nLet’s check that a single update of gradient descent will reduce the value of \\(g\\) provided that \\(\\alpha\\) is small enough. Here, \\(\\delta \\hat{\\mathbf{w}} = -\\alpha \\nabla g(\\hat{\\mathbf{w}})\\).\n\\[\n\\begin{aligned}\n    g(\\hat{\\mathbf{w}} - \\delta \\hat{\\mathbf{w}}) &\\approx g(\\hat{\\mathbf{w}}) + \\langle \\nabla g(\\mathbf{w}_0), \\delta \\mathbf{w} \\rangle \\\\\n    &= g(\\hat{\\mathbf{w}}) + \\langle \\nabla g(\\hat{\\mathbf{w}}), -\\alpha \\nabla g(\\hat{\\mathbf{w}}) \\rangle \\\\\n    &= g(\\hat{\\mathbf{w}}) - \\alpha \\langle \\nabla g(\\hat{\\mathbf{w}}),  \\nabla g(\\hat{\\mathbf{w}}) \\rangle \\\\\n    &= g(\\hat{\\mathbf{w}}) - \\alpha \\lVert\\nabla g(\\hat{\\mathbf{w}}) \\rVert^2\\;.\n\\end{aligned}\n\\]\nSince \\(\\lVert\\nabla g(\\hat{\\mathbf{w}}) \\rVert^2 &gt; 0\\) whenever \\(\\nabla g(\\hat{\\mathbf{w}}) \\neq\\mathbf{0}\\), we conclude that, unless \\(\\hat{w}\\) is a critical point (where the gradient is zero), then\n\\[\n\\begin{aligned}\n    g(\\hat{\\mathbf{w}} - \\alpha \\nabla g(\\hat{\\mathbf{w}})) &lt; g(\\hat{\\mathbf{w}})\\;.\n\\end{aligned}\n\\]\nIn other words, provided that \\(\\alpha\\) is small enough for the Taylor approximation to be a good one, multivariate gradient descent also always reduces the value of the objective function.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html#gradient-of-the-empirical-risk",
    "href": "chapters/23-gradient-descent.html#gradient-of-the-empirical-risk",
    "title": "9  Optimization with Gradient Descent",
    "section": "Gradient of the Empirical Risk",
    "text": "Gradient of the Empirical Risk\nRemember that our big objective here was to solve Equation 9.1 using gradient descent. To do this, we need to be able to calculate \\(\\nabla L(\\mathbf{w})\\), where the gradient is with respect to the entries of \\(\\mathbf{w}\\). Fortunately, the specific linear structure of the score function \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) makes this relatively simple: indeed, we actually only need to worry about the single variable derivatives of the per-observation loss \\(\\ell\\). To see this, we can compute\n\\[\n\\begin{align}\n\\nabla L(\\mathbf{w}) &= \\nabla \\left(\\frac{1}{n} \\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle , y_i)\\right) \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n \\nabla \\ell(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle , y_i) \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(s_i, y_i)}{ds} \\nabla \\langle \\mathbf{w}, \\mathbf{x}_i\\rangle  &\\quad \\text{(multivariate chain rule)} \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(s_i, y_i)}{ds}  \\mathbf{x}_i &\\quad \\text{(gradient of a linear function)} \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(s_i, y_i)}{ds} \\mathbf{x}_i &\\quad \\text{($s_i = \\langle \\mathbf{w}, \\mathbf{x}_i\\rangle$)} \\\\\n\\end{align}\n\\]\nThe good news here is that for linear models, we don’t actually need to be able to compute more gradients: we just need to be able to compute derivatives of the form \\(\\frac{d\\ell(s_i, y_i)}{ds}\\) and then plug in \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i\\rangle\\).\nLet’s do an example with the logistic loss:\n\\[\\ell(s, y) = -y \\log \\sigma(s) - (1-y)\\log (1-\\sigma(s))\\;.\\]\nA useful fact to know about the logistic sigmoid function \\(\\sigma\\) is that \\(\\frac{d\\sigma(s) }{ds} = \\sigma(s) (1 - \\sigma(s))\\). So, using that and the chain rule, the derivative we need is\n\\[\n\\begin{align}\n\\frac{d\\ell(s, y)}{ds} &= -y \\frac{1}{\\sigma(s)}\\frac{d\\sigma(s) }{ds} - (1-y)\\frac{1}{1-\\sigma(s)}\\left(- \\frac{d\\sigma(s) }{ds}\\right) \\\\\n&= -y \\frac{1}{\\sigma(s)}\\sigma(s) (1 - \\sigma(s)) - (1-y)\\frac{1}{1-\\sigma(s)}\\left(- \\sigma(s) (1 - \\sigma(s))\\right) \\\\\n&= -y (1 - \\sigma(s)) + (1-y)\\sigma(s) \\\\\n&= \\sigma(s) - y\\;.\n\\end{align}\n\\]\nFinally, we need to plug this back in to our empirical risk, obtaining the gradient of the empirical risk for logistic regression:\n \\[\n\\begin{align}\n\\nabla L(\\mathbf{w}) &= \\frac{1}{n} \\sum_{i = 1}^n (\\sigma(s_i) - y_i)\\mathbf{x}_i \\\\\n              &=\\frac{1}{n} \\sum_{i = 1}^n (\\sigma(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle) - y_i)\\mathbf{x}_i\\;.\n\\end{align}\n\\tag{9.4}\\]An important note about this formula that can easily trip one up: this looks a bit like a matrix multiplication or dot product, but it isn’t!\nThis gives us all the math that we need in order to learn logistic regression by choosing a learning rate and iterating the update \\(\\mathbf{w}^{(t+1)} \\gets \\mathbf{w}^{(t)} - \\alpha \\nabla L(\\mathbf{w}^{(t)})\\) until convergence.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html#example-logistic-regression",
    "href": "chapters/23-gradient-descent.html#example-logistic-regression",
    "title": "9  Optimization with Gradient Descent",
    "section": "Example: Logistic Regression",
    "text": "Example: Logistic Regression\nThis model also inherits from the LinearModel class that you previously started implementing. The training loop is also very similar to our training loop for the perceptron. The main difference is that the loss is calculated using the binary_cross_entropy function above, and the step function of the GradientDescentOptimizer works differently in a way that we will discuss in the following section.\nStarting with the code block below, you won’t be able to follow along in coding these notes unless you have sneakily implemented logistic regression in a hidden module.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\nThe logistic regression training loop relies on a new implementation of opt.step. For gradient descent, here’s the complete implementation: just a quick Python version of the gradient descent update Equation 9.3.\n\ndef step(self, X, y, lr = 0.01):\n    self.model.w -= lr*self.model.grad(X, y)\n\nThe method model.grad() is the challenging part of the implementation: this is where we actually need to turn Equation 9.4 into code.\nHere’s the complete training loop. This loop is very similar to our perceptron training loop – we’re just using a different loss and a different implementation of grad.\n\nfrom hidden.logistic import LogisticRegression, GradientDescentOptimizer\n\n# instantiate a model and an optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\n\nfor _ in range(100):\n\n    # not part of the update: just for tracking our progress    \n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n\n    # only this line actually changes the parameter value\n    # The whole definition is: \n    # self.model.w -= lr*self.model.grad(X, y)\n\n    opt.step(X, y, lr = 0.02)\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.semilogx()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\nFigure 9.1: Evolution of the binary cross entropy loss function in the logistic regression training loop.\n\n\n\n\n\nThe loss quickly levels out to a constant value, which is our optimized weight vector \\(\\mathbf{w}\\). Because our theory tells us that the loss function is convex, we know that the value of \\(\\mathbf{w}\\) we have found is the best possible, in the sense of minimizing the loss.\nLet’s check our training accuracy:\n\n(1.0*(LR.predict(X) == y)).mean()\n\ntensor(0.9167)\n\n\nNot too bad!",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  },
  {
    "objectID": "chapters/23-gradient-descent.html#recap",
    "href": "chapters/23-gradient-descent.html#recap",
    "title": "9  Optimization with Gradient Descent",
    "section": "Recap",
    "text": "Recap\nIn these lecture notes, we introduced gradient descent as a method for minimizing functions, and showed an application of gradient descent for logistic regression. Gradient descent is especially useful when working with convex functions, since in this case it is guaranteed to converge to the global minimum of the empirical risk (provided that the learning rate \\(\\alpha\\) is sufficiently low). The idea of gradient descent – incremental improvement to the weight vector \\(\\mathbf{w}\\) using information about the derivatives of the loss function–is a fundamental one which has led to many variations and improvements.",
    "crumbs": [
      "Machine Learning Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Optimization with Gradient Descent</span>"
    ]
  }
]