[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Welcome\nThis is a set of notes developed for an undergraduate course in machine learning. The target audience for these notes are undergraduates in computer science who have completed first courses in linear algebra and discrete mathematics. These notes draw on many sources, but are somewhat distinctive in the following ways:"
  },
  {
    "objectID": "index.html#pedagogical-features",
    "href": "index.html#pedagogical-features",
    "title": "Machine Learning",
    "section": "Pedagogical Features",
    "text": "Pedagogical Features\nThese notes are explicitly designed for undergraduate instruction in computer science. For this reason:\n\nComputational examples are integrated into the text and shown throughout.\nLive versions of lecture notes are supplied as downloadable Jupyter Notebooks, with certain code components removed. The purpose is to facilitate live-coding in lectures."
  },
  {
    "objectID": "index.html#use-and-reuse",
    "href": "index.html#use-and-reuse",
    "title": "Machine Learning",
    "section": "Use and Reuse",
    "text": "Use and Reuse\nThese notes were written by Phil Chodrow for the course CSCI 0451: Machine Learning at Middlebury College. All are welcome to use them for educational purposes. Attribution is appreciated but not expected."
  },
  {
    "objectID": "index.html#source-texts",
    "href": "index.html#source-texts",
    "title": "Machine Learning",
    "section": "Source Texts",
    "text": "Source Texts\nThese notes draw on several source texts, most of which are available for free online. These are:\n\nHardt and Recht (2022) is the primary influence for the overall arc of the notes.\nA Course in Machine Learning by Hal Daumé III is an accessible introduction to many of the topics and serves as a useful source of supplementary readings.\n\nAdditional useful readings:\n\nAbu-Mostafa, Magdon-Ismail, and Lin (2012): Learning From Data: A Short Course\nBarocas, Hardt, and Narayanan (2023) is an advanced text on questions of fairness in automated decision-making for readers who have some background in probability theory.\nBishop (2006) and Murphy (2022) are advanced texts which are most suitable for advanced readers who have already taken at least one course in probability theory.\n\nDeisenroth, Faisal, and Ong (2020) and Kroese et al. (2020) are useful readings focusing on some of the mathematical fundamentals.\nZhang, Lipton, and Li (2023) tells a helpful story of the fundamentals of deep learning."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Machine Learning",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis site was generated using the Quarto publishing system. It is hosted on GitHub and published via GitHub Pages."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Machine Learning",
    "section": "References",
    "text": "References\n\n\n\n\nAbu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012. Learning from Data: A Short Course. S.l. https://amlbook.com/.\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press. https://fairmlbook.org/pdf/fairmlbook.pdf.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nDeisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020. Mathematics for Machine Learning. Cambridge, UK New York, NY: Cambridge University Press. https://mml-book.github.io/book/mml-book.pdf.\n\n\nHardt, Moritz, and Benjamin Recht. 2022. Patterns, Predictions, and Actions. Princeton University Press. https://mlstory.org/pdf/patterns.pdf.\n\n\nKroese, Dirk P., Zdravko I. Botev, Thomas Taimre, and Radislav Vaisman. 2020. Data Science and Machine Learning: Mathematical and Statistical Methods. Chapman & Hall/CRC Machine Learning & Pattern Recognition Series. Boca Raton London New York: CRC Press, Taylor & Francis Group.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. Adaptive Computation and Machine Learning Series. Cambridge, Massachusetts: The MIT Press. https://probml.github.io/pml-book/book1.html.\n\n\nZhang, Aston, Zachary Lipton, and Mu Li. 2023. Dive into Deep Learning. Cambridge, UK: Cambridge University Press."
  },
  {
    "objectID": "chapters/01-data-and-models.html#supervised-learning",
    "href": "chapters/01-data-and-models.html#supervised-learning",
    "title": "1  Data, Patterns, and Models",
    "section": "Supervised Learning",
    "text": "Supervised Learning\nOur focus in these notes is almost exclusively on supervised learning. In supervised learning, we are able to view some attributes or features of a data point, which we call predictors. Traditionally, we collect these attributes into a vector called \\(\\mathbf{x}\\). Each data point then has a target, which could be either a scalar number or a categorical label. Traditionally, the target is named \\(y\\). We aim to predict the target based on the predictors using a model, which is a function \\(f\\). The result of applying the model \\(f\\) to the predictors \\(\\mathbf{x}\\) is our prediction or predicted target \\(f(\\mathbf{x})\\), to which we often give the name \\(\\hat{y}\\). Our goal is to choose \\(f\\) such that the predicted target \\(\\hat{y}\\) is equal to, or at least close to, the true target \\(y\\). We could summarize this with the heuristic statement:\n\\[\n\\begin{aligned}\n    \\text{``}f(\\mathbf{x}) = \\hat{y} \\approx y\\;.\\text{''}\n\\end{aligned}\n\\]\nHow we interpret this heuristic statement depends on context. In regression problems, this statement typically means “\\(\\hat{y}\\) is usually close to \\(y\\)”, while in classification problems this statement usually means that “\\(\\hat{y} = y\\) exactly most or all of the time.”\nIn our regression example from above, we can think of a function \\(f:\\mathbb{R}\\rightarrow \\mathbb{R}\\) that maps the predictor \\(x\\) to the prediction \\(\\hat{y}\\). In the case of classification, things are a little more complicated. Although the function \\(g(x_1) = 1 - x_1\\) is visually very relevant, that function is not itself the model we use for prediction. Instead, our prediction function should return one classification label for points on one side of the line defined by that function, and a different label for points on the other side. If we say that blue points are labeled \\(0\\) and brown points are labeled \\(1\\), then our predictor function can be written \\(f:\\mathbb{R}^2 \\rightarrow \\{0, 1\\}\\), and it could be written heuristically like this:\n\\[\n\\begin{aligned}\n    f(\\mathbf{x}) &= \\mathbb{1}[\\mathbf{x} \\text{ is above the line}] \\\\\n                  &= \\mathbb{1}[x_1 + x_2 \\geq 1] \\\\\n                  &= \\mathbb{1}[x_1 + x_2 - 1\\geq 0]\\;.\n\\end{aligned}\n\\]\nThis last expression looks a little clunky, but we will soon find out that it is the easiest one to generalize to an advanced setting.\n\n\n\n\nHere, \\(\\mathbb{1}\\) is the indicator function which is equal to 1 if its argument is true and 0 otherwise. Formally,\n\\[\n\\begin{aligned}\n    \\mathbb{1}[P] = \\begin{cases}\n        1 &\\quad P \\text{ is true} \\\\\n        0 &\\quad P \\text{ is false.}\n        \\end{cases}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapters/02-black-box-classification.html#classifying-the-palmer-penguins",
    "href": "chapters/02-black-box-classification.html#classifying-the-palmer-penguins",
    "title": "2  Classification as a Black Box",
    "section": "Classifying the Palmer Penguins",
    "text": "Classifying the Palmer Penguins\n\n\n\nImage source: @allisonhorst\n\n\nOur data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins.\n.The Palmer Penguins data was originally collected by Gorman, Williams, and Fraser (2014) and was nicely packaged and released for use in the data science community by Horst, Hill, and Gorman (2020). You can find a very concise summary of the main workflow using a similar data set in Vanderplas (2016).\nLet’s go ahead and acquire the data.\n\nimport warnings\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\n\n\ndf = pd.read_csv(url)\n\n The df variable holds a pandas.DataFrame object. You can think of a data frame as a table of data with a variety of useful behaviors for data manipulation and visualization.You can learn much more about the capabilities of pandas.DataFrame objects in Chapter 3 of Vanderplas (2016)\nLet’s take a look:\n\ndf.head() # first 5 rows\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\nIt’s always useful to get acquainted with the “basics” of the data. For example, how many rows and columns do we have?\n\ndf.shape # (rows, columns)\n\n(344, 17)\n\n\nWhat are the data types of the columns? str columns are represented with the generic object in Pandas.\n\ndf.dtypes \n\nstudyName               object\nSample Number            int64\nSpecies                 object\nRegion                  object\nIsland                  object\nStage                   object\nIndividual ID           object\nClutch Completion       object\nDate Egg                object\nCulmen Length (mm)     float64\nCulmen Depth (mm)      float64\nFlipper Length (mm)    float64\nBody Mass (g)          float64\nSex                     object\nDelta 15 N (o/oo)      float64\nDelta 13 C (o/oo)      float64\nComments                object\ndtype: object\n\n\nHere’s the question we’ll ask today about this data set:\n\nGiven some physiological measurements of a penguin, can we reliably infer its species?"
  },
  {
    "objectID": "chapters/02-black-box-classification.html#data-preparation",
    "href": "chapters/02-black-box-classification.html#data-preparation",
    "title": "2  Classification as a Black Box",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe can select our desired columns from the data frame, operate on them, and make assignments to them using the data-frame-as-dictionary paradigm explored in Vanderplas (2016).\nIn applied data science, at least 80% of the work is typically spent acquiring and preparing data. Here, we’re going to do some simple data preparation directed by our question. It’s going to be convenient to shorten the Species column for each penguin. Furthermore, for visualization purposes today we are going to focus on the Culmen Length (mm) and Culmen Depth (mm) columns.\n\n# use only these three columns\ndf = df[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Species\"]] \n\n# remove any rows that have missing data in any of the selected columns. \ndf = df.dropna()\n\n# slightly advanced syntax: \n# replace the column with the first word in each entry\ndf[\"Species\"] = df[\"Species\"].str.split().str.get(0)\n\nLet’s take a look at what we’ve done so far:\n\ndf.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nSpecies\n\n\n\n\n0\n39.1\n18.7\nAdelie\n\n\n1\n39.5\n17.4\nAdelie\n\n\n2\n40.3\n18.0\nAdelie\n\n\n4\n36.7\n19.3\nAdelie\n\n\n5\n39.3\n20.6\nAdelie\n\n\n\n\n\n\n\nAs another preprocessing step, we are going to add transformed labels represented as integers.\n\n# for later: assign an integer to each species\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf[\"species_label\"] = le.fit_transform(df[\"Species\"])\n\nfor i, c in enumerate(le.classes_):\n    print(f\"Class number {i} represents {c} penguins.\")\n\nClass number 0 represents Adelie penguins.\nClass number 1 represents Chinstrap penguins.\nClass number 2 represents Gentoo penguins.\n\n\nNow our data looks like this:\n\ndf.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nSpecies\nspecies_label\n\n\n\n\n0\n39.1\n18.7\nAdelie\n0\n\n\n1\n39.5\n17.4\nAdelie\n0\n\n\n2\n40.3\n18.0\nAdelie\n0\n\n\n4\n36.7\n19.3\nAdelie\n0\n\n\n5\n39.3\n20.6\nAdelie\n0\n\n\n\n\n\n\n\n\nTrain-Test Split\nWhen designing predictive models, it’s important to evaluate them in a context that simulates the prediction application as accurately as possible. One important way we do this is by performing a train-test split. We keep most of the data as training data which we’ll use to design the model. We’ll hold out a bit of the data as testing data, which we’ll treat as unseen and only use once we are ready to evaluate our final design. The testing data simulates the idea of “new, unseen data” – exactly the kind of data on which it would be useful for us to make predictions!\n\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, test_size = 0.2)\n\nLet’s check the size of our two split data sets:\n\ndf_train.shape, df_test.shape\n\n((273, 4), (69, 4))\n\n\nNow we’re going to forget that df_test exists for a while. Instead, we’ll turn our attention to analysis, visualization and modeling."
  },
  {
    "objectID": "chapters/02-black-box-classification.html#data-analysis-and-visualization",
    "href": "chapters/02-black-box-classification.html#data-analysis-and-visualization",
    "title": "2  Classification as a Black Box",
    "section": "Data Analysis and Visualization",
    "text": "Data Analysis and Visualization\nAs a first step, it’s useful to understand how many of each species there are in the training data:\nThis is an example of a “split-apply-combine” operation (Wickham 2011). We split the dataframe into three groups depending on the species label, apply an operation (in this case, computing the number of rows), and then combine the results into a single object. Pandas implements split-apply-combine primarily through the groupby method and several associated functions. There are some nice examples of split-apply-combine in Pandas in Vanderplas (2016).\n\ndf_train.groupby(\"Species\").size()\n\nSpecies\nAdelie       123\nChinstrap     54\nGentoo        96\ndtype: int64\n\n\nThere are more Adelie penguins than Chintraps or Gentoos in this data set. Here are the proportions:\n\ndf_train.groupby(\"Species\").size() / df_train.shape[0] # divide by total rows\n\nSpecies\nAdelie       0.450549\nChinstrap    0.197802\nGentoo       0.351648\ndtype: float64\n\n\nSo, over 40% of the penguins in the data are Adelie penguins. One important consequence of this proportion is the base rate of the classification problem. The base rate refers to how well we could perform at prediction if we did not use any kind of predictive modeling, but instead simply predicted the most common class for every penguin. Here, if we always predicted “Adelie” for the species, we’d expect to be right more than 40% of the time. So, a minimal expectation of anything fancier we do is that it should be correct much more than 40% of the time.\nNow let’s take a look at our (training) data and see whether our chosen columns look like they have a chance of predicting the penguin species. We’ll show the plot both without and with the species labels.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\np1 = sns.scatterplot(df_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", ax = ax[0], color = \"darkgrey\")\np2 = sns.scatterplot(df_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue = \"Species\", ax = ax[1])\n\n\n\n\nThese plots are generated using the Seaborn library for Python. Seaborn is a high-level wrapper around the classical matplotlib library for data visualization. Although Matplotlib is very flexible, Seaborn is optimized for visualizing data contained in Pandas data frames. You can find many examples of creating Seaborn plots in the official gallery, and many tips and examples for matplotlib in Vanderplas (2016).\n\n\n\n\nWe can think of the lefthand side as “what the model will see:” just physiological measurements with no labels. On the right we can see the data with its species labels included. We can see that the species are divided into clusters: Adelie penguins have measurements which tend to be similar to other Adelies; Chinstraps are similar to other Chinstraps, etc.\nThis pattern is promising! The approximate separation of the species suggests that a machine learning model which predicts the species label from these measurements is likely to be able to beat the base rate."
  },
  {
    "objectID": "chapters/02-black-box-classification.html#modeling-and-model-selection",
    "href": "chapters/02-black-box-classification.html#modeling-and-model-selection",
    "title": "2  Classification as a Black Box",
    "section": "Modeling and Model Selection",
    "text": "Modeling and Model Selection\nLet’s go ahead and fit some models! We’re going to fit two models that are pre-implemented in the package scikit-learn. For now, you can think of these models as black-box algorithms that accept predictor variables as inputs and return a predicted target as an output. In our case, the predictor variables are the culmen length and culmen depth columns, while the target we are attempting to predict is the species. Later on, we’ll learn more about how some of these models actually work.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\nIt’s convenient to split our data into predictors \\(\\mathbf{X}\\) and targets \\(\\mathbf{y}\\). We need to do this once for each of the training and test sets.\n\npredictor_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\ntarget_col = \"species_label\"\n\nX_train = df_train[predictor_cols]\ny_train = df_train[target_col]\n\nX_test = df_test[predictor_cols]\ny_test = df_test[target_col]\n\nLet’s take a quick look at X_train\n\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\n\n\n\n\n186\n49.7\n18.6\n\n\n181\n52.8\n20.0\n\n\n170\n46.4\n18.6\n\n\n341\n50.4\n15.7\n\n\n240\n50.2\n14.3\n\n\n...\n...\n...\n\n\n149\n37.8\n18.1\n\n\n121\n37.7\n19.8\n\n\n43\n44.1\n19.7\n\n\n310\n47.5\n15.0\n\n\n1\n39.5\n17.4\n\n\n\n\n273 rows × 2 columns\n\n\n\nWe’ll go in-depth on logistic regression later in this course.\nNow we’re ready to fit our first machine learning model. Let’s try logistic regression! In the Scikit-learn API, we first need to instantiate the LogisticRegression() class, and then call the fit() method of this class on the training predictors and targets.\n\nLR = LogisticRegression()\nm = LR.fit(X_train, y_train)\n\nSo, uh, did it work? The LogisticRegression() class includes a handy method to compute the accuracy of the classifier:\n\nLR.score(X_train, y_train)\n\n0.9633699633699634\n\n\nWow! Much better than the base rate. Note that this is the accuracy on the training data. In theory, accuracy on the test data could look very different.\nA useful way to visualize models with two numerical predictors is via decision regions. Each region describes the set of possible measurements that would result in a given classification.\nYou can unfold this code to see a simple implementation of a function for plotting decision regions which wraps the plot_decision_regions function of the mlxtend package.\n\n\nCode\ndef decision_regions(X, y, model, title):\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        ax = plot_decision_regions(X_train.to_numpy(), y_train.to_numpy(), clf = model, legend = 2)\n\n        handles, labels = ax.get_legend_handles_labels()\n        ax.legend(handles, \n                le.classes_, \n                framealpha=0.3, scatterpoints=1)\n\n        ax.set(xlabel = \"Culmen Length (mm)\", ylabel = \"Culmen Depth (mm)\", title = f\"{title}: Accuracy = {model.score(X, y):.3f}\")\n\ndecision_regions(X_train, y_train, LR, \"Decision Regions for Logistic Regression\")\n\n\n\n\n\nYou can learn more about how support vector machines work in Vanderplas (2016). We’ll also study these models later in the course.\nWhile we’re at it, let’s try fitting a different classifier, also supplied by Scikit-learn. This classifier is called support vector machine (SVM).\n\nSVM = SVC(gamma = 5)\nSVM.fit(X_train, y_train)\ndecision_regions(X_train, y_train, SVM, \"Decision Regions for Support Vector Machine\")\n\n\n\n\nWow! The support vector machine classifier achieved even higher accuracy on the training data. This is enabled by the greater flexibility of the SVM. Flexibility comes from a lot of places in machine learning, and generally refers to the ability of models to learn complicated decision boundaries like the ones shown here.\nBut is this increased flexibility a good thing? You might look at this predictor and think that something funny is going on. For example, shouldn’t a point on the bottom right be more likely to be a Gentoo penguin than an Adelie?…\n\nSimulating Evaluation: Cross-Validation\nNow we have two competing classification models: logistic regression and support vector machine. Which one is going to do the best job of prediction on totally new, unseen data? We could go ahead and evaluate on our test set, but for statistical reasons we need to avoid doing this until we’ve made a final choice of classifier.\nVanderplas (2016) has more on cross-validation and overfitting. We’ll confront overfitting agian many times in this course.\nIn order to make an assessment, we can simulate the process of fitting the model and evaluating on “test” data by witholding parts of our training data to use as testing. We split the data into chunks and withold each chunk, using the other chunks to train the data. This is called cross-validation, and it is illustrated in this figure:\n\n\n\nImage source: scikit-learn\n\n\nWe could do this with a janky for-loop, but the nice scikit-learn developers have implemented this for us. Here’s an example of cross-validation with 5 folds. This can take a little while, as there are actually 5 calls to model.fit() happening under the hood each time.\n\nfrom sklearn.model_selection import cross_val_score\n\nFirst let’s compute the cross-validation accuracies for logistic regression:\n\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv=5)\ncv_scores_LR\n\n/Users/philchodrow/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n\n\narray([0.964, 0.982, 0.964, 0.981, 0.926])\n\n\nA convenient way to summarize these results is by computing the average:\n\ncv_scores_LR.mean()\n\n0.9632996632996633\n\n\nLet’s compare to SVM:\n\ncv_scores_SVM = cross_val_score(SVM, X_train, y_train, cv=5)\ncv_scores_SVM.mean()\n\n0.831919191919192\n\n\nAh! It looks like our SVM classifier was indeed too flexible to do well in predicting data that it hasn’t seen before. Although the SVM had better training accuracy than the logistic regression model, it failed to generalize to the task of unseen prediction. This phenomenon is called overfitting. Dealing with overfitting is one of the fundamental modeling challenges in applied machine learning."
  },
  {
    "objectID": "chapters/02-black-box-classification.html#model-evaluation",
    "href": "chapters/02-black-box-classification.html#model-evaluation",
    "title": "2  Classification as a Black Box",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nSo far, we’ve fit a logistic regression model and a support vector machine model; compared the two on a cross-validation task; and determined that the logistic regression model is most likely to generalize. Let’s now retrain the logistic regression model on the complete training data and finally evaluate it on the test set:\n\nLR.fit(X_train,y_train) \nLR.score(X_test, y_test)\n\n0.9710144927536232\n\n\nNot bad! This is our final estimate for the accuracy of our model as a classification tool on unseen penguin data.\n\nBeyond Accuracy\nAccuracy is a simple measure of how many errors a model makes. In many applications, it’s important to understand what kind of errors the model makes, a topic which we’ll study much more when we come to decision theory in the near future. We can get a quick overview of the kinds of mistakes that a model makes by computing the confusion matrix between the true labels and predictions. This matrix cross-tabulates all the true labels with all the predicted ones.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[27,  1,  0],\n       [ 1, 13,  0],\n       [ 0,  0, 27]])\n\n\nThe entry in the ith row and jth column of the confusion matrix gives the number of data points that have true label i and predicted label j from our model.\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 27 Adelie penguin(s) who were classified as Adelie.\nThere were 1 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 1 Chinstrap penguin(s) who were classified as Adelie.\nThere were 13 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 27 Gentoo penguin(s) who were classified as Gentoo."
  },
  {
    "objectID": "chapters/02-black-box-classification.html#recap",
    "href": "chapters/02-black-box-classification.html#recap",
    "title": "2  Classification as a Black Box",
    "section": "Recap",
    "text": "Recap\nIn these notes, we took a very quick tour of the core data science workflow. We considered a simple classification problem in which we acquired some data, cleaned it up a bit, visualized several of its features, used those features to make a predictive classification model, visualized that model, and evaluated its accuracy. Along the way, we encountered the phenomenon of overfitting: models that are too flexible will achieve remarkable accuracy on the training set but will generalize poorly to unseen data. The problem of designing models that are “flexible enough” and “in the right way” is a fundamental driving force in modern machine learning, and the deep learning revolution can be viewed as the latest paradigm for seeking appropriately flexible models.\nSo far, we haven’t attempted to understand how any of these predictive models actually work. We’ll dive into this topic soon."
  },
  {
    "objectID": "chapters/02-black-box-classification.html#references",
    "href": "chapters/02-black-box-classification.html#references",
    "title": "2  Classification as a Black Box",
    "section": "References",
    "text": "References\n\n\n\n\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” Edited by André Chiaradia. PLoS ONE 9 (3): e90081. https://doi.org/10.1371/journal.pone.0090081.\n\n\nHorst, Allison M, Alison Presmanes Hill, and Kristen B Gorman. 2020. “Allisonhorst/Palmerpenguins: V0.1.0.” Zenodo. https://doi.org/10.5281/ZENODO.3960218.\n\n\nVanderplas, Jacob T. 2016. Python Data Science Handbook: Essential Tools for Working with Data. First edition. Sebastopol, CA: O’Reilly Media, Inc.\n\n\nWickham, Hadley. 2011. “The Split-Apply-Combine Strategy for Data Analysis.” Journal of Statistical Software 40 (1). https://doi.org/10.18637/jss.v040.i01."
  },
  {
    "objectID": "chapters/03-score-based-classification.html#what-about-nonlinear-scores",
    "href": "chapters/03-score-based-classification.html#what-about-nonlinear-scores",
    "title": "3  Score-Based Classification",
    "section": "What About Nonlinear Scores?",
    "text": "What About Nonlinear Scores?\nYou’ll notice in Figure 3.1 that the decision boundary is a straight line. This is due to the way that we chose to compute scores. Recall that the score function we used is \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\). Since we imposed a threshold \\(t\\), the decision boundary is defined by the equation \\(t = s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\). Generically, this is the equation of a hyperplane (technically, an affine subspace). The dimension of this space is \\(p-1\\), where \\(p\\) is the number of features. Here we have two features, so the decision boundary is a \\(2-1=1\\)-dimensional subspace–i.e. a line.\nWhat if we think a curved decision boundary would be more appropriate? In that case, we need to define a score function that factors in the features in a nonlinear way.\nWe started by representing each point as a 2-vector of predictors \\(\\mathbf{x} = \\left(\\text{loan interest rate}, \\text{loan percent income}\\right)\\). Let’s now add a feature map \\(\\phi\\) that accepts this vector and adds three nonlinear functions of the predictors:\n\\[\n\\begin{aligned}\n    \\phi(\\mathbf{x}) =\n        \\left(\\begin{matrix}\n            \\text{loan interest rate} \\\\\n            \\text{loan percent income} \\\\\n            \\left(\\text{loan interest rate}\\right)^2 \\\\  \n            \\left(\\text{loan percent income}\\right)^2 \\\\\n            \\text{loan interest rate} \\times \\text{loan percent income}\n        \\end{matrix}\\right)\n\\end{aligned}\n\\]\nBecause the new features are order-2 polynomials in the predictors, this feature map is often called the quadratic feature map.\nWe’ll still use an inner product to compute our score but now the formula will be  \\[\n\\begin{aligned}\n    s_i = \\langle \\mathbf{w}, \\phi(\\mathbf{x}_i) \\rangle\\;.\n\\end{aligned}\n\\]In order for this formula to make sense, we now need \\(\\mathbf{w}\\in \\mathbb{R}^5\\).\nHere’s an implementation of a score function with quadratic features:\n\ndef quadratic_score(w, X0, X1):\n    return w[0]*X0 + w[1]*X1 + w[2]*X0**2 + w[3]*X1**2 + w[4]*X0*X1\n\nNow we can set a new vector of weights \\(\\mathbf{w}\\in \\mathbb{R}^5\\) and a threshold \\(t\\).\n\nw = np.array([0.01, 1, 0.0005, 0.6, 0.001])\nthreshold = 0.5\n\nOur classification now looks like this:\n\nfig, ax = plt.subplots(1, 1)\nplot_score(ax, quadratic_score, w, df)\nscatter_data(ax, df)\nplot_threshold(ax, quadratic_score, w,  df, threshold)\n\n\n\n\nFigure 3.2: quadratic score-based classification.\n\n\n\n\nHow accurate were we?\n\ndf[\"decision\"] = predict(quadratic_score, w, threshold, df)\n(df[\"decision\"] == df[\"loan_status\"]).mean()\n\n0.777\n\n\nOur nonlinear score function was very slightly more accurate than our linear score function on training data. A few things to keep in mind:\n\nPerformance on training data is not always a reliable indicator of performance on unseen data.\nAdding nonlinear features is one way of adding flexibility to a model, allowing that model to learn complicated, “wiggly” decision patterns. As we saw with the Palmer penguins case study, too much model flexibility can lead to worse predictive performance. We’ll regularly revisit the problem of balancing flexibility/features against predictive generalization throughout these notes.\n\n\nRecap\nSo, we looked at a simplified data set in which we were able to observe some features of each prospective borrower \\(i\\) in the form of a vector \\(\\mathbf{x}_i\\). We then computed a score for each borrower \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) and used a threshold to decide whether or not to make a loan: the loan is approved if \\(s_i \\leq t\\) for a chosen threshold \\(t\\). We can think of this as a decision-making model for the loan approval problem.\nIs that the end of the story? Of course not! There are many questions remaining.\n\nModel Evaluation: How do we actually measure whether our decision-making model is good or not? Is accuracy the right measure? Is computing accuracy on the training data reliable? How would the model perform on unseen data that wasn’t used to decide \\(\\mathbf{w}\\) or \\(t\\)? What other ways could we measure the performance of models?\nLegitimacy: Is it morally and politically appropriate to use algorithmic decision-making in the context of loan applications? What is the potential for disparate harm? What is the potential for contributing to the reinforcement of historically disparity? In what cases could algorithmic loan-making be appropriate in a democratic society? In what cases could it constitute a violation of personal political or moral rights?\nTask Choice: How was the data collected? Is it complete? Why did I choose a certain set of predictors and targets? Are my predictors and targets reliable measurements of what they claim to represent? Whose interests are served by the existence of a machine learning model that completes this task?\nAlgorithm Design: What algorithm was used to find the model (i.e. the separating line)? Is that algorithm guaranteed to converge? Will it converge quickly? Would a different algorithm find a better model? Or would it find a model that is equally good more quickly?\nVectorization: Instead of classifying points in a measurement space, how could I instead classify images, videos, or bodies of text?\n\nWe’ll discuss all of these questions – in approximately this order – later in these notes."
  },
  {
    "objectID": "chapters/04-decision-theory.html#last-time",
    "href": "chapters/04-decision-theory.html#last-time",
    "title": "4  Decision Theory in Classification",
    "section": "Last time…",
    "text": "Last time…\n…we considered a prediction problem in which we observed \\(p\\) attributes of prospective borrower \\(i\\) in the form of a vector \\(\\mathbf{x}_i \\in \\mathbb{R}^p\\). We then formed a score for prospective borrower \\(i\\) using a weight vector \\(\\mathbf{w}\\in \\mathbb{R}^p\\) and an inner product:\nWe also developed the ability to compute nonlinear scores by instead computing the score as \\(s_i = \\langle \\mathbf{w},\\phi(\\mathbf{x}_i) \\rangle\\), where \\(\\phi\\) was a feature map that computed nonlinear functions of the entries of \\(\\mathbf{x}_i\\). For reasons that we’ll learn about when we study the theory of machine learning, this is still called a linear model, due to the fact that the score is a linear function of the vector \\(\\mathbf{w}\\). In this set of notes, we’ll always assume that \\(\\mathbf{x}\\) has already had a feature map applied to it, so that we can just focus on the simpler form of Equation 4.1.\n\\[\n\\begin{aligned}\n    s_i = \\langle \\mathbf{x}_i, \\mathbf{w}  \\rangle\\;.\n\\end{aligned}\n\\tag{4.1}\\]\nThen, we classified prospective borrowers into two categories based on a threshold \\(t \\in \\mathbb{R}\\):\n\nBorrowers who receive a loan had the property \\(s_i \\leq t\\).\nBorrowers who do not receive a loan have the property \\(s_i &gt; t\\).\n\nEquation 4.1 says that the score should be computed as a linear function of the features \\(\\mathbf{x}_i\\). Models with this property are called linear models and are fundamental in both classification and regression tasks.\nIn this set of notes, we are going to focus on one of the many questions we might ask about this framework: how do we choose the threshold \\(t\\)?  As we’ll see, this is a surprisingly tricky question that depends heavily on context.We’ll study later how to find \\(\\mathbf{w}\\)."
  },
  {
    "objectID": "chapters/04-decision-theory.html#lending-data-set",
    "href": "chapters/04-decision-theory.html#lending-data-set",
    "title": "4  Decision Theory in Classification",
    "section": "Lending Data Set",
    "text": "Lending Data Set\nTo illustrate our discussion, we are going to pull up the lending data set from the previous section.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nurl = \"../data/credit-risk/credit_risk_dataset.csv\"\n\n\n\ndf_all = pd.read_csv(url)\ndf = df_all[[\"loan_int_rate\", \"loan_percent_income\", \"loan_status\"]]\ndf = df.dropna()\n\nFollowing the usual paradigm in machine learning, we’re going to incorporate two elements which we previously saw when studying the Palmer penguins. First, we are going to hold off a part of our data set that we will not use for making any choices about how we design our decision algorithm. This held-off part of the data is called the test set. We’ll use it for a final evaluation of our model’s performance.\n\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size = 0.2) # 20% test set\n\nNext, we’ll distinguish our predictor and target variables in each of the train and test sets.\n\nX_train = df_train[[\"loan_int_rate\", \"loan_percent_income\"]]\ny_train = df_train[\"loan_status\"]\n\nX_test = df_test[[\"loan_int_rate\", \"loan_percent_income\"]]\ny_test = df_test[\"loan_status\"]"
  },
  {
    "objectID": "chapters/04-decision-theory.html#vectorized-computation-of-scores",
    "href": "chapters/04-decision-theory.html#vectorized-computation-of-scores",
    "title": "4  Decision Theory in Classification",
    "section": "Vectorized Computation of Scores",
    "text": "Vectorized Computation of Scores\nSuppose that we have a weight vector \\(\\mathbf{w}\\) and that we’d like to choose a threshold \\(t\\). To do this, we will compute all the scores on the training data and do some experiments. How should we compute training scores? As we know, the \\(i\\)th score is given by Equation 4.1. To compute scores for all \\(n\\) of our training points, we could write a loop like this: In our case, \\(n =\\) {python} n, the number of rows in the training data.\n\ns = [] # vector of scores\nfor i in range(n):\n    s.append(compute_score(X[i], w))\n\nwhere X[i] is the ith data point \\(\\mathbf{x}_i\\) and compute_score is a function that computes the score according to Equation 4.1. However, there’s a better way to do this if we step back from code into math for a moment. If \\(\\mathbf{s} \\in \\mathbb{R}^n\\) is a vector whose \\(i\\)th entry is the score \\(s_i\\), then we have\n\\[\n\\begin{aligned}\n    \\mathbf{s} = \\left(\n        \\begin{matrix}\n            \\langle \\mathbf{x}_1, \\mathbf{w} \\rangle \\\\\n            \\langle \\mathbf{x}_2, \\mathbf{w} \\rangle \\\\\n            \\vdots \\\\\n            \\langle \\mathbf{x}_n, \\mathbf{w} \\rangle\n        \\end{matrix}\n        \\right) = \\mathbf{X}\\mathbf{w}\\;,\n\\end{aligned}\n\\]\nwhere we have defined the predictor matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\)\n\\[\n\\begin{aligned}\n    \\mathbf{X} = \\left[\n        \\begin{matrix}\n            - \\mathbf{x}_1 -  \\\\\n            -\\mathbf{x}_2-  \\\\\n            \\vdots \\\\\n            -\\mathbf{x}_n -\n        \\end{matrix}\n        \\right] =\n        \\left[\n        \\begin{matrix}\n            x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n            x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n            \\vdots \\\\\n            x_{n1} & x_{n2} & \\cdots & x_{np}\n        \\end{matrix}\n        \\right]\\;.\n\\end{aligned}\n\\]\nThis is good news because it simplifies our life both mathematically and in code: the Numpy package supplies very fast matrix multiplication:\n\ndef linear_score(X, w):\n    return X@w\n\nNow, given \\(\\mathbf{w}\\), we can compute all the scores at once.\n\nw = np.array([0.01, 1.0])\ns = linear_score(X_train, w)\n\nHere is a histogram of the scores we just computed:\n\nhist = plt.hist(s)\nlabs = plt.gca().set(xlabel = r\"Score $s$\", ylabel = \"Frequency\")"
  },
  {
    "objectID": "chapters/04-decision-theory.html#types-of-error",
    "href": "chapters/04-decision-theory.html#types-of-error",
    "title": "4  Decision Theory in Classification",
    "section": "Types of Error",
    "text": "Types of Error\nNow that we have the scores, we can easily simulate decision-making with a given threshold. For example, the proportion predicted to default on their loan with a given threshold \\(t\\) can be computed like this:\n\nt = 0.4\npreds = s &gt;= t\npreds.mean()\n\n0.15005090785677924\n\n\nSo, how should we choose the threshold \\(t\\)? One possibility would be to try to choose the threshold in a way that maximizes the training accuracy, the number of times that the prediction agrees with the actual outcome (repaid or default) on the training data. Here’s an example of a quick grid search:\n\nfor t in np.linspace(0, 1, 11):\n    y_pred = s &gt;= t \n    acc = (y_pred == y_train).mean()\n    print(f\"A threshold of {t:.1f} gives an accuracy of {acc:.2f}.\")\n\nA threshold of 0.0 gives an accuracy of 0.22.\nA threshold of 0.1 gives an accuracy of 0.23.\nA threshold of 0.2 gives an accuracy of 0.46.\nA threshold of 0.3 gives an accuracy of 0.71.\nA threshold of 0.4 gives an accuracy of 0.82.\nA threshold of 0.5 gives an accuracy of 0.81.\nA threshold of 0.6 gives an accuracy of 0.79.\nA threshold of 0.7 gives an accuracy of 0.78.\nA threshold of 0.8 gives an accuracy of 0.78.\nA threshold of 0.9 gives an accuracy of 0.78.\nA threshold of 1.0 gives an accuracy of 0.78.\n\n\nHowever, accuracy is not always the most relevant measure. For example, Field and Stream estimates that there are, globally, approximately 70 unprovoked shark attacks each year. Since the population of the world is currently around \\(8.1\\times 10^9\\) people, the average probability that a specific individual will suffer an unprovoked shark attack in a year is approximately \\(70 / (8.1 \\times 10^9) \\approx 8.6 \\times 10^{-9}\\). So, if we created a shark attack predictor which always predicted “no shark attack,” our model would be correct approximately 99.999999% of the time. However, this model wouldn’t be very useful, and wouldn’t have anything to tell us about the activities that increase or reduce the risk of experience an attack.\nA second reason we may wish to measure something other than accuracy has to do with asymmetrical costs of error. If we incorrectly predict that an individual will suffer a shark attack but no attack occurs, this is not that big a problem. Yes, we were wrong, but no one got hurt. In contrast, if we incorrectly predict that an individual will not suffer a shark attach, then this is a big problem which potentially involves grievous bodily injury, death, trauma, legal liability, etc. So, in designing our predictor, we might want to prioritizing avoiding the second kind of error, even if that leads us to make more of the first kind of error.\nWhat are the types of error? For a binary outcome with a binary predictor, there are four possibilities:\n\n\nTable 4.1: Types of correct classifications and errors in a binary classification problem.\n\n\n\nAbbreviation\nTrue Outcome\nPredicted Outcome\n\n\n\n\nTrue positive\nTP\n1\n1\n\n\nFalse negative\nFN\n1\n0\n\n\nFalse positive\nFP\n0\n1\n\n\nTrue negative\nTN\n0\n0\n\n\n\n\nGiven a vector of true outcomes \\(\\mathbf{y}\\) and a vector of predictions \\(\\hat{\\mathbf{y}}\\), we can calculate frequencies of each outcome. For example, here are the false positives associated with a given threshold value:\n\nt = 0.5\ny_pred = s &gt;= t \n\n# number where outcome == 0 and prediction == 1\n((y_train == 0)*(y_pred == 1)).sum()\n\n285\n\n\nIn practice, it’s more convenient to compute all the error rates at once using the confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_train, y_pred)\n\narray([[18136,   285],\n       [ 4264,   887]])\n\n\nThe layout of the confusion matrix is:\ntrue positive,  false positive \nfalse negative, true negative\nIt is common to normalize these counts into rates:\n\n\n\n\n\n\n\n\n\n\nAbbreviation\nFormula\n\n\n\n\n\nTrue positive rate\nTPR\n\\(\\frac{\\mathrm{TPR}}{\\mathrm{TPR} + \\mathrm{FNR}}\\)\n\n\n\nFalse negative rate\nFNR\n\\(\\frac{\\mathrm{FNR}}{\\mathrm{TPR} + \\mathrm{FNR}}\\)\n\n\n\nFalse positive rate\nFPR\n\\(\\frac{\\mathrm{FPR}}{\\mathrm{FPR} + \\mathrm{TNR}}\\)\n\n\n\nTrue negative rate\nTNR\n\\(\\frac{\\mathrm{FPR}}{\\mathrm{FPR} + \\mathrm{TNR}}\\)\n\n\n\n\nIntuitively, the TPR measures the proportion of the time that the classifier predicts the correct (positive) label when the true outcome was positive. Similarly, the FPR measures the proportion of the time that the classifier predicts the incorrect (positive) label when the true outcome was negative. Because \\(\\mathrm{TPR} = 1 - \\mathrm{FNR}\\) and \\(\\mathrm{FPR} = 1 - \\mathrm{TNR}\\), folks usually only bother remembering and using \\(\\mathrm{TPR}\\) and \\(\\mathrm{FNR}\\).\nRather than computing these by hand, Scikit-learn offers a handy argument to confusion_matrix for computing these automatically and simultaneously:\n\nconfusion_matrix(y_train, y_pred, normalize = \"true\")\n\narray([[0.985, 0.015],\n       [0.828, 0.172]])\n\n\nLet’s do a quick check against the FPR using manual vectorized code. Cases where y_pred == 1 correspond to positive predictions, while cases where y_train == 0 correspond to true negative outcomes.\n\n# agrees with the top right corner of the normalized confusion matrix\n((y_pred == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n\n0.015471472775636501\n\n\n\nThe ROC Curve\nFor any given value of the threshold \\(t\\), we can compute the TPR and FPR. We can think of this process as defining a parametrized function, a curve in TPR-FPR space. This curve is the ROC curve ROC stands for “receiver operating characteristic,” a term that reflects the origin of the curve in detection of objects by radar.\nTo compute an ROC curve, we simply need to compute the TPR and FPR for many different values of the threshold \\(t\\) and plot them.\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\n\nnum_thresholds = 101\n\nFPR = np.zeros(num_thresholds)\nTPR = np.zeros(num_thresholds)\nT = np.linspace(s.min()-0.1, s.max()+0.1, num_thresholds)\ns    = linear_score(X_train, w)\n\nfor i in range(num_thresholds):\n    t = T[i]\n    preds    = s &gt;= t\n    FPR[i]   = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR[i]   = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\nax.plot(FPR, TPR, color = \"black\")\nax.plot([0,1], [0,1], linestyle=\"--\", color = \"grey\")\nax.set_aspect('equal')\n\nax.set(xlabel = \"False Positive Rate\", ylabel = \"True Positive Rate\", title = \"ROC Curve\")\n\n[Text(0.5, 0, 'False Positive Rate'),\n Text(0, 0.5, 'True Positive Rate'),\n Text(0.5, 1.0, 'ROC Curve')]\n\n\n\n\n\nWe can think of the ROC curve as a description of all the possible tradeoffs between the TPR and FPR that are achievable for a given score as we vary the threshold \\(t\\). For example, the curve tells us that if we are willing to tolerate a false positive rate of 0.40, then the best true positive rate we can achieve is approximately 0.77.\nROC curves are often used as a measure of the ability of a score function to classify data into two groups. Curves that bend farther towards the upper left corner of the plot are generally viewed as more effective classifiers. The area under the curve (AUC) is sometimes used as a single quantitative measure describing the classification quality.\n\n\nCost of Errors and Optimal Thresholding\nHow do we choose the tradeoff that works best for us? To answer this kind of question, we need to reflect back on the purpose for which we are building a classifier. According to Table 4.1, there are two ways to be correct (true positive, true negative) and two ways to make an error (false positive, false negative). In order to choose an appropriate tradeoff, we need to think about the benefit of being right in relation to the cost of being wrong.\nA logical way for a bank to approach this problem would be from the perspective of profit-maximization. In the lending business, a bank can make money when loans are fully repaid with interest, but lose money (usually much more) when an individual defaults on the loan. To keep the problem simple, suppose that the bank gains $1 every time they make a loan which is successfully paid back, and that the bank loses $2 every time they make a loan which ends in default. The first scenario happens when the bank makes a true positive identification, while the second case happens when the bank makes a false negative classification.  For a given threshold, the expected gain for the bank when making a loan is thenRemember that the “positive” outcome in this data set is default.\n\\[\n\\begin{aligned}\n    \\mathbb{E}[\\mathrm{gain}] = 1\\times \\text{TN} - 2\\times \\text{FN}\\;.\n\\end{aligned}\n\\]\nLet’s plot the expected gain as a function of the threshold:\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\ncost_of_FN = -2.0\ngain_of_TN = 1.0\n\ncost =  gain_of_TN*TNR  + cost_of_FN*FNR \n\nplt.plot(T, cost)\nplt.gca().set(ylim = (-0.2, 0.2), xlim = (0, 0.5))\nlabs = plt.gca().set(xlabel = r\"Threshold $t$\", ylabel = \"Expected profit per loan\")\n\n\n\n\nFor these costs, we observe that the bank can make a small expected profit (roughly 17 cents per loan) by using the given score function with threshold of roughly \\(t \\approx 0.21\\). Note that this is very different from the value of the thresold \\(t \\approx 0.4\\) which maximized the unweighted accuracy of the predictor."
  },
  {
    "objectID": "chapters/04-decision-theory.html#recap",
    "href": "chapters/04-decision-theory.html#recap",
    "title": "4  Decision Theory in Classification",
    "section": "Recap",
    "text": "Recap\nIn these notes, we studied a simple question: given a score \\(s_i = \\langle \\mathbf{x}_i, \\mathbf{w}\\rangle\\), how should we convert that score into a yes/no decision? We found that adjusting the threshold can have major consequences for the accuracy of the resulting classification algorithm, but also that pure accuracy may not be the most relevant metric to measure or optimize. We computed the ROC curve of the score, which is a visual indicator of the overall ability of the score function to balance the false positive rate against the true positive rate. Finally, we explored the possible tradeoffs between different kinds of errors by considering a simplified scenario in which different kinds of errors have different costs associated with them. We found that the threshold that optimizes expected gain under this setting can be very different from the threshold that optimizes unweighted accuracy."
  },
  {
    "objectID": "chapters/04-decision-theory.html#who-sets-the-cost-who-pays-the-cost",
    "href": "chapters/04-decision-theory.html#who-sets-the-cost-who-pays-the-cost",
    "title": "4  Decision Theory in Classification",
    "section": "Who Sets The Cost? Who Pays the Cost?",
    "text": "Who Sets The Cost? Who Pays the Cost?\nIn our analysis above, we assumed a simple optimization objective: the bank is going to maximize its net profit. In formulating this objective, we made assumptions about the costs of different outcomes – to the bank. It’s important to note that the costs of errors to the bank may look very different from the costs of those errors to individuals. For example, if the bank’s prediction system recommends that an individual be denied a loan and the bank acts on this recommendation, then the bank pays no cost. On the other hand, the individual may experience major costs, depending on the purpose for which the loan was requested.\nThis data set includes a coarse description of the purpose of each loan:\n\ndf_all.groupby(\"loan_intent\").size()\n\nloan_intent\nDEBTCONSOLIDATION    5212\nEDUCATION            6453\nHOMEIMPROVEMENT      3605\nMEDICAL              6071\nPERSONAL             5521\nVENTURE              5719\ndtype: int64\n\n\nWhat are the costs of being denied access to borrowed funds to pursue education? What about for medical care?\nIt is of fundamental importance to remember that machine learning systems are embedded in social context; that they are generally developed and implemented by people and organizations that occupy positions of power; and that the costs of these systems are often unequally shared by the people they impact. We will discuss these considerations in much greater detail soon."
  },
  {
    "objectID": "chapters/10-compas.html#data-preparation",
    "href": "chapters/10-compas.html#data-preparation",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Data Preparation",
    "text": "Data Preparation\n Let’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.This data set was obtained by Angwin et al. (2022) through a public records request. The data comprises two years worth of COMPAS scoring in Broward County, Florida.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nsns.set_style(\"whitegrid\")\nnp.set_printoptions(precision = 3)\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/compas/compas.csv\"\ncompas = pd.read_csv(url)\n\nFor today we are only going to consider a subset of columns.\n\ncols = [\"sex\", \"race\", \"decile_score\", \"two_year_recid\"]\ncompas = compas[cols]\n\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\n\nis_white = compas[\"race\"] == \"Caucasian\"\nis_black = compas[\"race\"] == \"African-American\"\n\ncompas = compas[is_white | is_black]\ncompas = compas.copy()\n\nOur data now looks like this:\n\ncompas.head()\n\n\n\n\n\n\n\n\nsex\nrace\ndecile_score\ntwo_year_recid\n\n\n\n\n1\nMale\nAfrican-American\n3\n1\n\n\n2\nMale\nAfrican-American\n4\n1\n\n\n3\nMale\nAfrican-American\n8\n0\n\n\n6\nMale\nCaucasian\n6\n1\n\n\n8\nFemale\nCaucasian\n1\n0"
  },
  {
    "objectID": "chapters/10-compas.html#preliminary-explorations",
    "href": "chapters/10-compas.html#preliminary-explorations",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Preliminary Explorations",
    "text": "Preliminary Explorations\nLet’s do some quick exploration of our data. How many defendants are present in this data of each sex?\n\ncompas.groupby(\"sex\").size()\n\nsex\nFemale    1219\nMale      4931\ndtype: int64\n\n\nWhat about race?\n\ncompas.groupby(\"race\").size()\n\nrace\nAfrican-American    3696\nCaucasian           2454\ndtype: int64\n\n\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as being produced by computing a score like \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) for each defendant \\(i\\), and then dividing these into the lowest 10% (decile score 1), the next 10% (decile score 2), the next 10% (decile score 3) and so on.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\np = sns.barplot(data = counts, x = \"decile_score\", y = \"n\", hue = \"race\", palette = \"pastel\")\n\n\n\n\nYou may notice that the number of white defendants who receive a given decile score tends to decrease as the score increases, whereas the number of Black defendants remains relatively constant.\nFinally, let’s take a look at the recidivism rate in the data:\n\ncompas[\"two_year_recid\"].mean()\n\n0.4661788617886179\n\n\nSo, in this data, approximately 47% of all defendants went on to be charged of another crime within the next two years. We can also compute the recidivism rate by race:\n\ncompas.groupby(\"race\")[\"two_year_recid\"].mean()\n\nrace\nAfrican-American    0.514340\nCaucasian           0.393643\nName: two_year_recid, dtype: float64"
  },
  {
    "objectID": "chapters/10-compas.html#the-propublica-findings",
    "href": "chapters/10-compas.html#the-propublica-findings",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "The ProPublica Findings",
    "text": "The ProPublica Findings\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of Angwin et al. (2022), we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\n\ncompas[\"predicted_high_risk\"] = (compas[\"decile_score\"] &gt; 4)\n\nNow we have a binary prediction, and we can compute things like confusion matrices:\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(compas[\"two_year_recid\"], \n                 compas[\"predicted_high_risk\"])\n\narray([[2129, 1154],\n       [ 993, 1874]])\n\n\nWe can normalize this confusion matrix to get things like the false positive and false negative rates:\n\nconfusion_matrix(compas[\"two_year_recid\"], \n                 compas[\"predicted_high_risk\"],\n                 normalize = \"true\")\n\narray([[0.648, 0.352],\n       [0.346, 0.654]])\n\n\nWe see that the algorithm (predicting recidivism if decile_score is 5 or above) is right about 65% of the time. A bit more specifically, both the true positive (TP) and true negative (TN) rates are approximately 65%. Both the false positive (FP) and false negative (FN) rates are approximately 35%.\nWe can also check the overall accuracy:\n\n(compas[\"two_year_recid\"] == compas[\"predicted_high_risk\"]).mean()\n\n0.6508943089430894\n\n\nThe accuracy is relatively consistent even when we break things down by race:\n\nblack_ix = compas[\"race\"] == \"African-American\"\nwhite_ix = compas[\"race\"] == \"Caucasian\"\n\ncorrect_pred = compas[\"two_year_recid\"] == compas[\"predicted_high_risk\"]\n\n# accuracy on Black defendants\naccuracy_black = correct_pred[black_ix].mean()\n\n# accuracy on white defendants\naccuracy_white = correct_pred[white_ix].mean()\n\naccuracy_black, accuracy_white\n\n(0.6382575757575758, 0.6699266503667481)\n\n\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race. Here’s the confusion matrix for Black defendants:\n\nconfusion_matrix(compas[\"two_year_recid\"][black_ix], \n                 compas[\"predicted_high_risk\"][black_ix],\n                 normalize = \"true\")\n\narray([[0.552, 0.448],\n       [0.28 , 0.72 ]])\n\n\nAnd here it is for white defendants:\n\nconfusion_matrix(compas[\"two_year_recid\"][white_ix], \n                 compas[\"predicted_high_risk\"][white_ix],\n                 normalize = \"true\")\n\narray([[0.765, 0.235],\n       [0.477, 0.523]])\n\n\nThe ProPublica study focused on the false positive rate (FPR), which is in the top right corner of the confusion matrices. The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to the idea of representational bias, in which algorithms learn and reproduce toxic stereotypes about certain groups of people.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. This is sometimes called allocative or distributional bias: bias in how resources or opportunities (in this case, freedom) are allocated or distributed between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well."
  },
  {
    "objectID": "chapters/10-compas.html#predictive-equality",
    "href": "chapters/10-compas.html#predictive-equality",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Predictive Equality",
    "text": "Predictive Equality\nLet’s see if we can be a little more precise about what is wrong here. We’ll give a name to the property that the COMPAS algorithm fails to satisfy:\nDefinition 5.1 is due to Corbett-Davies et al. (2017). Some related concepts to predictive equality are error rate balance (Chouldechova 2017), balance for the positive/negative class (Kleinberg 2018), and separation (Barocas, Hardt, and Narayanan 2023) are all related to predictive equality.\n\nDefinition 5.1 (Predictive Equality) A binary classifier satisfies predictive equality with respect to groups \\(A\\) and \\(B\\) if its false positive rate for group \\(A\\) are the same as its false positive rate for \\(B\\).\n\nSo, the claim of Angwin et al. (2022) et al. is:\n\n\nThe COMPAS algorithm fails to satisfy predictive equality with respect to race.\nThe COMPAS algorithm is therefore unjustly biased with respect to race.\n\n\nThis argument implicitly equates predictive equality with fairness or lack of bias. Is that the best or only way to think about fair decision-making?"
  },
  {
    "objectID": "chapters/10-compas.html#calibration",
    "href": "chapters/10-compas.html#calibration",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Calibration",
    "text": "Calibration\nIn fact, formally defining fairness in decision-making is a very complex topic.\nAngwin et al. (2022) kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. For example, Corbett-Davies et al. (2017) consider a different idea of fairness. While predictive equality requires that the FPRs for white and Black defendants be equal, calibration expresses a different intuition:\n\nDefinition 5.2 A score-based binary classifier satisfies calibration if, among everyone who receives a score of \\(s\\), the probability of a positive outcome does not vary systematically by group.\n\nIn the context of COMPAS, if COMPAS were calibrated, then it would be the case that\nCompare: a female student who receives an A- in a linear algebra class and a male student who receives an A- in a linear algebra class should both have the same chance of succeeding in a machine learning class.\n\nA white defendant and a Black defendant who each receive the same score should both have the same risk of recidivating.\n\nAnother way to say this is that a score of 7 means the same thing, no matter the race of the defendant.\nLet’s test for calibration in the decile scores. We can compute the recidivism rates for each race at each decile score using some Pandas .groupby magic:\n\nmeans = compas.groupby([\"race\", \"decile_score\"])[\"two_year_recid\"].mean().reset_index(name = \"mean\")\n\nsns.lineplot(data = means, x = \"decile_score\", y = \"mean\", hue = \"race\", palette = \"pastel\", legend = None)\n\npoints = sns.scatterplot(data = means, x = \"decile_score\", y = \"mean\", hue = \"race\", palette = \"pastel\")\n\n\n\n\nThe actual recidivism rate at each risk score is roughly the same between Black and white defendants, especially for decile scores past 5 or so.\nAngwin et al. (2022) convert the decile risk scores into a decision by assuming that an individual is classified as “high risk” if their decile score is 4 or above. If we follow this approach and ask about the rates of re-arrest in each group, we obtain the following results:\n\nmeans = compas.groupby([\"race\", \"predicted_high_risk\"])[\"two_year_recid\"].mean().reset_index(name = \"mean\")\n\np = sns.barplot(data = means, x = \"predicted_high_risk\", y = \"mean\", hue = \"race\", palette = \"pastel\")\n\n\n\n\nFrom the perspective of calibration using this threshold, the algorithm might actually appear to be biased in favor of Black defendants rather than white ones: of those who were predicted high risk, slightly more Black than white defendants were arrested within the next two years. Formal statistical hypothesis tests are typically used to determine whether this difference is sufficiently “real” to warrant correction. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be reasonably well calibrated.\nIndeed, in a rejoinder article published by affiliates of the company Northpointe which produced COMPAS, the fact that COMPAS satisfies calibration is one of the primary arguments (Flores, Bechtel, and Lowenkamp 2016)."
  },
  {
    "objectID": "chapters/10-compas.html#recap",
    "href": "chapters/10-compas.html#recap",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Recap",
    "text": "Recap\nIn these notes, we replicated the data analysis of Angwin et al. (2022), finding that the COMPAS algorithm has disparate error rates between Black and white defendants. We introduced the idea that we could formally define a definition of fairness, like predictive equality, and the idea that the COMPAS predictor violates this definition. On the other hand, we also introduced a different definition of fairness—calibration—and found that the COMPAS algorithm is relatively well-calibrated."
  },
  {
    "objectID": "chapters/10-compas.html#some-questions-moving-forward",
    "href": "chapters/10-compas.html#some-questions-moving-forward",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Some Questions Moving Forward",
    "text": "Some Questions Moving Forward\nThese findings raise some of the following questions:\n\nCan we have it all? Could we modify the COMPAS algorithm in such a way that it satisfies both predictive equality and calibration? Could we then call it “fair” or “unbiased?”\nAre there other ways to define fairness? Which ones are most compelling to us? Does the right idea of fairness depend on the context in which we apply it?\nHow did this happen? The COMPAS algorithm was never trained on race data about the defendant. How did it happen that this algorithm nevertheless made recommendations at different rates across groups?\nIs automated decision-making legitimate in this setting? Can it be legitimate (just, fair) to use an automated decision-system for making recommendations about parole and sentencing decisions at all? What safeguards and forms of recourse are necessary for the legitimate use of automated decision-making in criminal justice?\nWhat are the systemic impacts? Disparate sentencing decisions can have downstream impacts on communities and institutions. How could application of the COMPAS algorithm exacerbate systemic inequalities?"
  },
  {
    "objectID": "chapters/10-compas.html#references",
    "href": "chapters/10-compas.html#references",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "References",
    "text": "References\n\n\n\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2022. “Machine Bias.” In Ethics of Data and Analytics, 254–64. Auerbach Publications.\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press. https://fairmlbook.org/pdf/fairmlbook.pdf.\n\n\nChouldechova, Alexandra. 2017. “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.” Big Data 5 (2): 153–63. https://doi.org/10.1089/big.2016.0047.\n\n\nCorbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. “Algorithmic Decision Making and the Cost of Fairness.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 797–806. Halifax NS Canada: ACM. https://doi.org/10.1145/3097983.3098095.\n\n\nFlores, Anthony W, Kristin Bechtel, and Christopher T Lowenkamp. 2016. “False Positives, False Negatives, and False Analyses: A Rejoinder to Machine Bias: There’s Software Used Across the Country to Predict Future Criminals. And It’s Biased Against Blacks.” Federal Probation 80: 38.\n\n\nKleinberg, Jon. 2018. “Inherent Trade-Offs in Algorithmic Fairness.” In Abstracts of the 2018 ACM International Conference on Measurement and Modeling of Computer Systems, 40–40. Irvine CA USA: ACM. https://doi.org/10.1145/3219617.3219634."
  }
]