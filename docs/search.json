[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Welcome\nThis is a set of notes developed for an undergraduate course in machine learning. The target audience for these notes are undergraduates in computer science who have completed first courses in linear algebra and discrete mathematics. These notes draw on many sources, but are somewhat distinctive in the following ways:"
  },
  {
    "objectID": "index.html#pedagogical-features",
    "href": "index.html#pedagogical-features",
    "title": "Machine Learning",
    "section": "Pedagogical Features",
    "text": "Pedagogical Features\nThese notes are explicitly designed for undergraduate instruction in computer science. For this reason:\n\nComputational examples are integrated into the text and shown throughout.\nLive versions of lecture notes are supplied as downloadable Jupyter Notebooks, with certain code components removed. The purpose is to facilitate live-coding in lectures."
  },
  {
    "objectID": "index.html#use-and-reuse",
    "href": "index.html#use-and-reuse",
    "title": "Machine Learning",
    "section": "Use and Reuse",
    "text": "Use and Reuse\nThese notes were written by Phil Chodrow for the course CSCI 0451: Machine Learning at Middlebury College. All are welcome to use them for educational purposes. Attribution is appreciated but not expected."
  },
  {
    "objectID": "index.html#source-texts",
    "href": "index.html#source-texts",
    "title": "Machine Learning",
    "section": "Source Texts",
    "text": "Source Texts\nThese notes draw on several source texts, most of which are available for free online. These are:\n\nHardt and Recht (2022) is the primary influence for the overall arc of the notes.\nA Course in Machine Learning by Hal Daumé III is an accessible introduction to many of the topics and serves as a useful source of supplementary readings.\n\nAdditional useful readings:\n\nAbu-Mostafa, Magdon-Ismail, and Lin (2012): Learning From Data: A Short Course\nBarocas, Hardt, and Narayanan (2023) is an advanced text on questions of fairness in automated decision-making for readers who have some background in probability theory.\nBishop (2006) and Murphy (2022) are advanced texts which are most suitable for advanced readers who have already taken at least one course in probability theory.\n\nDeisenroth, Faisal, and Ong (2020) and Kroese et al. (2020) are useful readings focusing on some of the mathematical fundamentals.\nZhang, Lipton, and Li (2023) tells a helpful story of the fundamentals of deep learning."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Machine Learning",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis site was generated using the Quarto publishing system. It is hosted on GitHub and published via GitHub Pages."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Machine Learning",
    "section": "References",
    "text": "References\n\n\n\n\nAbu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012. Learning from Data: A Short Course. S.l. https://amlbook.com/.\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press. https://fairmlbook.org/pdf/fairmlbook.pdf.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nDeisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020. Mathematics for Machine Learning. Cambridge, UK New York, NY: Cambridge University Press. https://mml-book.github.io/book/mml-book.pdf.\n\n\nHardt, Moritz, and Benjamin Recht. 2022. Patterns, Predictions, and Actions. Princeton University Press. https://mlstory.org/pdf/patterns.pdf.\n\n\nKroese, Dirk P., Zdravko I. Botev, Thomas Taimre, and Radislav Vaisman. 2020. Data Science and Machine Learning: Mathematical and Statistical Methods. Chapman & Hall/CRC Machine Learning & Pattern Recognition Series. Boca Raton London New York: CRC Press, Taylor & Francis Group.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. Adaptive Computation and Machine Learning Series. Cambridge, Massachusetts: The MIT Press. https://probml.github.io/pml-book/book1.html.\n\n\nZhang, Aston, Zachary Lipton, and Mu Li. 2023. Dive into Deep Learning. Cambridge, UK: Cambridge University Press."
  },
  {
    "objectID": "chapters/01-data-and-models.html#supervised-learning",
    "href": "chapters/01-data-and-models.html#supervised-learning",
    "title": "1  Data, Patterns, and Models",
    "section": "Supervised Learning",
    "text": "Supervised Learning\nOur focus in these notes is almost exclusively on supervised learning. In supervised learning, we are able to view some attributes or features of a data point, which we call predictors. Traditionally, we collect these attributes into a vector called \\(\\mathbf{x}\\). Each data point then has a target, which could be either a scalar number or a categorical label. Traditionally, the target is named \\(y\\). We aim to predict the target based on the predictors using a model, which is a function \\(f\\). The result of applying the model \\(f\\) to the predictors \\(\\mathbf{x}\\) is our prediction or predicted target \\(f(\\mathbf{x})\\), to which we often give the name \\(\\hat{y}\\). Our goal is to choose \\(f\\) such that the predicted target \\(\\hat{y}\\) is equal to, or at least close to, the true target \\(y\\). We could summarize this with the heuristic statement:\n\\[\n\\begin{aligned}\n    \\text{``}f(\\mathbf{x}) = \\hat{y} \\approx y\\;.\\text{''}\n\\end{aligned}\n\\]\nHow we interpret this heuristic statement depends on context. In regression problems, this statement typically means “\\(\\hat{y}\\) is usually close to \\(y\\)”, while in classification problems this statement usually means that “\\(\\hat{y} = y\\) exactly most or all of the time.”\nIn our regression example from above, we can think of a function \\(f:\\mathbb{R}\\rightarrow \\mathbb{R}\\) that maps the predictor \\(x\\) to the prediction \\(\\hat{y}\\). In the case of classification, things are a little more complicated. Although the function \\(g(x_1) = 1 - x_1\\) is visually very relevant, that function is not itself the model we use for prediction. Instead, our prediction function should return one classification label for points on one side of the line defined by that function, and a different label for points on the other side. If we say that blue points are labeled \\(0\\) and brown points are labeled \\(1\\), then our predictor function can be written \\(f:\\mathbb{R}^2 \\rightarrow \\{0, 1\\}\\), and it could be written heuristically like this:\n\\[\n\\begin{aligned}\n    f(\\mathbf{x}) &= \\mathbb{1}[\\mathbf{x} \\text{ is above the line}] \\\\\n                  &= \\mathbb{1}[x_1 + x_2 \\geq 1] \\\\\n                  &= \\mathbb{1}[x_1 + x_2 - 1\\geq 0]\\;.\n\\end{aligned}\n\\]\nThis last expression looks a little clunky, but we will soon find out that it is the easiest one to generalize to an advanced setting.\n\n\n\n\nHere, \\(\\mathbb{1}\\) is the indicator function which is equal to 1 if its argument is true and 0 otherwise. Formally,\n\\[\n\\begin{aligned}\n    \\mathbb{1}[P] = \\begin{cases}\n        1 &\\quad P \\text{ is true} \\\\\n        0 &\\quad P \\text{ is false.}\n        \\end{cases}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapters/02-black-box-classification.html#classifying-the-palmer-penguins",
    "href": "chapters/02-black-box-classification.html#classifying-the-palmer-penguins",
    "title": "2  Classification as a Black Box",
    "section": "Classifying the Palmer Penguins",
    "text": "Classifying the Palmer Penguins\n\n\n\nImage source: @allisonhorst\n\n\nOur data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins.\nThe Palmer Penguins data was originally collected by Gorman, Williams, and Fraser (2014) and was nicely packaged and released for use in the data science community by Horst, Hill, and Gorman (2020). You can find a very concise summary of the main workflow using a similar data set in Vanderplas (2016).\nLet’s go ahead and acquire the data.\n\nimport warnings\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\n\n\ndf = pd.read_csv(url)\n\n The df variable holds a pandas.DataFrame object. You can think of a data frame as a table of data with a variety of useful behaviors for data manipulation and visualization.You can learn much more about the capabilities of pandas.DataFrame objects in Chapter 3 of Vanderplas (2016)\nLet’s take a look:\n\ndf.head() # first 5 rows\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\nIt’s always useful to get acquainted with the “basics” of the data. For example, how many rows and columns do we have?\n\ndf.shape # (rows, columns)\n\n(344, 17)\n\n\nWhat are the data types of the columns? str columns are represented with the generic object in Pandas.\n\ndf.dtypes \n\nstudyName               object\nSample Number            int64\nSpecies                 object\nRegion                  object\nIsland                  object\nStage                   object\nIndividual ID           object\nClutch Completion       object\nDate Egg                object\nCulmen Length (mm)     float64\nCulmen Depth (mm)      float64\nFlipper Length (mm)    float64\nBody Mass (g)          float64\nSex                     object\nDelta 15 N (o/oo)      float64\nDelta 13 C (o/oo)      float64\nComments                object\ndtype: object\n\n\nHere’s the question we’ll ask today about this data set:\n\nGiven some physiological measurements of a penguin, can we reliably infer its species?"
  },
  {
    "objectID": "chapters/02-black-box-classification.html#data-preparation",
    "href": "chapters/02-black-box-classification.html#data-preparation",
    "title": "2  Classification as a Black Box",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe can select our desired columns from the data frame, operate on them, and make assignments to them using the data-frame-as-dictionary paradigm explored in Vanderplas (2016).\nIn applied data science, at least 80% of the work is typically spent acquiring and preparing data. Here, we’re going to do some simple data preparation directed by our question. It’s going to be convenient to shorten the Species column for each penguin. Furthermore, for visualization purposes today we are going to focus on the Culmen Length (mm) and Culmen Depth (mm) columns.\n\n# use only these three columns\ndf = df[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Species\"]] \n\n# remove any rows that have missing data in any of the selected columns. \ndf = df.dropna()\n\n# slightly advanced syntax: \n# replace the column with the first word in each entry\ndf[\"Species\"] = df[\"Species\"].str.split().str.get(0)\n\nLet’s take a look at what we’ve done so far:\n\ndf.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nSpecies\n\n\n\n\n0\n39.1\n18.7\nAdelie\n\n\n1\n39.5\n17.4\nAdelie\n\n\n2\n40.3\n18.0\nAdelie\n\n\n4\n36.7\n19.3\nAdelie\n\n\n5\n39.3\n20.6\nAdelie\n\n\n\n\n\n\n\nAs another preprocessing step, we are going to add transformed labels represented as integers.\n\n# for later: assign an integer to each species\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf[\"species_label\"] = le.fit_transform(df[\"Species\"])\n\nfor i, c in enumerate(le.classes_):\n    print(f\"Class number {i} represents {c} penguins.\")\n\nClass number 0 represents Adelie penguins.\nClass number 1 represents Chinstrap penguins.\nClass number 2 represents Gentoo penguins.\n\n\nNow our data looks like this:\n\ndf.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nSpecies\nspecies_label\n\n\n\n\n0\n39.1\n18.7\nAdelie\n0\n\n\n1\n39.5\n17.4\nAdelie\n0\n\n\n2\n40.3\n18.0\nAdelie\n0\n\n\n4\n36.7\n19.3\nAdelie\n0\n\n\n5\n39.3\n20.6\nAdelie\n0\n\n\n\n\n\n\n\n\nTrain-Test Split\nWhen designing predictive models, it’s important to evaluate them in a context that simulates the prediction application as accurately as possible. One important way we do this is by performing a train-test split. We keep most of the data as training data which we’ll use to design the model. We’ll hold out a bit of the data as testing data, which we’ll treat as unseen and only use once we are ready to evaluate our final design. The testing data simulates the idea of “new, unseen data” – exactly the kind of data on which it would be useful for us to make predictions!\n\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, test_size = 0.2)\n\nLet’s check the size of our two split data sets:\n\ndf_train.shape, df_test.shape\n\n((273, 4), (69, 4))\n\n\nNow we’re going to forget that df_test exists for a while. Instead, we’ll turn our attention to analysis, visualization and modeling."
  },
  {
    "objectID": "chapters/02-black-box-classification.html#data-analysis-and-visualization",
    "href": "chapters/02-black-box-classification.html#data-analysis-and-visualization",
    "title": "2  Classification as a Black Box",
    "section": "Data Analysis and Visualization",
    "text": "Data Analysis and Visualization\nAs a first step, it’s useful to understand how many of each species there are in the training data:\nThis is an example of a “split-apply-combine” operation (Wickham 2011). We split the dataframe into three groups depending on the species label, apply an operation (in this case, computing the number of rows), and then combine the results into a single object. Pandas implements split-apply-combine primarily through the groupby method and several associated functions. There are some nice examples of split-apply-combine in Pandas in Vanderplas (2016).\n\ndf_train.groupby(\"Species\").size()\n\nSpecies\nAdelie       118\nChinstrap     53\nGentoo       102\ndtype: int64\n\n\nThere are more Adelie penguins than Chintraps or Gentoos in this data set. Here are the proportions:\n\ndf_train.groupby(\"Species\").size() / df_train.shape[0] # divide by total rows\n\nSpecies\nAdelie       0.432234\nChinstrap    0.194139\nGentoo       0.373626\ndtype: float64\n\n\nSo, over 40% of the penguins in the data are Adelie penguins. One important consequence of this proportion is the base rate of the classification problem. The base rate refers to how well we could perform at prediction if we did not use any kind of predictive modeling, but instead simply predicted the most common class for every penguin. Here, if we always predicted “Adelie” for the species, we’d expect to be right more than 40% of the time. So, a minimal expectation of anything fancier we do is that it should be correct much more than 40% of the time.\nNow let’s take a look at our (training) data and see whether our chosen columns look like they have a chance of predicting the penguin species. We’ll show the plot both without and with the species labels.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\np1 = sns.scatterplot(df_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", ax = ax[0], color = \"darkgrey\")\np2 = sns.scatterplot(df_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue = \"Species\", style = \"Species\", ax = ax[1])\n\n\n\n\nThese plots are generated using the Seaborn library for Python. Seaborn is a high-level wrapper around the classical matplotlib library for data visualization. Although Matplotlib is very flexible, Seaborn is optimized for visualizing data contained in Pandas data frames. You can find many examples of creating Seaborn plots in the official gallery, and many tips and examples for matplotlib in Vanderplas (2016).\n\n\n\n\nWe can think of the lefthand side as “what the model will see:” just physiological measurements with no labels. On the right we can see the data with its species labels included. We can see that the species are divided into clusters: Adelie penguins have measurements which tend to be similar to other Adelies; Chinstraps are similar to other Chinstraps, etc.\nThis pattern is promising! The approximate separation of the species suggests that a machine learning model which predicts the species label from these measurements is likely to be able to beat the base rate."
  },
  {
    "objectID": "chapters/02-black-box-classification.html#modeling-and-model-selection",
    "href": "chapters/02-black-box-classification.html#modeling-and-model-selection",
    "title": "2  Classification as a Black Box",
    "section": "Modeling and Model Selection",
    "text": "Modeling and Model Selection\nLet’s go ahead and fit some models! We’re going to fit two models that are pre-implemented in the package scikit-learn. For now, you can think of these models as black-box algorithms that accept predictor variables as inputs and return a predicted target as an output. In our case, the predictor variables are the culmen length and culmen depth columns, while the target we are attempting to predict is the species. Later on, we’ll learn more about how some of these models actually work.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\nIt’s convenient to split our data into predictors \\(\\mathbf{X}\\) and targets \\(\\mathbf{y}\\). We need to do this once for each of the training and test sets.\n\npredictor_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\ntarget_col = \"species_label\"\n\nX_train = df_train[predictor_cols]\ny_train = df_train[target_col]\n\nX_test = df_test[predictor_cols]\ny_test = df_test[target_col]\n\nLet’s take a quick look at X_train\n\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\n\n\n\n\n99\n43.2\n18.5\n\n\n84\n37.3\n17.8\n\n\n337\n48.8\n16.2\n\n\n109\n43.2\n19.0\n\n\n105\n39.7\n18.9\n\n\n...\n...\n...\n\n\n53\n42.0\n19.5\n\n\n230\n40.9\n13.7\n\n\n201\n49.8\n17.3\n\n\n294\n46.4\n15.0\n\n\n205\n50.7\n19.7\n\n\n\n\n273 rows × 2 columns\n\n\n\nWe’ll go in-depth on logistic regression later in this course.\nNow we’re ready to fit our first machine learning model. Let’s try logistic regression! In the Scikit-learn API, we first need to instantiate the LogisticRegression() class, and then call the fit() method of this class on the training predictors and targets.\n\nLR = LogisticRegression()\nm = LR.fit(X_train, y_train)\n\nSo, uh, did it work? The LogisticRegression() class includes a handy method to compute the accuracy of the classifier:\n\nLR.score(X_train, y_train)\n\n0.9706959706959707\n\n\nWow! Much better than the base rate. Note that this is the accuracy on the training data. In theory, accuracy on the test data could look very different.\nA useful way to visualize models with two numerical predictors is via decision regions. Each region describes the set of possible measurements that would result in a given classification.\nYou can unfold this code to see a simple implementation of a function for plotting decision regions which wraps the plot_decision_regions function of the mlxtend package.\n\n\nCode\ndef decision_regions(X, y, model, title):\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        ax = plot_decision_regions(X_train.to_numpy(), y_train.to_numpy(), clf = model, legend = 2)\n\n        handles, labels = ax.get_legend_handles_labels()\n        ax.legend(handles, \n                le.classes_, \n                framealpha=0.3, scatterpoints=1)\n\n        ax.set(xlabel = \"Culmen Length (mm)\", ylabel = \"Culmen Depth (mm)\", title = f\"{title}: Accuracy = {model.score(X, y):.3f}\")\n\ndecision_regions(X_train, y_train, LR, \"Decision Regions for Logistic Regression\")\n\n\n\n\n\nYou can learn more about how support vector machines work in Vanderplas (2016). We’ll also study these models later in the course.\nWhile we’re at it, let’s try fitting a different classifier, also supplied by Scikit-learn. This classifier is called support vector machine (SVM).\n\nSVM = SVC(gamma = 5)\nSVM.fit(X_train, y_train)\ndecision_regions(X_train, y_train, SVM, \"Decision Regions for Support Vector Machine\")\n\n\n\n\nWow! The support vector machine classifier achieved even higher accuracy on the training data. This is enabled by the greater flexibility of the SVM. Flexibility comes from a lot of places in machine learning, and generally refers to the ability of models to learn complicated decision boundaries like the ones shown here.\nBut is this increased flexibility a good thing? You might look at this predictor and think that something funny is going on. For example, shouldn’t a point on the bottom right be more likely to be a Gentoo penguin than an Adelie?…\n\nSimulating Evaluation: Cross-Validation\nNow we have two competing classification models: logistic regression and support vector machine. Which one is going to do the best job of prediction on totally new, unseen data? We could go ahead and evaluate on our test set, but for statistical reasons we need to avoid doing this until we’ve made a final choice of classifier.\nVanderplas (2016) has more on cross-validation and overfitting. We’ll confront overfitting agian many times in this course.\nIn order to make an assessment, we can simulate the process of fitting the model and evaluating on “test” data by witholding parts of our training data to use as testing. We split the data into chunks and withold each chunk, using the other chunks to train the data. This is called cross-validation, and it is illustrated in this figure:\n\n\n\nImage source: scikit-learn\n\n\nWe could do this with a janky for-loop, but the nice scikit-learn developers have implemented this for us. Here’s an example of cross-validation with 5 folds. This can take a little while, as there are actually 5 calls to model.fit() happening under the hood each time.\n\nfrom sklearn.model_selection import cross_val_score\n\nFirst let’s compute the cross-validation accuracies for logistic regression:\n\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv=5)\ncv_scores_LR\n\n/Users/philchodrow/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n\n\narray([0.945, 0.927, 0.982, 0.963, 0.963])\n\n\nA convenient way to summarize these results is by computing the average:\n\ncv_scores_LR.mean()\n\n0.956094276094276\n\n\nLet’s compare to SVM:\n\ncv_scores_SVM = cross_val_score(SVM, X_train, y_train, cv=5)\ncv_scores_SVM.mean()\n\n0.8607407407407406\n\n\nAh! It looks like our SVM classifier was indeed too flexible to do well in predicting data that it hasn’t seen before. Although the SVM had better training accuracy than the logistic regression model, it failed to generalize to the task of unseen prediction. This phenomenon is called overfitting. Dealing with overfitting is one of the fundamental modeling challenges in applied machine learning."
  },
  {
    "objectID": "chapters/02-black-box-classification.html#model-evaluation",
    "href": "chapters/02-black-box-classification.html#model-evaluation",
    "title": "2  Classification as a Black Box",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nSo far, we’ve fit a logistic regression model and a support vector machine model; compared the two on a cross-validation task; and determined that the logistic regression model is most likely to generalize. Let’s now retrain the logistic regression model on the complete training data and finally evaluate it on the test set:\n\nLR.fit(X_train,y_train) \nLR.score(X_test, y_test)\n\n0.9420289855072463\n\n\nNot bad! This is our final estimate for the accuracy of our model as a classification tool on unseen penguin data.\n\nBeyond Accuracy\nAccuracy is a simple measure of how many errors a model makes. In many applications, it’s important to understand what kind of errors the model makes, a topic which we’ll study much more when we come to decision theory in the near future. We can get a quick overview of the kinds of mistakes that a model makes by computing the confusion matrix between the true labels and predictions. This matrix cross-tabulates all the true labels with all the predicted ones.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[32,  1,  0],\n       [ 2, 12,  1],\n       [ 0,  0, 21]])\n\n\nThe entry in the ith row and jth column of the confusion matrix gives the number of data points that have true label i and predicted label j from our model.\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 32 Adelie penguin(s) who were classified as Adelie.\nThere were 1 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 2 Chinstrap penguin(s) who were classified as Adelie.\nThere were 12 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 1 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 21 Gentoo penguin(s) who were classified as Gentoo."
  },
  {
    "objectID": "chapters/02-black-box-classification.html#recap",
    "href": "chapters/02-black-box-classification.html#recap",
    "title": "2  Classification as a Black Box",
    "section": "Recap",
    "text": "Recap\nIn these notes, we took a very quick tour of the core data science workflow. We considered a simple classification problem in which we acquired some data, cleaned it up a bit, visualized several of its features, used those features to make a predictive classification model, visualized that model, and evaluated its accuracy. Along the way, we encountered the phenomenon of overfitting: models that are too flexible will achieve remarkable accuracy on the training set but will generalize poorly to unseen data. The problem of designing models that are “flexible enough” and “in the right way” is a fundamental driving force in modern machine learning, and the deep learning revolution can be viewed as the latest paradigm for seeking appropriately flexible models.\nSo far, we haven’t attempted to understand how any of these predictive models actually work. We’ll dive into this topic soon."
  },
  {
    "objectID": "chapters/02-black-box-classification.html#references",
    "href": "chapters/02-black-box-classification.html#references",
    "title": "2  Classification as a Black Box",
    "section": "References",
    "text": "References\n\n\n\n\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” Edited by André Chiaradia. PLoS ONE 9 (3): e90081. https://doi.org/10.1371/journal.pone.0090081.\n\n\nHorst, Allison M, Alison Presmanes Hill, and Kristen B Gorman. 2020. “Allisonhorst/Palmerpenguins: V0.1.0.” Zenodo. https://doi.org/10.5281/ZENODO.3960218.\n\n\nVanderplas, Jacob T. 2016. Python Data Science Handbook: Essential Tools for Working with Data. First edition. Sebastopol, CA: O’Reilly Media, Inc.\n\n\nWickham, Hadley. 2011. “The Split-Apply-Combine Strategy for Data Analysis.” Journal of Statistical Software 40 (1). https://doi.org/10.18637/jss.v040.i01."
  },
  {
    "objectID": "chapters/03-score-based-classification.html#what-about-nonlinear-scores",
    "href": "chapters/03-score-based-classification.html#what-about-nonlinear-scores",
    "title": "3  Score-Based Classification",
    "section": "What About Nonlinear Scores?",
    "text": "What About Nonlinear Scores?\nYou’ll notice in Figure 3.1 that the decision boundary is a straight line. This is due to the way that we chose to compute scores. Recall that the score function we used is \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\). Since we imposed a threshold \\(t\\), the decision boundary is defined by the equation \\(t = s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\). Generically, this is the equation of a hyperplane (technically, an affine subspace). The dimension of this space is \\(p-1\\), where \\(p\\) is the number of features. Here we have two features, so the decision boundary is a \\(2-1=1\\)-dimensional subspace–i.e. a line.\nWhat if we think a curved decision boundary would be more appropriate? In that case, we need to define a score function that factors in the features in a nonlinear way.\nWe started by representing each point as a 2-vector of predictors \\(\\mathbf{x} = \\left(\\text{loan interest rate}, \\text{loan percent income}\\right)\\). Let’s now add a feature map \\(\\phi\\) that accepts this vector and adds three nonlinear functions of the predictors:\n\\[\n\\begin{aligned}\n    \\phi(\\mathbf{x}) =\n        \\left(\\begin{matrix}\n            \\text{loan interest rate} \\\\\n            \\text{loan percent income} \\\\\n            \\left(\\text{loan interest rate}\\right)^2 \\\\  \n            \\left(\\text{loan percent income}\\right)^2 \\\\\n            \\text{loan interest rate} \\times \\text{loan percent income}\n        \\end{matrix}\\right)\n\\end{aligned}\n\\]\nBecause the new features are order-2 polynomials in the predictors, this feature map is often called the quadratic feature map.\nWe’ll still use an inner product to compute our score but now the formula will be  \\[\n\\begin{aligned}\n    s_i = \\langle \\mathbf{w}, \\phi(\\mathbf{x}_i) \\rangle\\;.\n\\end{aligned}\n\\]In order for this formula to make sense, we now need \\(\\mathbf{w}\\in \\mathbb{R}^5\\).\nHere’s an implementation of a score function with quadratic features:\n\ndef quadratic_score(w, X0, X1):\n    return w[0]*X0 + w[1]*X1 + w[2]*X0**2 + w[3]*X1**2 + w[4]*X0*X1\n\nNow we can set a new vector of weights \\(\\mathbf{w}\\in \\mathbb{R}^5\\) and a threshold \\(t\\).\n\nw = np.array([0.01, 1, 0.0005, 0.6, 0.001])\nthreshold = 0.5\n\nOur classification now looks like this:\n\nfig, ax = plt.subplots(1, 1)\nplot_score(ax, quadratic_score, w, df)\nscatter_data(ax, df)\nplot_threshold(ax, quadratic_score, w,  df, threshold)\n\n\n\n\nFigure 3.2: quadratic score-based classification.\n\n\n\n\nHow accurate were we?\n\ndf[\"decision\"] = predict(quadratic_score, w, threshold, df)\n(df[\"decision\"] == df[\"loan_status\"]).mean()\n\n0.777\n\n\nOur nonlinear score function was very slightly more accurate than our linear score function on training data. A few things to keep in mind:\n\nPerformance on training data is not always a reliable indicator of performance on unseen data.\nAdding nonlinear features is one way of adding flexibility to a model, allowing that model to learn complicated, “wiggly” decision patterns. As we saw with the Palmer penguins case study, too much model flexibility can lead to worse predictive performance. We’ll regularly revisit the problem of balancing flexibility/features against predictive generalization throughout these notes.\n\n\nRecap\nSo, we looked at a simplified data set in which we were able to observe some features of each prospective borrower \\(i\\) in the form of a vector \\(\\mathbf{x}_i\\). We then computed a score for each borrower \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) and used a threshold to decide whether or not to make a loan: the loan is approved if \\(s_i \\leq t\\) for a chosen threshold \\(t\\). We can think of this as a decision-making model for the loan approval problem.\nIs that the end of the story? Of course not! There are many questions remaining.\n\nModel Evaluation: How do we actually measure whether our decision-making model is good or not? Is accuracy the right measure? Is computing accuracy on the training data reliable? How would the model perform on unseen data that wasn’t used to decide \\(\\mathbf{w}\\) or \\(t\\)? What other ways could we measure the performance of models?\nLegitimacy: Is it morally and politically appropriate to use algorithmic decision-making in the context of loan applications? What is the potential for disparate harm? What is the potential for contributing to the reinforcement of historically disparity? In what cases could algorithmic loan-making be appropriate in a democratic society? In what cases could it constitute a violation of personal political or moral rights?\nTask Choice: How was the data collected? Is it complete? Why did I choose a certain set of predictors and targets? Are my predictors and targets reliable measurements of what they claim to represent? Whose interests are served by the existence of a machine learning model that completes this task?\nAlgorithm Design: What algorithm was used to find the model (i.e. the separating line)? Is that algorithm guaranteed to converge? Will it converge quickly? Would a different algorithm find a better model? Or would it find a model that is equally good more quickly?\nVectorization: Instead of classifying points in a measurement space, how could I instead classify images, videos, or bodies of text?\n\nWe’ll discuss all of these questions – in approximately this order – later in these notes."
  },
  {
    "objectID": "chapters/04-decision-theory.html#last-time",
    "href": "chapters/04-decision-theory.html#last-time",
    "title": "4  Decision Theory in Classification",
    "section": "Last time…",
    "text": "Last time…\n…we considered a prediction problem in which we observed \\(p\\) attributes of prospective borrower \\(i\\) in the form of a vector \\(\\mathbf{x}_i \\in \\mathbb{R}^p\\). We then formed a score for prospective borrower \\(i\\) using a weight vector \\(\\mathbf{w}\\in \\mathbb{R}^p\\) and an inner product:\nWe also developed the ability to compute nonlinear scores by instead computing the score as \\(s_i = \\langle \\mathbf{w},\\phi(\\mathbf{x}_i) \\rangle\\), where \\(\\phi\\) was a feature map that computed nonlinear functions of the entries of \\(\\mathbf{x}_i\\). For reasons that we’ll learn about when we study the theory of machine learning, this is still called a linear model, due to the fact that the score is a linear function of the vector \\(\\mathbf{w}\\). In this set of notes, we’ll always assume that \\(\\mathbf{x}\\) has already had a feature map applied to it, so that we can just focus on the simpler form of Equation 4.1.\n\\[\n\\begin{aligned}\n    s_i = \\langle \\mathbf{x}_i, \\mathbf{w}  \\rangle\\;.\n\\end{aligned}\n\\tag{4.1}\\]\nThen, we classified prospective borrowers into two categories based on a threshold \\(t \\in \\mathbb{R}\\):\n\nBorrowers who receive a loan had the property \\(s_i \\leq t\\).\nBorrowers who do not receive a loan have the property \\(s_i &gt; t\\).\n\nEquation 4.1 says that the score should be computed as a linear function of the features \\(\\mathbf{x}_i\\). Models with this property are called linear models and are fundamental in both classification and regression tasks.\nIn this set of notes, we are going to focus on one of the many questions we might ask about this framework: how do we choose the threshold \\(t\\)?  As we’ll see, this is a surprisingly tricky question that depends heavily on context.We’ll study later how to find \\(\\mathbf{w}\\)."
  },
  {
    "objectID": "chapters/04-decision-theory.html#lending-data-set",
    "href": "chapters/04-decision-theory.html#lending-data-set",
    "title": "4  Decision Theory in Classification",
    "section": "Lending Data Set",
    "text": "Lending Data Set\nTo illustrate our discussion, we are going to pull up the lending data set from the previous section.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/credit_risk_dataset.csv\"\n\n\n\ndf_all = pd.read_csv(url)\ndf = df_all[[\"loan_int_rate\", \"loan_percent_income\", \"loan_status\"]]\ndf = df.dropna()\n\nFollowing the usual paradigm in machine learning, we’re going to incorporate two elements which we previously saw when studying the Palmer penguins. First, we are going to hold off a part of our data set that we will not use for making any choices about how we design our decision algorithm. This held-off part of the data is called the test set. We’ll use it for a final evaluation of our model’s performance.\n\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size = 0.2, random_state = 123) # 20% test set\n\nNext, we’ll distinguish our predictor and target variables in each of the train and test sets.\n\nX_train = df_train[[\"loan_int_rate\", \"loan_percent_income\"]]\ny_train = df_train[\"loan_status\"]\n\nX_test = df_test[[\"loan_int_rate\", \"loan_percent_income\"]]\ny_test = df_test[\"loan_status\"]"
  },
  {
    "objectID": "chapters/04-decision-theory.html#vectorized-computation-of-scores",
    "href": "chapters/04-decision-theory.html#vectorized-computation-of-scores",
    "title": "4  Decision Theory in Classification",
    "section": "Vectorized Computation of Scores",
    "text": "Vectorized Computation of Scores\nSuppose that we have a weight vector \\(\\mathbf{w}\\) and that we’d like to choose a threshold \\(t\\). To do this, we will compute all the scores on the training data and do some experiments. How should we compute training scores? As we know, the \\(i\\)th score is given by Equation 4.1. To compute scores for all \\(n\\) of our training points, we could write a loop like this: In our case, \\(n =\\) {python} n, the number of rows in the training data.\n\ns = [] # vector of scores\nfor i in range(n):\n    s.append(compute_score(X[i], w))\n\nwhere X[i] is the ith data point \\(\\mathbf{x}_i\\) and compute_score is a function that computes the score according to Equation 4.1. However, there’s a better way to do this if we step back from code into math for a moment. If \\(\\mathbf{s} \\in \\mathbb{R}^n\\) is a vector whose \\(i\\)th entry is the score \\(s_i\\), then we have\n\\[\n\\begin{aligned}\n    \\mathbf{s} = \\left(\n        \\begin{matrix}\n            \\langle \\mathbf{x}_1, \\mathbf{w} \\rangle \\\\\n            \\langle \\mathbf{x}_2, \\mathbf{w} \\rangle \\\\\n            \\vdots \\\\\n            \\langle \\mathbf{x}_n, \\mathbf{w} \\rangle\n        \\end{matrix}\n        \\right) = \\mathbf{X}\\mathbf{w}\\;,\n\\end{aligned}\n\\]\nwhere we have defined the predictor matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\)\n\\[\n\\begin{aligned}\n    \\mathbf{X} = \\left[\n        \\begin{matrix}\n            - \\mathbf{x}_1 -  \\\\\n            -\\mathbf{x}_2-  \\\\\n            \\vdots \\\\\n            -\\mathbf{x}_n -\n        \\end{matrix}\n        \\right] =\n        \\left[\n        \\begin{matrix}\n            x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n            x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n            \\vdots \\\\\n            x_{n1} & x_{n2} & \\cdots & x_{np}\n        \\end{matrix}\n        \\right]\\;.\n\\end{aligned}\n\\]\nThis is good news because it simplifies our life both mathematically and in code: the Numpy package supplies very fast matrix multiplication:\n\ndef linear_score(X, w):\n    return X@w\n\nNow, given \\(\\mathbf{w}\\), we can compute all the scores at once.\n\nw = np.array([0.01, 1.0])\ns = linear_score(X_train, w)\n\nHere is a histogram of the scores we just computed:\n\nhist = plt.hist(s)\nlabs = plt.gca().set(xlabel = r\"Score $s$\", ylabel = \"Frequency\")"
  },
  {
    "objectID": "chapters/04-decision-theory.html#types-of-error",
    "href": "chapters/04-decision-theory.html#types-of-error",
    "title": "4  Decision Theory in Classification",
    "section": "Types of Error",
    "text": "Types of Error\nNow that we have the scores, we can easily simulate decision-making with a given threshold. For example, the proportion predicted to default on their loan with a given threshold \\(t\\) can be computed like this:\n\nt = 0.4\npreds = s &gt;= t\npreds.mean()\n\n0.15386899711522145\n\n\nSo, how should we choose the threshold \\(t\\)? One possibility would be to try to choose the threshold in a way that maximizes the training accuracy, the number of times that the prediction agrees with the actual outcome (repaid or default) on the training data. Here’s an example of a quick grid search:\n\nfor t in np.linspace(0, 1, 11):\n    y_pred = s &gt;= t\n    acc = (y_pred == y_train).mean()\n    print(f\"A threshold of {t:.1f} gives an accuracy of {acc:.2f}.\")\n\nA threshold of 0.0 gives an accuracy of 0.22.\nA threshold of 0.1 gives an accuracy of 0.23.\nA threshold of 0.2 gives an accuracy of 0.46.\nA threshold of 0.3 gives an accuracy of 0.71.\nA threshold of 0.4 gives an accuracy of 0.82.\nA threshold of 0.5 gives an accuracy of 0.80.\nA threshold of 0.6 gives an accuracy of 0.79.\nA threshold of 0.7 gives an accuracy of 0.78.\nA threshold of 0.8 gives an accuracy of 0.78.\nA threshold of 0.9 gives an accuracy of 0.78.\nA threshold of 1.0 gives an accuracy of 0.78.\n\n\nHowever, accuracy is not always the most relevant measure. For example, Field and Stream estimates that there are, globally, approximately 70 unprovoked shark attacks each year. Since the population of the world is currently around \\(8.1\\times 10^9\\) people, the average probability that a specific individual will suffer an unprovoked shark attack in a year is approximately \\(70 / (8.1 \\times 10^9) \\approx 8.6 \\times 10^{-9}\\). So, if we created a shark attack predictor which always predicted “no shark attack,” our model would be correct approximately 99.999999% of the time. However, this model wouldn’t be very useful, and wouldn’t have anything to tell us about the activities that increase or reduce the risk of experience an attack.\nA second reason we may wish to measure something other than accuracy has to do with asymmetrical costs of error. If we incorrectly predict that an individual will suffer a shark attack but no attack occurs, this is not that big a problem. Yes, we were wrong, but no one got hurt. In contrast, if we incorrectly predict that an individual will not suffer a shark attach, then this is a big problem which potentially involves grievous bodily injury, death, trauma, legal liability, etc. So, in designing our predictor, we might want to prioritizing avoiding the second kind of error, even if that leads us to make more of the first kind of error.\nWhat are the types of error? For a binary outcome with a binary predictor, there are four possibilities:\n\n\nTable 4.1: Types of correct classifications and errors in a binary classification problem.\n\n\n\nAbbreviation\nTrue Outcome\nPredicted Outcome\n\n\n\n\nTrue positive\nTP\n1\n1\n\n\nFalse negative\nFN\n1\n0\n\n\nFalse positive\nFP\n0\n1\n\n\nTrue negative\nTN\n0\n0\n\n\n\n\nGiven a vector of true outcomes \\(\\mathbf{y}\\) and a vector of predictions \\(\\hat{\\mathbf{y}}\\), we can calculate frequencies of each outcome. For example, here are the false positives associated with a given threshold value:\n\nt = 0.5\ny_pred = s &gt;= t \n\n# number where outcome == 0 and prediction == 1\n((y_train == 0)*(y_pred == 1)).sum()\n\n286\n\n\nIn practice, it’s more convenient to compute all the error rates at once using the confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_train, y_pred)\n\narray([[18062,   286],\n       [ 4319,   905]])\n\n\nThe layout of the confusion matrix is:\ntrue negative,  false positive \nfalse negative, true positive\nIt is common to normalize these counts into rates:\n\n\n\n\n\n\n\n\n\n\nAbbreviation\nFormula\n\n\n\n\n\nTrue negative rate\nTNR\n\\(\\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}}\\)\n\n\n\nFalse positive rate\nFPR\n\\(\\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}}\\)\n\n\n\nFalse negative rate\nFNR\n\\(\\frac{\\mathrm{FN}}{\\mathrm{TP} + \\mathrm{FN}}\\)\n\n\n\nTrue positive rate\nTPR\n\\(\\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}\\)\n\n\n\n\nIntuitively, the TPR measures the proportion of the time that the classifier predicts the correct (positive) label when the true outcome was positive. Similarly, the FPR measures the proportion of the time that the classifier predicts the incorrect (positive) label when the true outcome was negative. Because \\(\\mathrm{TPR} = 1 - \\mathrm{FNR}\\) and \\(\\mathrm{FPR} = 1 - \\mathrm{TNR}\\), folks usually only bother remembering and using \\(\\mathrm{TPR}\\) and \\(\\mathrm{FNR}\\).\nRather than computing these by hand, Scikit-learn offers a handy argument to confusion_matrix for computing these automatically and simultaneously:\n\nconfusion_matrix(y_train, y_pred, normalize = \"true\")\n\narray([[0.984, 0.016],\n       [0.827, 0.173]])\n\n\nLet’s do a quick check against the FPR using manual vectorized code. Cases where y_pred == 1 correspond to positive predictions, while cases where y_train == 0 correspond to true negative outcomes.\n\n# agrees with the top right corner of the normalized confusion matrix\n((y_pred == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n\n0.015587529976019185\n\n\n\nThe ROC Curve\nFor any given value of the threshold \\(t\\), we can compute the TPR and FPR. We can think of this process as defining a parametrized function, a curve in TPR-FPR space. This curve is the ROC curve ROC stands for “receiver operating characteristic,” a term that reflects the origin of the curve in detection of objects by radar.\nTo compute an ROC curve, we simply need to compute the TPR and FPR for many different values of the threshold \\(t\\) and plot them.\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\n\nnum_thresholds = 101\n\nFPR = np.zeros(num_thresholds)\nTPR = np.zeros(num_thresholds)\nT = np.linspace(s.min()-0.1, s.max()+0.1, num_thresholds)\ns    = linear_score(X_train, w)\n\nfor i in range(num_thresholds):\n    t = T[i]\n    preds    = s &gt;= t\n    FPR[i]   = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR[i]   = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\nax.plot(FPR, TPR, color = \"black\")\nax.plot([0,1], [0,1], linestyle=\"--\", color = \"grey\")\nax.set_aspect('equal')\n\nlabs = ax.set(xlabel = \"False Positive Rate\", ylabel = \"True Positive Rate\", title = \"ROC Curve\")\n\n\n\n\nWe can think of the ROC curve as a description of all the possible tradeoffs between the TPR and FPR that are achievable for a given score as we vary the threshold \\(t\\). For example, the curve tells us that if we are willing to tolerate a false positive rate of 0.40, then the best true positive rate we can achieve is approximately 0.77.\nROC curves are often used as a measure of the ability of a score function to classify data into two groups. Curves that bend farther towards the upper left corner of the plot are generally viewed as more effective classifiers. The area under the curve (AUC) is sometimes used as a single quantitative measure describing the classification quality.\n\n\nCost of Errors and Optimal Thresholding\nHow do we choose the tradeoff that works best for us? To answer this kind of question, we need to reflect back on the purpose for which we are building a classifier. According to Table 4.1, there are two ways to be correct (true positive, true negative) and two ways to make an error (false positive, false negative). In order to choose an appropriate tradeoff, we need to think about the benefit of being right in relation to the cost of being wrong.\nA logical way for a bank to approach this problem would be from the perspective of profit-maximization. In the lending business, a bank can make money when loans are fully repaid with interest, but lose money (usually much more) when an individual defaults on the loan. To keep the problem simple, suppose that the bank gains $1 every time they make a loan which is successfully paid back, and that the bank loses $2 every time they make a loan which ends in default. The first scenario happens when the bank makes a true positive identification, while the second case happens when the bank makes a false negative classification.  For a given threshold, the expected gain for the bank when making a loan is thenRemember that the “positive” outcome in this data set is default.\n\\[\n\\begin{aligned}\n    \\mathbb{E}[\\mathrm{gain}] = 1\\times \\text{TN} - 2\\times \\text{FN}\\;.\n\\end{aligned}\n\\]\nLet’s plot the expected gain as a function of the threshold:\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\ncost_of_FN = -2.0\ngain_of_TN = 1.0\n\ngain =  gain_of_TN*TNR  + cost_of_FN*FNR \n\nplt.plot(T, gain)\nplt.gca().set(ylim = (-0.2, 0.2), xlim = (0, 0.5))\nlabs = plt.gca().set(xlabel = r\"Threshold $t$\", ylabel = \"Expected profit per loan\")\n\n\n\n\nFor these costs, we observe that the bank can make a small expected profit (roughly 17 cents per loan) by using the given score function with threshold of roughly \\(t \\approx 0.21\\). Note that this is very different from the value of the thresold \\(t \\approx 0.4\\) which maximized the unweighted accuracy of the predictor.\nAt this stage, we could go on to estimate the profit gained by using this predictor and threshold on the test data set instead of the training data set. The code below simply consolidates the many steps that we have walked through in these notes, applied to the test data.\n\nt = 0.21\n\n# compute the scores\ns     = linear_score(X_test, w)\npreds = s &gt;= t\n\n# compute error rates\nFPR   = ((preds == 1) & (y_test == 0)).sum() / (y_test == 0).sum()\nTPR   = ((preds == 1) & (y_test == 1)).sum() / (y_test == 1).sum()\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\n# compute the expected gain\ngain = gain_of_TN*TNR  + cost_of_FN*FNR \ngain\n\n0.17641653321131712\n\n\nOur performance on the test data is very slightly worse than our performance on the training data, which is to be expected."
  },
  {
    "objectID": "chapters/04-decision-theory.html#recap",
    "href": "chapters/04-decision-theory.html#recap",
    "title": "4  Decision Theory in Classification",
    "section": "Recap",
    "text": "Recap\nIn these notes, we studied a simple question: given a score \\(s_i = \\langle \\mathbf{x}_i, \\mathbf{w}\\rangle\\), how should we convert that score into a yes/no decision? We found that adjusting the threshold can have major consequences for the accuracy of the resulting classification algorithm, but also that pure accuracy may not be the most relevant metric to measure or optimize. We computed the ROC curve of the score, which is a visual indicator of the overall ability of the score function to balance the false positive rate against the true positive rate. Finally, we explored the possible tradeoffs between different kinds of errors by considering a simplified scenario in which different kinds of errors have different costs associated with them. We found that the threshold that optimizes expected gain under this setting can be very different from the threshold that optimizes unweighted accuracy."
  },
  {
    "objectID": "chapters/04-decision-theory.html#who-sets-the-cost-who-pays-the-cost",
    "href": "chapters/04-decision-theory.html#who-sets-the-cost-who-pays-the-cost",
    "title": "4  Decision Theory in Classification",
    "section": "Who Sets The Cost? Who Pays the Cost?",
    "text": "Who Sets The Cost? Who Pays the Cost?\nIn our analysis above, we assumed a simple optimization objective: the bank is going to maximize its net profit. In formulating this objective, we made assumptions about the costs of different outcomes – to the bank. It’s important to note that the costs of errors to the bank may look very different from the costs of those errors to individuals. For example, if the bank’s prediction system recommends that an individual be denied a loan and the bank acts on this recommendation, then the bank pays no cost. On the other hand, the individual may experience major costs, depending on the purpose for which the loan was requested.\nThis data set includes a coarse description of the purpose of each loan:\n\ndf_all.groupby(\"loan_intent\").size()\n\nloan_intent\nDEBTCONSOLIDATION    5212\nEDUCATION            6453\nHOMEIMPROVEMENT      3605\nMEDICAL              6071\nPERSONAL             5521\nVENTURE              5719\ndtype: int64\n\n\nWhat are the costs of being denied access to borrowed funds to pursue education? What about for medical care?\nIt is of fundamental importance to remember that machine learning systems are embedded in social context; that they are generally developed and implemented by people and organizations that occupy positions of power; and that the costs of these systems are often unequally shared by the people they impact. We will discuss these considerations in much greater detail soon."
  },
  {
    "objectID": "chapters/10-compas.html#data-preparation",
    "href": "chapters/10-compas.html#data-preparation",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Data Preparation",
    "text": "Data Preparation\n Let’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.This data set was obtained by Angwin et al. (2022) through a public records request. The data comprises two years worth of COMPAS scoring in Broward County, Florida.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nsns.set_style(\"whitegrid\")\nnp.set_printoptions(precision = 3)\npd.set_option('display.precision', 3)\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/compas/compas.csv\"\ncompas = pd.read_csv(url)\n\nFor today we are only going to consider a subset of columns.\n\ncols = [\"sex\", \"race\", \"decile_score\", \"two_year_recid\"]\ncompas = compas[cols]\n\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\n\nis_white = compas[\"race\"] == \"Caucasian\"\nis_black = compas[\"race\"] == \"African-American\"\n\ncompas = compas[is_white | is_black]\ncompas = compas.copy()\n\nOur data now looks like this:\n\ncompas.head()\n\n\n\n\n\n\n\n\nsex\nrace\ndecile_score\ntwo_year_recid\n\n\n\n\n1\nMale\nAfrican-American\n3\n1\n\n\n2\nMale\nAfrican-American\n4\n1\n\n\n3\nMale\nAfrican-American\n8\n0\n\n\n6\nMale\nCaucasian\n6\n1\n\n\n8\nFemale\nCaucasian\n1\n0"
  },
  {
    "objectID": "chapters/10-compas.html#preliminary-explorations",
    "href": "chapters/10-compas.html#preliminary-explorations",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Preliminary Explorations",
    "text": "Preliminary Explorations\nLet’s do some quick exploration of our data. How many defendants are present in this data of each sex?\n\ncompas.groupby(\"sex\").size()\n\nsex\nFemale    1219\nMale      4931\ndtype: int64\n\n\nWhat about race?\n\ncompas.groupby(\"race\").size()\n\nrace\nAfrican-American    3696\nCaucasian           2454\ndtype: int64\n\n\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as being produced by computing a score like \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) for each defendant \\(i\\), and then dividing these into the lowest 10% (decile score 1), the next 10% (decile score 2), the next 10% (decile score 3) and so on.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\np = sns.barplot(data = counts, \n                x = \"decile_score\", \n                y = \"n\", \n                hue = \"race\", \n                palette = \"BuPu\", \n                saturation = 0.5)\n\n\n\n\nYou may notice that the number of white defendants who receive a given decile score tends to decrease as the score increases, whereas the number of Black defendants remains relatively constant.\nLet’s also take a look at the recidivism rate in the data:\n\ncompas[\"two_year_recid\"].mean()\n\n0.4661788617886179\n\n\nSo, in these data, approximately 47% of all defendants went on to be charged of another crime within the next two years. This is sometimes called the prevalence of the outcome. Although this is not a “good” outcome, it is labeled 1 in the target data and so we refer to this as the “positive” outcome. Prevalence without further specification usually refers to prevalence of the positive outcome.\nThe base rate of prediction accuracy in this problem is 53%: if we always guessed that the defendant was not arrested within two years, we would be right 53% of the time.\nWe can also compute the prevalence broken down by race of the defendant:\n\ncompas.groupby(\"race\")[\"two_year_recid\"].mean()\n\nrace\nAfrican-American    0.514\nCaucasian           0.394\nName: two_year_recid, dtype: float64\n\n\n\n\nWhen interpreting these different prevalences, it is important to remember that\n\nRace is itself a socially-constructed system of human categorization invented by humans with political and economic motives to describe other humans as property (Bonilla-Silva 2018).\n\nThe relation between arrest and actual criminal offense can display racial bias, with effects varying by geography (Fogliato et al. 2021).\nDecisions about which behaviors are criminal are contingent political decisions which have, historically, fallen hardest on Black Americans (Yusef and Yusef 2017).\n\nThe prevalences between the two groups are substantially different. This difference will have major consequences later on for the possibility of different kinds of fairness in classifiers.\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of Angwin et al. (2022), we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\n\ncompas[\"predicted_high_risk\"] = (compas[\"decile_score\"] &gt; 4)\n\nNow that we’ve done that, we can ask: how likely are Black and white defendants to receive positive predictions in this data?\n\ncompas.groupby(\"race\")[\"two_year_recid\", \"predicted_high_risk\"].mean()\n\n/var/folders/xn/wvbwvw0d6dx46h9_2bkrknnw0000gn/T/ipykernel_52033/3539224628.py:1: FutureWarning:\n\nIndexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n\n\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\nrace\n\n\n\n\n\n\nAfrican-American\n0.514\n0.588\n\n\nCaucasian\n0.394\n0.348\n\n\n\n\n\n\n\nBlack defendants are substantially more likely to receive a positive prediction than white defendants, and the disparity is larger than the observed prevalence of the positive outcome.\n\n\n\n\n\n\nFairness (Part 1)\n\n\n\nIs this fair? What is your gut telling you? Yes, no, possibly? What information would you need in order to make a judgment? What is the principle on which your judgment rests?"
  },
  {
    "objectID": "chapters/10-compas.html#the-propublica-findings",
    "href": "chapters/10-compas.html#the-propublica-findings",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "The ProPublica Findings",
    "text": "The ProPublica Findings\nLet’s now ask a few questions about the the predictive accuracy of this algorithm. First, how accurate it is it overall?\n\ncompas[\"correct_prediction\"] = (compas[\"predicted_high_risk\"] == compas[\"two_year_recid\"])\ncompas[\"correct_prediction\"].mean()\n\n0.6508943089430894\n\n\nRecall that the base rate in this problem is 53%, so our accuracy is somewhat better than random guessing.\nWhat about the accuracy on Black and white defendants separately?\n\ncompas.groupby([\"race\"])[\"correct_prediction\"].mean()\n\nrace\nAfrican-American    0.638\nCaucasian           0.670\nName: correct_prediction, dtype: float64\n\n\nThe overall accuracies for Black and white defendants are comparable, and both are somewhat higher than the base rate of 53%.\nWhat about the error rates? Here is a simple calculation which computes the false positive rate (FPR) in the first row and the true positive rate (TPR) on the bottom row:\n\ncompas.groupby([\"two_year_recid\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\n\n\n0\n0\n0.352\n\n\n1\n1\n0.654\n\n\n\n\n\n\n\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race:\n\ncompas.groupby([\"two_year_recid\", \"race\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\ntwo_year_recid\nrace\npredicted_high_risk\n\n\n\n\n0\n0\nAfrican-American\n0.448\n\n\n1\n0\nCaucasian\n0.235\n\n\n2\n1\nAfrican-American\n0.720\n\n\n3\n1\nCaucasian\n0.523\n\n\n\n\n\n\n\nThe false positive rate for Black defendants is much higher than the false positive rate for white defendants. This was the main finding of Angwin et al. (2022). The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to the idea of representational bias, in which algorithms learn and reproduce toxic stereotypes about certain groups of people.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. This is sometimes called allocative or distributional bias: bias in how resources or opportunities (in this case, freedom) are allocated or distributed between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well.\nWe can think about the argument of Angwin et al. (2022) as a two-step argument:\n\n\nThe COMPAS algorithm has disparate error rates by race.\nTherefore, the COMPAS algorithm is unjustly biased with respect to race.\n\n\nThis argument implicitly equates equality of error rates with lack of bias.\n\n\n\n\n\n\nFairness (Part 2)\n\n\n\n\nSuppose that we developed an alternative algorithm in which the false positive rates were equal, but there were still more positive predictions for Black defendants overall. Would that be enough to ensure fairness?\nSuppose that we developed an alternative prediction algorithm in which the rate of positive prediction was the same across racial groups, but the false positive rates were different. Would that be to ensure fairness?"
  },
  {
    "objectID": "chapters/10-compas.html#the-rebuttal",
    "href": "chapters/10-compas.html#the-rebuttal",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "The Rebuttal",
    "text": "The Rebuttal\nAngwin et al. (2022) kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. In particular, Northpointe, the company that developed COMPAS, issued a report Flores, Bechtel, and Lowenkamp (2016) in which they argued that their algorithm was fair. Their argument is based on an idea of fairness which is sometimes called sufficiency Corbett-Davies et al. (2017).\nHere’s the intuition expressed by sufficiency. Imagine that you and your friend both received an A- in Data Structures. Suppose, however, that the instructor says different things to each of you:\n\nTo you, the instructor says: “You did fine in this class, but I don’t think that you are prepared to take Computer Architecture. I gave you a higher grade than I would normally because you wear cool hats in class.”\nTo your friend, the instructor says: “*You did fine in this class and I think you are prepared to take Computer Architecture. Some students got a bump in their grade because they are cool-hat-wearers, but you didn’t get that benefit.”\n\nFeels unfair, right? The instructor is saying that:\n\nWhat a grade means for you in terms of your future success depends on your identity group.\n\n\n\n\n\n\n\nSuppose that you heard this, but instead of cool hats it was because you are a member of an identity group that “needs some help” in order to achieve equitable representation in the CS major. How would you feel? Would that feel fair to you?\n\n\n\nWe’ll formally define sufficiency in a future lecture. For now, let’s use an informal definition:\n\nSufficiency means that a positive prediction means the same thing for future outcomes for each racial group.\n\nTo operationalize this idea, we are looking for the rate of re-arrest to be the same between (a) Black defendants who received a positive prediction and (b) white defendants who received a positive prediction.\nLet’s check this:\n\ncompas.groupby([\"predicted_high_risk\", \"race\"])[\"two_year_recid\"].mean().reset_index()\n\n\n\n\n\n\n\n\npredicted_high_risk\nrace\ntwo_year_recid\n\n\n\n\n0\nFalse\nAfrican-American\n0.350\n\n\n1\nFalse\nCaucasian\n0.288\n\n\n2\nTrue\nAfrican-American\n0.630\n\n\n3\nTrue\nCaucasian\n0.591\n\n\n\n\n\n\n\nThe rates of rearrest are relatively similar between groups when controlling for the predictions they collectively received. Formal statistical hypothesis tests are typically used to determine whether this difference is sufficiently “real” to warrant correction. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be relatively close to satisfying sufficiency.\nIndeed, in a rejoinder article published by affiliates of the company Northpointe which produced COMPAS, the fact that COMPAS satisfies sufficiency is one of the primary arguments (Flores, Bechtel, and Lowenkamp 2016)."
  },
  {
    "objectID": "chapters/10-compas.html#recap",
    "href": "chapters/10-compas.html#recap",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Recap",
    "text": "Recap\nIn these notes, we replicated the data analysis of Angwin et al. (2022), finding that the COMPAS algorithm has disparate error rates between Black and white defendants. We introduced the idea that fairness actually has several different facets in our moral intuitions, and found that the COMPAS algorithm satisfies one of them (sufficiency: equal scores mean the same thing regardless of your group membership) but not the others (equal prediction rates and equal error rates)."
  },
  {
    "objectID": "chapters/10-compas.html#some-questions-moving-forward",
    "href": "chapters/10-compas.html#some-questions-moving-forward",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "Some Questions Moving Forward",
    "text": "Some Questions Moving Forward\n\nCan we have it all? Could we modify the COMPAS algorithm in such a way that it satisfies all the ideas of fairness that we discussed above? Could we then call it “fair” or “unbiased?”\nAre there other ways to define fairness? Which ones are most compelling to us? Does the right idea of fairness depend on the context in which we apply it?\nHow did this happen? The COMPAS algorithm was never trained on race data about the defendant. How did it happen that this algorithm nevertheless made recommendations at different rates across groups?\nIs automated decision-making legitimate in this setting? Can it be legitimate (just, fair) to use an automated decision-system for making recommendations about parole and sentencing decisions at all? What safeguards and forms of recourse are necessary for the legitimate use of automated decision-making in criminal justice?\nWhat are the systemic impacts? Disparate sentencing decisions can have downstream impacts on communities and institutions. How could application of the COMPAS algorithm exacerbate systemic inequalities?"
  },
  {
    "objectID": "chapters/10-compas.html#references",
    "href": "chapters/10-compas.html#references",
    "title": "5  Introduction to Algorithmic Disparity: COMPAS",
    "section": "References",
    "text": "References\n\n\n\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2022. “Machine Bias.” In Ethics of Data and Analytics, 254–64. Auerbach Publications.\n\n\nBonilla-Silva, Eduardo. 2018. Racism Without Racists: Color-Blind Racism and the Persistence of Racial Inequality in America. Fifth edition. Lanham: Rowman & Littlefield.\n\n\nCorbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. “Algorithmic Decision Making and the Cost of Fairness.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 797–806. Halifax NS Canada: ACM. https://doi.org/10.1145/3097983.3098095.\n\n\nFlores, Anthony W, Kristin Bechtel, and Christopher T Lowenkamp. 2016. “False Positives, False Negatives, and False Analyses: A Rejoinder to Machine Bias: There’s Software Used Across the Country to Predict Future Criminals. And It’s Biased Against Blacks.” Federal Probation 80: 38.\n\n\nFogliato, Riccardo, Alice Xiang, Zachary Lipton, Daniel Nagin, and Alexandra Chouldechova. 2021. “On the Validity of Arrest as a Proxy for Offense: Race and the Likelihood of Arrest for Violent Crimes.” In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 100–111. Virtual Event USA: ACM. https://doi.org/10.1145/3461702.3462538.\n\n\nYusef, Kideste Wilder, and Tseleq Yusef. 2017. “Criminalizing Race, Racializing Crime: Assessing the Discipline of Criminology Through a Historical Lens.” In The Handbook of the History and Philosophy of Criminology, edited by Ruth Ann Triplett, 1st ed., 272–88. Wiley. https://doi.org/10.1002/9781119011385.ch16."
  },
  {
    "objectID": "chapters/12-statistical-fairness.html#three-statistical-definitions-of-fairness",
    "href": "chapters/12-statistical-fairness.html#three-statistical-definitions-of-fairness",
    "title": "6  Statistical Definitions of Fairness in Decision-Making",
    "section": "Three Statistical Definitions of Fairness",
    "text": "Three Statistical Definitions of Fairness\nLast time, we introduced the idea that fairness in decision-making could be defined formally, and models could be audited to determine the extent to which those models conformed to a given definition. In this section, we’ll discuss some of the definitions in Chapter 3 of Barocas, Hardt, and Narayanan (2023) and implement Python functions to measure the extent to which the COMPAS risk score conforms to those definitions.\nTo line ourselves up with the notation of Barocas, Hardt, and Narayanan (2023), let’s define the following random variables: Let \\(A\\) be a random variable that describes the group membership of an individual. Let \\(Y\\) be the outcome we want to predict. Let \\(R\\) be the value of our risk score. Let \\(\\hat{Y}\\) be our model’s prediction about whether \\(Y\\) occurs.\nIn the case of COMPAS:\n\n\\(A\\) is the race of the individual, with possible values \\(A = a\\) and \\(A = b\\).\n\\(Y = 1\\) if the individual was arrested within two years after release, and \\(Y = 0\\) if not.\n\\(R\\) is the decile risk score.\n\\(\\hat{Y} = 1\\) if \\(R \\geq 4\\) and \\(\\hat{Y} = 0\\) otherwise.\n\n\nStatistical Independence\nHere’s our first concept of fairness: independence. For our present purposes, we focus on the definition of independence for binary classifiers as given by Barocas, Hardt, and Narayanan (2023).\n\n\n\n\n\n\n\nDefinition 6.1 (Statistical Independence For Binary Classifiers)  The model predictions \\(\\hat{Y}\\) satisfy statistical independence if \\(\\mathbb{P}(\\hat{Y} = 1 | A = a) = {P}(\\hat{Y} = 1 | A = b)\\).\n\n\n\n\nRecall that \\(\\mathbb{P}(Y = 1|A = a)\\) is the probability that \\(Y = 1\\) given that \\(A=a\\). It can be computed using the formula \\(\\mathbb{P}(Y = 1|A = a) = \\frac{\\mathbb{P}(Y = 1, A = a)}{\\mathbb{P}(A = a)}\\).Colloquially, Definition 6.1 says that the probability of a positive prediction \\(\\hat{Y} = 1\\) does not depend on the group membership \\(A\\). In the COMPAS data, independence would require that the probability of the model predicting that an individual will be arrested within two years be the same for Black and white defendants.\nLet’s write a Python function to empirically check independence that will accept a data frame df and three additional arguments:\nFor independence, we don’t actually need the target column, but this approach will let us keep a consistent API for our more complicated implementations below.\n\ngroup_col, the name of the column describing group memberships.\ntarget, the name of the column holding the binary outcomes.\npred, the name of the column holding the predicted binary outcomes.\n\n\ndef test_independence(df, group_col, target, pred):\n    return df.groupby(group_col)[pred].aggregate([np.mean, len])\n\nLet’s run our function to check for independence:\n\ntest_independence(compas, \"race\", \"two_year_recid\", \"predicted_high_risk\")\n\n\n\n\n\n\n\n\nmean\nlen\n\n\nrace\n\n\n\n\n\n\nAfrican-American\n0.588\n3696\n\n\nCaucasian\n0.348\n2454\n\n\n\n\n\n\n\nThe mean column gives the proportion of the time in which the predictor \\(\\hat{Y}\\) had value equal to 1, for each of the two groups. This is an empirical estimate of the probability \\(\\mathbb{P}(\\hat{Y} = 1 | A = a)\\). We can see that the two proportions are substantially different between groups, strongly suggesting that this model violates the independence criterion. Formally, statistical tests beyond the scope of this course would be needed to reject the hypothesis that the two proportions are different. In this case, you can take my word for it that the relevant test provides strong support for rejecting the null.\nAs discussed in Barocas, Hardt, and Narayanan (2023), independence is a very strong expression of the idea that predictions, and therefore automated decisions, should be the same in aggregate across all groups present in the data. This idea sometimes accompanies another idea, that all groups are equally worthy, meritorious, or deserving of a given decision outcome.\n\n\nError-Rate Balance\n The primary finding of Angwin et al. (2022) was, famously, that the COMPAS algorithm makes very different kinds of errors on Black and white defendants.This definition can be generalized from binary classifiers to score functions via the concept of separation, which is discussed in Barocas, Hardt, and Narayanan (2023).\n\n\n\n\n\n\n\nDefinition 6.2 (Error Rate Balance for Binary Classifiers) The model predictions \\(\\hat{Y}\\) satisfy error-rate balance if the following conditions both hold:\n\\[\n\\begin{aligned}\n    \\mathbb{P}(\\hat{Y} = 1 | Y = 1, A = a) &= \\mathbb{P}(\\hat{Y} =1  | Y = 1, A = b) & \\text{(balanced true positives)} \\\\\n    \\mathbb{P}(\\hat{Y} = 1 | Y = 0, A = a) &= \\mathbb{P}(\\hat{Y} =1  | Y = 0, A = b)\\;. & \\text{(balanced false positives)}\n\\end{aligned}\n\\]\n\n\n\n\nError rate balance requires that the true positive rate and false positive rates be equal on the two groups. Given some data in which we have \\(\\mathrm{TP}\\) instances of true positives, \\(\\mathrm{FP}\\) instances of false positives, \\(\\mathrm{TN}\\) instances of true negatives, and \\(\\mathrm{FN}\\) instances of false negatives, we can estimate the TPR and FPR via the formulas\n\\[\n\\begin{aligned}\n    \\mathrm{TPR} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} \\\\  \n    \\mathrm{FPR} &= \\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}}\\;.\n\\end{aligned}\n\\]\nLet’s write another function with the same API to give a summary of error rates between two groups using these formulas. As we know, it’s pretty convenient to do this with confusion matrices. It’s not much more difficult to do it “by hand” using vectorized Pandas computations:\n\ndef test_error_rate_balance(df, group_col, target, pred):\n    return df.groupby([group_col, target])[pred].mean().reset_index()\n\nWe can use this function to do an empirical test for error rate balance:\n\ntest_error_rate_balance(compas, \"race\", \"two_year_recid\", \"predicted_high_risk\")\n\n\n\n\n\n\n\n\nrace\ntwo_year_recid\npredicted_high_risk\n\n\n\n\n0\nAfrican-American\n0\n0.448\n\n\n1\nAfrican-American\n1\n0.720\n\n\n2\nCaucasian\n0\n0.235\n\n\n3\nCaucasian\n1\n0.523\n\n\n\n\n\n\n\n The false positive rates are in the rows in which two_year_recid == 0, and the true positive rates are in the rows in which two_year_recid == 1.As before, before concluding that the COMPAS algorithm violates error rate balance as in Definition 6.2, it is technically necessary to perform a statistical test to reject the null hypothesis that the true population error rates are the same.\n\n\nSufficiency\nFinally, as we mentioned last time, the analysis of Angwin et al. (2022) received heavy pushback from Flores, Bechtel, and Lowenkamp (2016) and others, who argued that error rate balance wasn’t really the right thing to measure. Instead, we should check sufficiency, which we’ll define here for binary classifiers:\n\n\n\n\n\n\n\nDefinition 6.3 (Sufficiency) Model predictions \\(\\hat{Y}\\) satisfy sufficiency if the following two conditions hold: \\[\n\\begin{aligned}\n    \\mathbb{P}(Y = 1 | \\hat{Y} = 1, A = a) &= \\mathbb{P}(Y = 1 | \\hat{Y} = 1, A = b) \\\\\n    \\mathbb{P}(Y = 0 | \\hat{Y} = 0, A = a) &= \\mathbb{P}(Y = 0 | \\hat{Y} = 0, A = b) \\;.\n\\end{aligned}\n\\]\n\n\n\n\nThe quantity \\(\\mathbb{P}(Y = 1 | \\hat{Y} = 1, A = a)\\) is sometimes called the positive predictive value (PPV) of \\(\\hat{Y}\\) for group \\(a\\). You can think of it as the “value” of a positive prediction: given that the prediction is positive (\\(\\hat{Y} = 1\\)) for a member of group \\(a\\), how likely is it that the prediction is accurate? Similarly, \\(\\mathbb{P}(Y = 0 | \\hat{Y} = 0, A = a)\\) is sometimes called the negative predictive value (NPV) of \\(\\hat{Y}\\) for group \\(a\\). So, the sufficiency criterion demands that the positive and negative predictive values be equal across groups.\nGiven some data in which we have \\(\\mathrm{TP}\\) instances of true positives, \\(\\mathrm{FP}\\) instances of false positives, \\(\\mathrm{TN}\\) instances of true negatives, and \\(\\mathrm{FN}\\) instances of false negatives, we can estimate the PPV and NPV via the formulas\n\\[\n\\begin{aligned}\n    \\mathrm{PPV} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}} \\\\  \n    \\mathrm{NPV} &= \\frac{\\mathrm{TN}}{\\mathrm{TN} + \\mathrm{FN}}\\;.\n\\end{aligned}\n\\]\nLet’s write a function to check for sufficiency in the COMPAS predictions. This function will compute the positive and negative predictive values by group:\n\ndef test_sufficiency(df, group_col, target, pred):\n    df_ = df.copy()\n    df_[\"correct\"] = df_[pred] == df_[target]\n    return df_.groupby([pred, group_col])[\"correct\"].mean().reset_index()\n\n\ntest_sufficiency(compas, \"race\", \"two_year_recid\", \"predicted_high_risk\")\n\n\n\n\n\n\n\n\npredicted_high_risk\nrace\ncorrect\n\n\n\n\n0\n0\nAfrican-American\n0.650\n\n\n1\n0\nCaucasian\n0.712\n\n\n2\n1\nAfrican-American\n0.630\n\n\n3\n1\nCaucasian\n0.591\n\n\n\n\n\n\n\nThe negative predictive values are in the rows in which predicted_high_risk == 0 and the positive predictive values are in the rows in which predicted_high_risk == 1. We observe that the negative predictive value is slightly higher for white defendants, while the positive predictive value is slightly higher for Black defendants. These differences, however, are much lower than the error rate disparity noted above."
  },
  {
    "objectID": "chapters/12-statistical-fairness.html#can-we-have-it-all",
    "href": "chapters/12-statistical-fairness.html#can-we-have-it-all",
    "title": "6  Statistical Definitions of Fairness in Decision-Making",
    "section": "Can We Have It All?",
    "text": "Can We Have It All?\nOk, well COMPAS isn’t an ideal algorithm by any means. But couldn’t we just define some more conceptions of fairness, pick the ones that we wanted to use, and then design an algorithm that satisfied all of them?\nSadly, no: we can’t even have error rate balance and sufficiency simultaneously.\n\n\n\n\n\n\n\nTheorem 6.1 (Incompatibility of Error Rate Balance and Sufficiency (Chouldechova 2017)) If the true rates \\(p_a\\) and \\(p_b\\) of positive outcomes in the groups \\(a\\) and \\(b\\) are not equal (\\(p_a \\neq p_b\\)), then there does not exist a model that produces predictions which satisfy both error rate balance and sufficiency.\n\n\n\n\n\nProof. Our big-picture approach is proof by contrapositive. We’ll show that if there were a model that satisfied error rate balance and sufficiency, then \\(p_a = p_b\\).\nLet’s briefly forget about group labels – we’ll reintroduce them in a moment.\nFirst, the prevalence of positive outcomes is the fraction of positive outcomes. There are \\(\\mathrm{TP} + \\mathrm{FN}\\) total positive outcomes, and \\(\\mathrm{TP} + \\mathrm{FP} + \\mathrm{TN} + \\mathrm{FN}\\) outcomes overal, so we can write the prevalence as\n\\[\n\\begin{aligned}\n    p = \\frac{\\mathrm{TP} + \\mathrm{FN}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{TN} + \\mathrm{FN}};.\n\\end{aligned}\n\\]\nFrom above, the true and false positive rates are:\n\\[\n\\begin{aligned}\n    \\mathrm{TPR} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} \\\\\n    \\mathrm{FPR} &= \\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}}\\;.  \n\\end{aligned}\n\\]\nThe PPV is: \\[\n\\begin{aligned}\n    \\mathrm{PPV} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}\\;.\n\\end{aligned}\n\\]\nOk, now it’s time to do some algebra. Let’s start with the \\(\\mathrm{TPR}\\) and see if we can find an equation that relates it to the \\(\\mathrm{FPR}\\). First, we’ll multiply by \\(\\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}}\\). If we do this and insert the definitions of these quantities, we’ll get\n\\[\n\\begin{aligned}\n    \\mathrm{TPR} \\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} \\frac{\\mathrm{FP}}{\\mathrm{TP} + \\mathrm{FP}} \\frac{\\mathrm{TP} + \\mathrm{FP}}{\\mathrm{TP}} \\\\\n    &= \\frac{\\mathrm{FP}}{\\mathrm{TP} + \\mathrm{FN}}\\;.\n\\end{aligned}\n\\]\nLet’s now also multiply by a factor of \\(\\frac{p}{1-p}\\):\n\\[\n\\begin{aligned}\n    \\mathrm{TPR} \\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}}\\frac{p}{1-p} &= \\frac{\\mathrm{FP}}{\\mathrm{TP} + \\mathrm{FN}}\\frac{p}{1-p} \\\\\n    &= \\frac{\\mathrm{FP}}{\\mathrm{TP} + \\mathrm{FN}} \\frac{\\mathrm{TP} + \\mathrm{FN}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{TN} + \\mathrm{FN}} \\frac{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{TN} + \\mathrm{FN}}{\\mathrm{FP} + \\mathrm{TN}} \\\\\n    &= \\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}} \\\\\n    &= \\mathrm{FPR}\\;.\n\\end{aligned}\n\\]\nSo, with some algebra, we have proven an equation for any classifier:\n\\[\n\\begin{aligned}\n    \\mathrm{TPR} \\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}}\\frac{p}{1-p} = \\mathrm{FPR}\\;.\n\\end{aligned}\n\\]\nIt’s convenient to rearrange this equation slightly:\n\\[\n\\begin{aligned}\n    \\frac{\\mathrm{TPR}}{\\mathrm{FPR}} \\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}} = \\frac{1-p}{p}\\;.\n\\end{aligned}\n\\]\nor\n\\[\n\\begin{aligned}\n    p = \\left(1 + \\frac{\\mathrm{TPR}}{\\mathrm{FPR}} \\frac{1 - \\mathrm{PPV}}{\\mathrm{PPV}}\\right)^{-1}\\;.\n\\end{aligned}\n\\tag{6.1}\\]\nNow suppose that I want to enforce error rate balance and sufficiency for two groups \\(a\\) and \\(b\\), where \\(p_a \\neq p_b\\). So, from error rate balance I am going to require that \\(\\mathrm{TPR}_a = \\mathrm{TPR}_b\\), \\(\\mathrm{FPR}_a = \\mathrm{FPR}_b\\), and from sufficiency I am going to enforce that \\(\\mathrm{PPV}_a = \\mathrm{PPV}_b\\). Now, however, we have a problem: by Equation 6.1, it must also be the case that \\(p_a = p_b\\). This contradicts our assumption from the theorem. We cannot mathematically satisfy both error rate balance and sufficiency. This completes the proof.\n\n\n\n\n\n\n\nDiscussion\n\n\n\nDo you feel that it is more important for a recidivism prediction algorithm like COMPAS to satisfy error rate balance or sufficiency? Why?"
  },
  {
    "objectID": "chapters/12-statistical-fairness.html#fairness-context-and-legitimacy",
    "href": "chapters/12-statistical-fairness.html#fairness-context-and-legitimacy",
    "title": "6  Statistical Definitions of Fairness in Decision-Making",
    "section": "Fairness, Context, and Legitimacy",
    "text": "Fairness, Context, and Legitimacy\nThe proof above shows that, when the prevalences of positive outcomes differ between groups, we have no hope of being able to have both balanced error rates and sufficiency. In Chapter 3, Barocas, Hardt, and Narayanan (2023) give a few other examples of fairness definitions, as well as proofs that some of these definitions are incompatible with each other. We can’t just have it all – we have to choose.\nThe quantitative story of fairness in automated decision-making is not cut-and-dried – we need to make choices, which may be subject to politics. Let’s close this discussion with three increasingly difficult questions:\n\nWhat is the right definition of fairness by which to judge the operation of a decision-making algorithm?\nIs “fairness” even the right rubric for assessing the impact of a given algorithm?\nIs it legitimate to use automated decision-making at all for a given application context?\n\nWe’ll consider each of these questions soon."
  },
  {
    "objectID": "chapters/12-statistical-fairness.html#references",
    "href": "chapters/12-statistical-fairness.html#references",
    "title": "6  Statistical Definitions of Fairness in Decision-Making",
    "section": "References",
    "text": "References\n\n\n\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2022. “Machine Bias.” In Ethics of Data and Analytics, 254–64. Auerbach Publications.\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press. https://fairmlbook.org/pdf/fairmlbook.pdf.\n\n\nChouldechova, Alexandra. 2017. “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.” Big Data 5 (2): 153–63. https://doi.org/10.1089/big.2016.0047.\n\n\nFlores, Anthony W, Kristin Bechtel, and Christopher T Lowenkamp. 2016. “False Positives, False Negatives, and False Analyses: A Rejoinder to Machine Bias: There’s Software Used Across the Country to Predict Future Criminals. And It’s Biased Against Blacks.” Federal Probation 80: 38."
  }
]