{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Image Classification\n",
    "\n",
    "In this set of lecture notes, we'll work through an applied case study of deep learning for image classification. Like [our last adventure with an image classification task](51-vectorization.ipynb), we'll focus on sign-language classification using convolutional kernels. This time, however, we won't take the kernels as given. Instead, we'll attempt to optimize the kernels as part of the learning process. \n",
    "\n",
    "Along the way, we'll also study some of the practicalities of working with larger models in torch, including model inspection, GPU acceleration, and data set management. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from torch.nn import Conv2d, MaxPool2d, Parameter\n",
    "from torch.nn.functional import relu\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "train_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/sign-language-mnist/sign_mnist_train.csv\"\n",
    "test_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/sign-language-mnist/sign_mnist_test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_url)\n",
    "df_val   = pd.read_csv(test_url)\n",
    "\n",
    "def prep_data(df): \n",
    "    n, p = df.shape[0], df.shape[1] - 1\n",
    "    y = torch.tensor(df[\"label\"].values)\n",
    "    X = df.drop([\"label\"], axis = 1)\n",
    "    X = torch.tensor(X.values)\n",
    "    X = torch.reshape(X, (n, 1, 28, 28))\n",
    "    X = X / 255\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = prep_data(df_train)\n",
    "X_val, y_val     = prep_data(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "def show_images(X, y, rows, cols, channel = 0):\n",
    "\n",
    "    fig, axarr = plt.subplots(rows, cols, figsize = (2*cols, 2*rows))\n",
    "    for i, ax in enumerate(axarr.ravel()):\n",
    "        ax.imshow(X[i, channel].detach(), cmap = \"Greys_r\")\n",
    "        ax.set(title = f\"{ALPHABET[y[i]]}\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "show_images(X_train, y_train, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(X_train, y_train), \n",
    "    batch_size = 32, \n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(X_val, y_val), \n",
    "    batch_size = 32, \n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A Note On Multiclass Classification\n",
    "\n",
    "We're actually now making our first formal study of a multiclass classification problem, in which we are trying to distinguish data observations into more than two possible categories. Whereas before we didn't really comment on the specific structure of this problem, here we need to build up a model from scratch and therefore need to understand how it works!\n",
    "\n",
    "Typically, classification models return a score *for each class*. Then, the class with the highest score is usually considered to be the model's prediction. This means that the score function should actually return a *vector* of scores for each data observation.\n",
    "\n",
    "In order to make this happen for a single-layer model, we move from a matrix-vector multiplication $\\mathbf{X}\\mathbf{w}$ to a matrix-matrix multiplication $\\mathbf{X}\\mathbf{W}$, where $\\mathbf{W} \\in \\mathbb{R}^{p \\times r}$ has number of rows equal to the number of features and number of columns equal to the number of classes. \n",
    "\n",
    "More generally, we can define our model in any way we like, as long as it returns a vector of scores for each data observation. \n",
    "\n",
    "It is also necessary to modify the loss function for classification models. Instead of the binary cross entropy, we need to define a multiclass generalization. The most common choice of per-observation loss function between a vector of class scores $\\mathbf{s} \\in \\mathbb{R}^r$ and the true label $y_i$ is \n",
    "\n",
    "$$\n",
    "\\ell(\\mathbf{s}_i, y_i) = \\sum_{j = 1}^r \\mathbb{1}[y_i = j]\\log\\left(\\frac{e^{s_{ij}}}{\\sum_{k = 1}^r e^{s_{ik}}}\\right) \n",
    "$$\n",
    "\n",
    "The function \n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(\\mathbf{s}) = \\left(\\begin{matrix}\n",
    "    \\frac{e^{s_1}}{\\sum_{j = 1}^r e^{s_j}} \\\\ \n",
    "    \\frac{e^{s_2}}{\\sum_{j = 1}^r e^{s_j}} \\\\ \n",
    "    \\vdots \n",
    "    \\frac{e^{s_r}}{\\sum_{j = 1}^r e^{s_j}}\n",
    "    \\end{matrix}\\right)\n",
    "$$\n",
    "\n",
    "is a generalization of the logistic sigmoid function to the multiclass setting. It is called the softmax function because it has a tendency to accentuate the largest value in the vector $\\mathbf{s}$. With this notation, we can write the cross-entropy loss as \n",
    "\n",
    "$$\n",
    "\\ell(\\mathbf{s}_i, y_i) = \\sum_{j = 1}^r \\mathbb{1}[y_i = j]\\log \\mathrm{softmax}(\\mathbf{s}_i)_j\\;.\n",
    "$$\n",
    "\n",
    "Summing the per-observation loss over all data points gives the empirical risk to be minimized. \n",
    "\n",
    "Let's implement a linear model with the multiclass cross entropy. This first model is equivalent to multiclass logistic regression. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The `forward` method computes a matrix of scores. Each row of this matrix gives the scores for a single observation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The `predict` method just computes these scores and returns the column index of the largest score in each column. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It's very useful to get in the habit of *inspecting* your models in order to understand how they are organized and how many parameters need to be trained. One convenient way to do this is with the `summary` function provided by the `torchsummary` package. This function requires that we input the dimensions of a single observation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Even this simple multiclass logistic model has over 20,000 parameters to train! \n",
    "\n",
    "Let's implement a simple training loop: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, k_epochs = 1, print_every = 2000):\n",
    "    # loss function is cross-entropy (multiclass logistic)\n",
    "    loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "    # optimizer is Adam, which does fancier stuff with the gradients\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(k_epochs): \n",
    "        for i, data in enumerate(data_loader_train):\n",
    "            X, y = data\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % print_every == 0:\n",
    "                print(f\"Epoch {epoch}, batch {i:>3}, loss on batch: {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, k_epochs = 2, print_every = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Once we have trained the model, we can extract predictions and compare them to true targets. Here's our accuracy after training on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    total = 0 \n",
    "    total_correct = 0\n",
    "    for X, y in data_loader_val:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        scores = model.forward(X)\n",
    "        y_pred =  torch.argmax(scores, dim = 1)\n",
    "        total += X.size(0)\n",
    "        total_correct += (y_pred == y).sum().item()\n",
    "        \n",
    "    print(f\"Validation accuracy = {total_correct / total:.3f}\")\n",
    "        \n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutional Models\n",
    "\n",
    "Our favorite logistic regression is a great algorithm, but there is lots of room to improve! Last time we studied this data set, we used convolutional kernels to extract more helpful features from the data before finally plugging those features into a logistic regression model. We sandwiched those convolutional layers between pooling and ReLU activation layers. This time, instead of treating these kernels as given, we are going to learn them as part of the optimization routine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # first convolutional layer\n",
    "        # 1 color channel, 100 different kernels, kernel size = 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 100, 5)\n",
    "\n",
    "        # shrinks the image by taking the largest pixel in each 2x2 square\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # 100 channels (from previous convolutional layer), 50 new kernels, kernel size = 3x3\n",
    "        self.conv2 = nn.Conv2d(100, 50, 3)\n",
    "\n",
    "        # EVEN MOAR CONVOLUTION: 50 channels, 50 new kernels, kernel size 3x3\n",
    "        self.conv3 = nn.Conv2d(50, 50, 3)\n",
    "\n",
    "        # a linear layer on top of the kernels\n",
    "        self.fc1 = nn.Linear(50, len(ALPHABET))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # these layers use the spatial structure of the image\n",
    "        # so we don't flatten yet\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        # now we're ready to flatten all dimensions except batch\n",
    "        x = torch.flatten(x, 1) \n",
    "\n",
    "        # pass results through a fully-connected linear layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What does this model look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This model has more parameters than the logistic regression model. However, the convolutional layers also give it the potential to usefully leverage the spatial structure of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, k_epochs = 10, print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
