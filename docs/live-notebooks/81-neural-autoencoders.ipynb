{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Autoencoders and Dimensionality Reduction\n",
    "\n",
    "Last time, we studied unsupervised learning through the framework of *autoencoders*. When using autoencoders, the learning problem is to find an *encoder* $\\phi_e$ and a *decoder* $\\psi_d$ such that a reconstructed data point $\\hat{\\mathbf{x}} = \\psi_d(\\phi_e(\\mathbf{x}))$ is \"close\" to the original data point $\\mathbf{x}$, where \"close\" needs to be measured in terms of a loss function. We looked at two examples of autoencoders defined in terms of the square-error loss function: k-means and principal component analysis (PCA). \n",
    "\n",
    "PCA in particular is an example of an encoding algorithm for *dimensionality reduction*. The aim of dimensionality reduction is to learn a lower-dimensional representation of the data set in a new feature space that captures its fundamental structure. PCA is the most common form of *linear* dimensionality reduction, in which we aim to learn linear structure that summarizes the data. In these notes we'll focus on neural network methods for *nonlinear* dimensionality reduction, which is sometimes also called *manifold learning*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learning a Circle\n",
    "\n",
    "Let's start by attempting to learn a circular structure in noisy circular data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "n_points = 100\n",
    "\n",
    "r = 1\n",
    "noise = 0.05\n",
    "\n",
    "theta     = torch.linspace(0, 2*torch.pi, n_points)\n",
    "r_        = r + torch.randn(n_points)*noise\n",
    "x1_       = r_*torch.cos(theta) \n",
    "x2_       = r_*torch.sin(theta)\n",
    "\n",
    "x1 = r*torch.cos(theta)\n",
    "x2 = r*torch.sin(theta)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
    "X = torch.stack([x1_, x2_], dim=1)\n",
    "ax.scatter(X[:,0], X[:,1], facecolors = \"none\", edgecolors = \"steelblue\")\n",
    "ax.plot(x1, x2, color = \"black\",  label = \"Low-dimensional structure\")\n",
    "ax.plot([-1.1, -0.9], [0, 0], color = \"firebrick\", label = \"Noise\")\n",
    "ax.annotate(\"Noise\", [-0.8, 0], color = \"firebrick\")\n",
    "ax.annotate(\"Low-dimensional structure\\n(to be learned)\", [-0.6, 0.4], color = \"black\")\n",
    "ax.annotate(\"Noise\", [-0.8, 0], color = \"firebrick\")\n",
    "ax.annotate(\"Data\", [-0.6, -0.5], color = \"steelblue\")\n",
    "ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This data is approximately circular. A given data point can be *mostly* described by its angle from the center of the circle. \n",
    "\n",
    "The following neural network attempts to learn a low-dimensional representation of this data using gradient descent. The encoder is a nonlinear map that sends each data point to a 1D representation. The decoder is a nonlinear map that sends a 1d scalar number to a point in 2D space. Minimizing the reconstruction error corresponds to trying to attempting to learn this low-dimensional structure from the data. \n",
    "\n",
    "Note that this model doesn't include any explicit representation of the idea of \"circularity\": it just tries to learn a low-dimensional representation that minimizes the reconstruction error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's try training the model on the example data. We'll use a standard training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, n_epochs=1000, **kwargs): \n",
    "    optimizer = torch.optim.Adam(model.parameters(), **kwargs)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for epoch in range(n_epochs): \n",
    "        optimizer.zero_grad()\n",
    "        X_hat = model(X)\n",
    "        loss = loss_fn(X_hat, X)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1000 == 0: \n",
    "            print(f\"Epoch {epoch}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = ManifoldLearner(2, 1, 10)\n",
    "train(ml, X, 10000, lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now let's take a look at the learned data representation. First, we'll generate some evenly-spaced \"codes.\" Then, we'll decode these codes in order to see how they map into the original data space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(5*X.min(), 5*X.max(), 10001)[:, None]\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We see that the decoder has mapped 1D codes into 2D data points. Such a mapping, if continuous, describes a curve in 2D space. Let's take a look! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
    "ax.scatter(X[:,0], X[:,1], facecolors = \"none\", edgecolors = \"steelblue\", label = \"Data\")\n",
    "ax.plot(approx.detach()[:,0], approx.detach()[:,1], color = \"black\", label = \"Encoded 1D structure\")\n",
    "ax.legend()\n",
    "labs = ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can see that the encoder approximated the circular structure of the data relatively well for most of the data domain, with only a gap in a small part of the learned curve. As we might expect, the learned curve is not a perfect circle due to the noisiness of the data and the limited flexibility of our model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 2: Image Embedding\n",
    "\n",
    "[*Major components of the code in this section are adapted from [\"Variational Autoencoder: PyTorch Tutorial\"](https://medium.com/@rekalantar/variational-auto-encoder-vae-pytorch-tutorial-dce2d2fe0f5f) by Reza Kalantar.*]{.aside} \n",
    "\n",
    "Let's consider the problem of embedding images in a low-dimensional space. Our aim is to learn a low-dimensional representation of a complex image data set that represents similarities between the images in some way. \n",
    "\n",
    "Image embedding via autoencoders was a state-of-the-art technique for image generation before the advent of diffusion methods. \n",
    "\n",
    "To illustrate the task of image embedding, we will finally use a very famous data set. MNIST is a data set containing 28x28 grayscale images of handwritten digits. The data set was originally collected by the National Institute of Standards and Technology (NIST) to motivate research into the automated recognition of handwritten digits; the initial motivation for this task was to automate the process of reading zip codes on snail mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "s = torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a copy of the MNIST training data set\n",
    "path = '~/datasets'\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = MNIST(path, transform=transform, download=True)\n",
    "\n",
    "# create a data loader\n",
    "batch_size = 100\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here's an excerpt of the data from the training data loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 25 sample training images for visualization\n",
    "image = next(iter(train_loader))[0]\n",
    "\n",
    "fig, ax = plt.subplots(5, 5, figsize=(5, 5))\n",
    "\n",
    "for ax, im in zip(ax.ravel(), image):\n",
    "    ax.imshow(im[0], cmap='gray_r')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The data also comes with true labels for the digits, which we will ignore for our purposes today. \n",
    "\n",
    "This data excerpt places the images somewhat randomly. Can we find a low-dimensional representation of the data that would, for example, place the 0s together, the 9s together, etc? Maybe this representation would even place 3s near 8s and 9s near 4s, since these digits are orthographically somewhat similar. \n",
    "\n",
    "Our aim is to learn an underlying low-dimensional structure to this data using a neural autoencoder. Here is the neural network we'll use. This autoencoder is somewhat more complex than the other encoders we've studied, but it operates on the same principle: use an encoder to learn a low-dimensional code, alongside a decoder which reconstructs approximate original data points from the low-dimensional code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For data that has a natural minimum and maximum value, it is often more appropriate to use a cross-entropy loss function than a square-error loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, x_hat):\n",
    "    return nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we can define a standard training loop: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        overall_loss = 0\n",
    "        for batch_idx, (x, _) in enumerate(train_loader):\n",
    "            # print(x.size())\n",
    "            x = x.to(device)\n",
    "            x = x.flatten(start_dim=1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_hat = model(x)\n",
    "            loss = loss_function(x, x_hat)\n",
    "            \n",
    "            overall_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next we need to instantiate a model and optimizer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = AutoEncoder().to(device)\n",
    "optimizer = torch.optim.Adam(AE.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "And now it's training time! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(AE, optimizer, epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Once we've trained the autoencoder, we can generate images simply by decoding a vector in latent space. Let's first generate images corresponding to two vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "v1 = torch.tensor([-3, 5.0])\n",
    "v2 = torch.tensor([ 2.5,  5.0])\n",
    "\n",
    "for i, v in enumerate([v1, v2]):\n",
    "    img = AE.decode(v.to(device).float()).detach().reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap = 'gray_r')\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What would you say the \"average\" of these two images is? One way to address this question would to simply average the pixels together, which results in the image on the left below. This image just looks like a 6 and a 4 superimposed on top of each other and doesn't appear to mean much of anything. \n",
    "\n",
    "Alternatively, we could average the *codes* together and then decode the result. The code-based method actually gives a reasonable image as the answer: according to the latent-space representation of the data, the \"average\" of a 6 and a 4 is an 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "img = 1/2*(AE.decode(v1.to(device)) + AE.decode(v2.to(device)))\n",
    "ax[0].imshow(img.detach().reshape(28, 28), cmap='gray_r')\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].set(title = \"Average of Pixels\")\n",
    "\n",
    "img = AE.decode(1/2*(v1.to(device) + v2.to(device)))\n",
    "ax[1].imshow(img.detach().reshape(28, 28), cmap='gray_r')\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].set(title = \"Decoded average of codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It can be fun to visualize the entire latent space learned by the model. Here's a function that does this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space(model, scale=1.0, n=25, digit_size=28, figsize=15):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "\n",
    "    # construct a grid \n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = torch.tensor([[xi, yi]], dtype=torch.float).to(device)\n",
    "            x_decoded = model.decode(z_sample)\n",
    "            digit = x_decoded[0].detach().cpu().reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size : (i + 1) * digit_size, j * digit_size : (j + 1) * digit_size,] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    plt.title('Latent Space')\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"mean, z [0]\")\n",
    "    plt.ylabel(\"var, z [1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys\")\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space(AE,  scale = 5, n = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=200, device=device):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "\n",
    "        # encoder: just a simple stack of linear lears\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        # latent mean and variance \n",
    "        self.mean_layer   = nn.Linear(latent_dim, 2)\n",
    "        self.logvar_layer = nn.Linear(latent_dim, 2)\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "     \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, logvar = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterization(self, mean, logvar):\n",
    "        noise = torch.randn_like(logvar).to(device)      \n",
    "        z = mean + logvar.exp().sqrt()*noise\n",
    "        return z\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterization(mean, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The loss function and training loops are slightly more complicated due to the need to incorporate the distributional representations of the data into the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "    KLD = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return reproduction_loss + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        overall_loss = 0\n",
    "        for batch_idx, (x, _) in enumerate(train_loader):\n",
    "            # print(x.size())\n",
    "            x = x.to(device)\n",
    "            x = x.flatten(start_dim=1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            loss = loss_function(x, x_hat, mean, log_var)\n",
    "            \n",
    "            overall_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE = VariationalAutoEncoder().to(device)\n",
    "optimizer = torch.optim.Adam(VAE.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(VAE, optimizer, epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "How does the latent space look under the variational autoencoder? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space(VAE, scale = 1.5, n = 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This latent space learned by the variational autoencoder contains arguably fewer strange artifacts and nonsensical transitions than the \"standard\" autoencoder. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
