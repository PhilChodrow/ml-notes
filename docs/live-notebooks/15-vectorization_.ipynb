{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "mermaid: \n",
    "    theme: default\n",
    "---\n",
    "\n",
    "# Vectorization\n",
    "\n",
    "So far in this course, we've considered the general *supervised learning* scenario, in which we are given a feature matrix $\\mathbf{X} \\in \\mathbb{R}^{n\\times p}$ and a target vector $\\mathbf{y} \\in \\mathbb{R}^n$. We then solve the empirical risk minimization problem in order to choose model parameters that minimize a loss function on the training data. The exact structure of this loss function depends on things like whether we are doing classification or regression, what our computational resources are, and other considerations. \n",
    "\n",
    "But feature matrices $\\mathbf{X}$ and target vectors $\\mathbf{y}$ don't just exist in the world: they are *collected* and *measured*. We can think of data collection and measurement as posing three fundamental questions: \n",
    "\n",
    "- **Data collection**: Which **rows** (observations) exist in $\\mathbf{X}$ and $\\mathbf{y}$? \n",
    "- **Measurement**: which **columns** (features) exist in $\\mathbf{X}$? \n",
    "- **Measurement**: what is the **target** $\\mathbf{y}$ and how is it measured? \n",
    "\n",
    "Broadly, we can think of the complete machine learning workflow as having phases corresponding to problem definition, data collection + measurement, modeling, and evaluation. Here's roughly how this looks: \n",
    "\n",
    "```{mermaid}\n",
    "flowchart TB\n",
    "\n",
    "    subgraph problem[problem definition]\n",
    "        need[identify need]-->design_collection[design data collection]\n",
    "    end\n",
    "    subgraph measurement[data collection + measurement]\n",
    "        training[training data] \n",
    "        testing[testing data]\n",
    "    end\n",
    "    subgraph modeling\n",
    "        explore[explore data] --> engineer[engineer features]\n",
    "        engineer --> design[design model]\n",
    "    end\n",
    "    subgraph assessment\n",
    "        test --> audit\n",
    "        audit --> deploy\n",
    "        deploy-->evaluate\n",
    "    end\n",
    "    design_collection-->measurement\n",
    "    training --vectorization--> modeling\n",
    "    design --> assessment\n",
    "    testing --vectorization--> assessment\n",
    "    need-->assessment\n",
    "\n",
    "```\n",
    "\n",
    "So far, we've spent most of our time in the \"modeling\" module, especially the last two steps. We've also studied some of the ways to test and audit algorithms, especially with regards to questions of fairness and equity. Today we're going to discuss **vectorization**. We can think of vectorization as what happens *between* the collection of raw data and the use of that data as input for machine learning models.  \n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "::: {#def-vectorization}\n",
    "\n",
    "## Vectorization\n",
    "\n",
    "**Vectorization** is the act of assigning to each data observation a vector $\\mathbf{x}$, thus forming a feature matrix $\\mathbf{X}$. Formally, a **vectorization map** is a function $v:\\mathcal{D}\\rightarrow \\mathbb{R}^p$ such that, if $d \\in \\mathcal{D}$ is a data observation, then $\\mathbf{x} = v(d)$ is a set of features corresponding to $d$. \n",
    "\n",
    ":::\n",
    ":::\n",
    "\n",
    "The reason that vectorization is necessary is that, fundamentally, **machine learning models only understand matrices of numbers**. So, if our data *isn't* numbers, we need to convert it into numbers in order to use it for modeling. \n",
    "\n",
    "## What Data Needs Vectorization?\n",
    "\n",
    "Most of it! \n",
    "\n",
    "- If your data comes to you as a table or matrix containing only numbers, in which each row corresponds to exactly one observation, then you may not need to vectorize. \n",
    "- If your data comes to you in *any other form*, then you need to vectorize. \n",
    "\n",
    "Some data that usually require vectorization:\n",
    "\n",
    "- Images\n",
    "- Text\n",
    "- Audio files\n",
    "- Genomic data\n",
    "- Etc. etc. \n",
    "\n",
    "There are tons of ways of vectorizing different kinds of data, and we're not going to cover all of them. Instead, we're going to go a little more in depth on **text vectorization**. We'll discuss image vectorization much more when we get to convolutional neural networks. [For your projects, depending on the data you want to work with, you may need to research vectorization schemes appropriate to your data.]{.aside}\n",
    "\n",
    "## Vectorization vs. Feature Engineering\n",
    "\n",
    "Vectorizing data is very closely related to the problem of engineering useful features in a data set. Formally, we talk about vectorizing when we simply mean the problem of getting non-vector data into a vector format. Feature engineering then comes after: we use the vector to construct new useful features. In practice, these operations are often intermingled. For example, we might aim to go from non-vector data to a vector of *useful* features directly. In fact, we'll do this below. \n",
    "\n",
    "## Case Study: Sign Language Recognition\n",
    "\n",
    "There is no single approach for vectorizing data. The most effective ways to vectorize a given data set depend strongly on where the data comes from and what aspects of it we expect to be useful for a prediction task. Effective vectorization methods for images can be very different from vectorization methods from text, which can in turn be very different from vectorization methods for audio files. Rather than make a superficial touch on each one, we'll instead focus on a specific running example: vectorization of images. We'll continue with the same example when we get to deep learning later in these notes.\n",
    "\n",
    "Our data for today is the [Sign Language MNIST](https://www.kaggle.com/datasets/datamunge/sign-language-mnist) data set, which I retrieved from Kaggle. This data set poses a challenge: can we train a model to recognize a letter of American Sign Language from a hand gesture? \n",
    "\n",
    "First, let's retrieve the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from torch.nn import Conv2d, MaxPool2d, Parameter\n",
    "from torch.nn.functional import relu\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/sign-language-mnist/sign_mnist_train.csv\"\n",
    "\n",
    "test_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/sign-language-mnist/sign_mnist_test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_url)\n",
    "df_test  = pd.read_csv(test_url)\n",
    "\n",
    "ALPHABET = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "n, p = df_train.shape[0], df_train.shape[1] - 1\n",
    "n_test = df_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Natively, this data set comes to us as a data frame in which each column represents a pixel. This is actually *already* a kind of vectorization, and we'll soon want to make some different choices. With this in mind, it's better to work with the data in its native image form. The following code block reshapes the data and gets us there, on both the training and test sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df): \n",
    "    n, p = df.shape[0], df.shape[1] - 1\n",
    "    y = torch.tensor(df[\"label\"].values)\n",
    "    X = df.drop([\"label\"], axis = 1)\n",
    "    X = torch.tensor(X.values)\n",
    "    X = torch.reshape(X, (n, 1, 28, 28))\n",
    "    X = X / 255\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = prep_data(df_train)\n",
    "X_test, y_test = prep_data(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Our data is now shaped like a \"stack\" of images. Let's take a look at a single slice: [The reason that we have to do `X[0,0]` rather than simply `X[0]` is that `X` is 4-dimensional. The second dimension is for the *channel* of the image. An RGB image needs to represent color values for 3 distinct colors and therefore has 3 channels. Since our image is greyscale, we only have 1 channel, which is labeled `0`.]{.aside}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's visualize this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.gca().set(title = f\"{ALPHABET[y_train[10]]}\")\n",
    "no_ax = plt.gca().axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's look at a handful of images and their labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(X, y, rows, cols, channel = 0):\n",
    "\n",
    "    fig, axarr = plt.subplots(rows, cols, figsize = (2*cols, 2*rows))\n",
    "    for i, ax in enumerate(axarr.ravel()):\n",
    "        ax.imshow(X[i, channel].detach(), cmap = \"Greys_r\")\n",
    "        ax.set(title = f\"{ALPHABET[y[i]]}\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "show_images(X_train, y_train, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "How frequently does each letter appear in the data set? \n",
    "\n",
    "Let's look at the frequencies of each letter in this data set: [There are no \"J\"s or \"Z\"s in this data because these characters require motion rather than a static gesture.]{.aside}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (6, 2))\n",
    "letters, counts = torch.unique(y_train, return_counts = True)\n",
    "proportions = counts / counts.sum()\n",
    "proportions\n",
    "\n",
    "ax.scatter(letters, proportions*100, facecolor = \"none\", edgecolor = \"steelblue\")\n",
    "ax.set_xticks(range(26))\n",
    "ax.set_xticklabels(list(ALPHABET))\n",
    "ax.set(xlabel = \"Letter\", ylabel = \"Frequency\")\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(decimals = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The most frequent letter (\"R\") in this data comprises no more than 5% of the entire data set. So, as a minimal aim, we would like a model that gets the right character at least 5% of the time. \n",
    "\n",
    "## First Approach: Pixel-By-Pixel Prediction\n",
    "\n",
    "One simple way to vectorize our data would be to simply treat the greyscale value of each pixel as a feature. [This is actually the form our data came in: we are imagining that we were instead supplied with the images in rectangular form.]{.aside} This approach gives us a feature matrix in which each column corresponds to an individual pixel, of which are there are 784 in each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reshaping our square $28 \\times 28$ images into long vectors of length $784 = 28^2$ is the only feature engineering step we need in this case. [Running this code on your laptop may lead to `ConvergenceWarnings` in logistic regression. For the purposes of following along with the notes it's ok to ignore these, although it's also possible to adjust the optimization algorithm or maximum number of iterations in `LogisticRegression` in order to make them go away.]{.aside}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "How did we do on the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wow, that's a high accuracy for just going pixel-by-pixel! It almost seems too good to be true..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Indeed, we did much worse on the test data, with dramatic overfitting. That said, our model has still learned considerable pattern in the data: random guessing would lead to test accuracy in the range of 4-5%. \n",
    "\n",
    "The following function wraps up this experiment for convenient reuse down the line. The only addition is the incorporation of a `pipeline` function, which is going to reflect our later vectorization workflows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization_experiment(pipeline = lambda x: x, return_confusion_matrix = False):\n",
    "    X_train_transformed = pipeline(X_train)\n",
    "    X_train_flat = X_train_transformed.flatten(start_dim = 1)\n",
    "    print(f\"Number of features = {X_train_flat.size(1)}\")\n",
    "\n",
    "    LR = LogisticRegression() \n",
    "    LR.fit(X_train_flat, y_train)\n",
    "    print(f\"Training accuracy = {LR.score(X_train_flat, y_train):.2f}\")\n",
    "\n",
    "    X_test_transformed = pipeline(X_test) \n",
    "    X_test_flat = X_test_transformed.flatten(start_dim = 1)\n",
    "    print(f\"Testing accuracy  = {LR.score(X_test_flat, y_test):.2f}\")\n",
    "\n",
    "    if return_confusion_matrix: \n",
    "        y_test_pred = LR.predict(X_test_flat)\n",
    "        return confusion_matrix(y_test, y_test_pred, normalize = \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization_experiment() # same experiment as above"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutional Kernels\n",
    "\n",
    "A common approach to vectorization of images is to apply a *convolutional kernel*. [Convolutional kernels are not related in any way to positive-definite kernels.]{.aside} A convolutional kernel is a component of a vectorization pipeline which is specifically suited to the structure of images. In particular, *images are fundamentally spatial*. We might want to construct data features which reflect not just the value of an individual pixel, but also the values of pixels nearby that one. \n",
    "\n",
    "One of the most common types of layers is a *convolutional* layer. The idea of an image convolution is pretty simple. We define a square *kernel matrix* containing some numbers, and we \"slide it over\" the input data. At each location, we multiply the data values by the kernel matrix values, and add them together. Here's an illustrative diagram:\n",
    "\n",
    "![](https://d2l.ai/_images/correlation.svg)\n",
    "\n",
    "*Image from [Dive Into Deep Learning](https://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html)*\n",
    "\n",
    "In this example, the value of 19 is computed as $0\\times 0 + 1\\times 1 + 3\\times 2 + 4\\times 3 = 19$. \n",
    "\n",
    "Let's create some simple $5\\times 5$ kernels that we'll apply to this image data. Our first one is designed to detect vertical lines in images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's make some more kernels for horizontal and vertical lines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag1    = torch.eye(5)*5 - 1\n",
    "horizontal = torch.transpose(vertical, 1, 0)\n",
    "diag2    = diag1.flip(1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 4)\n",
    "for i, kernel in enumerate([vertical, horizontal, diag1, diag2]):\n",
    "    ax[i].imshow(kernel, vmin = -1.5, vmax = 2)\n",
    "    ax[i].axis(\"off\")\n",
    "    ax[i].set(title = f'{[\"Vertical\", \"Horizontal\", \"Diagonal Down\", \"Diagonal Up\"][i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now let's make a function which will *apply* these convolutional kernels to our images. The output of each kernel will be stored as a separate *channel* of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_convolutions(X): \n",
    "\n",
    "    # this is actually a neural network layer -- we'll learn how to use these\n",
    "    # in that context soon \n",
    "    conv1 = Conv2d(1, 4, 5)\n",
    "\n",
    "    # need to disable gradients for this layer\n",
    "    for p in conv1.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # replace kernels in layer with our custom ones\n",
    "    conv1.weight[0, 0] = Parameter(vertical)\n",
    "    conv1.weight[1, 0] = Parameter(horizontal)\n",
    "    conv1.weight[2, 0] = Parameter(diag1)\n",
    "    conv1.weight[3, 0] = Parameter(diag2)\n",
    "\n",
    "    # apply to input data and disable gradients\n",
    "    return conv1(X).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we're ready to compute the result of applying our kernels to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We now have a training data set of diffferent size. There's one channel (2nd index) for each of the four kernels we've applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's take a look at how each of the four kernels act on some images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_viz(pipeline):\n",
    "\n",
    "    fig, ax = plt.subplots(5, 5, figsize = (8, 8))\n",
    "\n",
    "    X_convd = pipeline(X_train)\n",
    "\n",
    "    for i in range(5): \n",
    "        for j in range(5):\n",
    "            if i == 0: \n",
    "                ax[i,j].imshow(X_train[j, 0])\n",
    "            \n",
    "            else: \n",
    "                ax[i, j].imshow(X_convd[j,i-1])\n",
    "            \n",
    "            ax[i,j].tick_params(\n",
    "                        axis='both',      \n",
    "                        which='both',     \n",
    "                        bottom=False,     \n",
    "                        left=False,\n",
    "                        right=False,         \n",
    "                        labelbottom=False, \n",
    "                        labelleft=False)\n",
    "            ax[i,j].grid(False)\n",
    "            ax[i, 0].set(ylabel = [\"Original\", \"Vertical\", \"Horizontal\", \"Diag Down\", \"Diag Up\"][i])\n",
    "\n",
    "kernel_viz(apply_convolutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We see that the Vertical kernel, for example, tends to accentuate vertical boundaries between light and dark in the images. The horizontal kernel accentuates horizontal boundaries, and so on. \n",
    "\n",
    "Ok, so we have Done Something to our images. Does this actually lead to better classification accuracy when we use logistic regression? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uhhhhh, well that actually seems to have made it worse. Somehow, we've actually overfit even more! One reason for this is that we have actually *increased* the number of features, without adding more data points. But shouldn't we have gotten a bump from having these purportedly useful features in the mix? \n",
    "\n",
    "Surprisingly, no. The reason is that kernel convolution is a fundamentally linear operation -- it's just a matter of multiplying the data by constants and adding the results together. Logistic regression is a linear model, and the way it calculates the score is *also* a linear operation. As you may remember, the result of doing two linear operations back-to-back is simply a different linear operation. But since everything involved is linear, our vectorization approach hasn't really added anything to the expressive power of logistic regression -- only add more feature columns to enable even greater overfitting. \n",
    "\n",
    "## Nonlinearity\n",
    "\n",
    "In order to add to the expressive power of logistic regression, we need to do something *nonlinear* to the result of applying our convolutions. A common choice is the rectified linear unit, or ReLU. This very simple nonlinear function looks like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.linspace(-1, 1, 101)\n",
    "f = relu(x)\n",
    "plt.plot(x, y, color = \"slategrey\")\n",
    "labs = plt.gca().set(xlabel = r\"$z$\",ylabel = r\"$\\mathrm{ReLU}(z)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, it's just two linear functions pasted together, but that makes it technically nonlinear! The effect of applying the relu transformation on our convolved images is that pixels with negative values in each channel are set to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Does setting a bunch of the pixels to 0 really help that much? Surprisingly, yes! Just applying this nonlinear transformation to our convolved images already significantly improves the classification power of our model on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pooling\n",
    "\n",
    "However, we do still have that very large number of features, which slows down model fitting and raises overfitting issues. To address this, let's *reduce* the data in a nonlinear way. We'll do this with max pooling. You can think of it as a kind of \"summarization\" step in which we intentionally make the current output somewhat \"blockier.\" Technically, it involves sliding a window over the current batch of data and picking only the largest element within that window. Here's an example of how this looks:\n",
    "\n",
    "![](https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png)\n",
    "\n",
    "*Image credit: Computer Science Wiki*\n",
    "\n",
    "A useful effect of pooling is that it reduces the number of features in our data. In the image above, we reduce the number of features by a factor of $2\\times 2 = 4$. We can be even more aggressive than this: we'll reduce our data resolution by a factor of 16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Our data now looks a lot chunkier, and doesn't really resemble recognizable hands at all. Surprisingly, however, this is another *big* improvement on our modeling power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Relative to the previous experiments, we have about 6% as many features to feed into our logistic regression models, which leads to a big speed-up. Despite this, the testing accuracy has gone considerably! We have managed to extract *useful*, vectorized features from our image-structured data set. [This level of accuracy is not enough to be practically useful. We'll later see how to do better on this data set.]{.aside}\n",
    "\n",
    "## Other Kinds of Vectorization\n",
    "\n",
    "Our pipeline above: convolutional kernels, pooling, and flattening, is an extremely common vectorization/feature engineering pipeline for image data. Indeed, it is at the foundation of *convolutional neural networks*, which are one of the standard ways for performing image classification. We'll work with convolutional neural networks later in these notes. How do we vector other data?\n",
    "\n",
    "**Text** is often vectorized in one of two paradigms. \n",
    "\n",
    "1. In classical text analysis, it is common to form a *document-term matrix* in which the rows correspond to a given body of text (say, a single chapter of a book) and the columns correspond to words. The document-term matrix collects information on how frequently a given word appears in a given body of text. Various normalizations are possible. This kind of vectorization is common in sentiment analysis and topic modeling applications. \n",
    "2. In modern generative applications, we often represent bodies of text as sequences of words. Each word in the sequence is assigned a *vector* which is intended to represent the semantic content of the word in some way. A sequence of words is then a sequence of vectors in this vector space. The problem of finding good vectors to represent words is known as *word embedding* and is fundamental to a number of text-based tasks in modern deep learning. \n",
    "\n",
    "**Audio** is often vectorized using classical tools from signal processing, including (discrete) Fourier transforms and wavelet transforms. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/philchodrow/My Drive (pchodrow@middlebury.edu)/teaching/ml-notes/env/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
