{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Last time](22-convex-erm.ipynb), we studied the *empirical risk minimization* (ERM). ERM casts the machine learning problem as an optimization problem: we want to find a vector of weights $\\mathbf{w}$ such that \n",
    "\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmin}{argmin}\n",
    "\\begin{aligned}\n",
    "\\hat{\\mathbf{w}} &= \\argmin_{\\mathbf{w}} L(\\mathbf{w}) \\\\ \n",
    "&= \\argmin_{\\mathbf{w}} \\frac{1}{n}\\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle, y_i)\\;.\n",
    "\\end{aligned}\n",
    "$${#eq-erm}\n",
    "\n",
    "As usual, $\\mathbf{X} \\in \\mathbb{R}^{n\\times p}$ is the feature matrix\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\left[\\begin{matrix} & - & \\mathbf{x}_1 & - \\\\ \n",
    "& - & \\mathbf{x}_2 & - \\\\ \n",
    "& \\vdots & \\vdots & \\vdots \\\\ \n",
    "& - & \\mathbf{x}_{n} & - \\end{matrix}\\right] \n",
    "$$\n",
    "\n",
    "and $\\mathbf{y}$ is the vector of targets, which we usually assume to be binary in the context of classification. The per-observation *loss function* $\\ell: \\mathbb{R}\\times \\mathbb{R}\\rightarrow \\mathbb{R}$ measures the quality of the score $s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle$ assigned to data point $i$ by comparing it to $y_i$ and outputing a real number $\\ell(s_i, y_i)$. \n",
    "\n",
    "So, our mathematical task is to find a vector of weights $\\mathbf{w}$ that solves @eq-erm. How do we do it? The modern answer is *gradient descent*, and this set of lecture notes is about what that means and why it works. \n",
    "\n",
    "## Linear Approximations of Single-Variable Functions\n",
    "\n",
    "Recall the limit definition of a derivative of a single-variable function. Let $g:\\mathbb{R} \\rightarrow \\mathbb{R}$. The derivative of $g$ at point $w_0$, if it exists, is \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{dg(w_0)}{dw} = \\lim_{\\delta w\\rightarrow 0}\\frac{g(w_0 + \\delta w) - g(w_0)}{\\delta w}\\;.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If we imagine that $\\delta w$ is very small but nonzero, we can interpret this equation a bit loosely as the statement that \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{dg(w_0)}{dw} \\approx \\frac{g(w_0 + \\delta w) - g(w_0)}{\\delta w}\\;,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which upon some algebraic rearrangement says that \n",
    "\n",
    "$$\n",
    "g(w_0 + \\delta w) \\approx g(w_0) + \\frac{dg(w_0)}{dw} \\delta w\\;.\n",
    "$$\n",
    "\n",
    "Taylor's theorem makes this statement precise: \n",
    "\n",
    "::: {.callout-note}\n",
    "::: {#thm-taylor}\n",
    "\n",
    "## Taylor's Theorem: Univariate Functions\n",
    "\n",
    "Let $g:\\mathbb{R}\\rightarrow \\mathbb{R}$ be differentiable at point $w_0$. Then, there exists $a > 0$ such that, if $\\delta w < a$, then \n",
    "\n",
    "$$\n",
    "g(w_0 + \\delta w) = g(w_0) + \\frac{dg(w_0)}{dw} \\delta w + o(\\delta w)\\;.\n",
    "$$\n",
    "\n",
    ":::\n",
    ":::\n",
    "\n",
    "Here, $o(\\delta w)$ means \"terms that grow small in comparison to $\\delta w$ when $\\delta w$ itself grows small.\"\n",
    "\n",
    "Another common way to write Taylor's theorem is \n",
    "\n",
    "$$\n",
    "g(w) = g(w_0) + \\frac{dg(w_0)}{dw} (w - w_0) + o(|w - w_0|)\\;,\n",
    "$$\n",
    "\n",
    "which comes from substituting $\\delta w = t - w_0$. \n",
    "\n",
    "Taylor's theorem says that, in a neighborhood of $w_0$, we can approximate $g(w)$ with a linear function. Here's an example of how that looks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "w = torch.linspace(-1, 1, 101)\n",
    "\n",
    "g = lambda w: w**2\n",
    "\n",
    "plt.plot(w, g(w), label = r\"$g(w)$\", color = \"black\")\n",
    "plt.gca().set(xlabel = r\"$w$\", ylim = (-0.2, 0.5))\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient Descent in 1 dimension\n",
    "\n",
    "Suppose that we have a function $g$ and we would like to solve the the optimization problem \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{w} = \\argmin _w g(w) \\;.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "How do we go about doing this? You might remember from calculus that one way starts with solving the equation \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{dg(\\hat{w})}{dw} = 0\\;,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "but it is not always feasible to solve this equation exactly in practice. \n",
    "\n",
    "In *iterative* approaches, we instead imagine that we have a current guess $\\hat{w}$ which we would like to improve. To this end, consider the casual Taylor approximation [In the rest of these notes, we will assume that term $o(\\delta w)$ is small enough to be negligible. ]{.alert}\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g(\\hat{w} + \\delta \\hat{w}) \\approx g(\\hat{w}) + \\frac{dg(\\hat{w})}{dw} \\delta \\hat{w}\\;.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We'd like to update our estimate of $\\hat{w}$. Suppose we make a strategic choice: $\\delta hat{w} = -\\alpha \\frac{dg(\\hat{w})}{dw}$ for some small $\\alpha > 0$. We therefore decide that we will do the update \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\hat{w} \\gets \\hat{w} - \\alpha \\frac{dg(\\hat{w})}{dw}\\;.\n",
    "\\end{aligned}\n",
    "$${#eq-w-update}\n",
    "\n",
    "What does this update do to the value of $g$? Let's check: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    g(\\hat{w} + \\delta \\hat{w}) &\\approx g(\\hat{w}) + \\frac{dg(\\hat{w})}{dw} \\delta \\hat{w} \\\\ \n",
    "    &= g(\\hat{w}) - \\frac{dg(\\hat{w})}{dw} \\alpha \\frac{dg(\\hat{w})}{dw}\\\\ \n",
    "    &= g(\\hat{w}) - \\alpha\\left(\\frac{dg(\\hat{w})}{dw}\\right)^2\\;.   \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is the big punchline. Let's look at the second term. If $(\\frac{dg(\\hat{w})}{dw}\\right)^2 = 0$ then that must mean that $\\frac{dg(\\hat{w})}{dw}$ and that we are at a critical point, which we could check for being a local minimum. On the other hand, if $\\frac{dg(\\hat{w})}{dw} \\neq 0$, then $\\left(\\frac{dg(\\hat{w})}{dw}\\right)^2 > 0$. This means that \n",
    "\n",
    "$$\n",
    "g(\\hat{w} + \\delta \\hat{w}) &\\approx &= g(\\hat{w}) - \\alpha\\left(\\frac{dg(\\hat{w})}{dw}\\right)^2 \\\\ \n",
    "                            &<  g(\\hat{w})\\;,\n",
    "$$\n",
    "\n",
    "provided that $\\alpha$ is small enough for the error terms in Taylor's Theorem to be small. We have informally derived the following fact: \n",
    "\n",
    "::: {.callout-tip}\n",
    "\n",
    "## Single-Variable Gradient-Descent Works\n",
    "\n",
    "Let $g:\\mathbb{R}\\rightarrow \\mathbb{R}$ be differentiable and assume that $\\frac{dg(\\hat{w})}{dw} \\neq 0$. Then, if $\\alpha$ is sufficiently small, @eq-w-update is guaranteed to reduce the value of $g$. \n",
    "\n",
    ":::\n",
    "\n",
    "Let's see an example of single-variable gradient descent in action: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_     = -0.7\n",
    "grad  = lambda w: 2*w\n",
    "alpha = 0.1\n",
    "w_vec = [w_]\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    w_vec.append(w_)\n",
    "\n",
    "w_vec = torch.tensor(w_vec)\n",
    "\n",
    "plt.plot(w, g(w), label = r\"$g(w)$\")\n",
    "plt.scatter(w_vec, g(w_vec), color = \"black\", label = r\"Gradient descent updates\", s = 10, zorder = 10)\n",
    "plt.gca().set(xlabel = r\"$w$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can see the updates from gradient descent eventually converging to the point $w = 0$, which is the global minimum of this function. \n",
    "\n",
    "\n",
    "## Gradient Descent in Multiple Dimensions\n",
    "\n",
    "Our empirical risk function $L$ is not a single-variable function; indeed, $L: \\mathbb{R}^p \\rightarrow \\mathbb{R}$. So, we can't directly apply the results above. Fortunately, these results extend in a smooth way to this setting. The main thing we need is the definition of the *gradient* of a multivariate function. \n",
    "\n",
    "## Gradients\n",
    "\n",
    "[We're not going to talk much about what it means for a function to be multivariate differentiable. You can assume that all the functions we will deal with in this class are unless I highlight otherwise. For a more rigorous definition, you should check out a multivariable calculus class.]{.aside}\n",
    "\n",
    "::: {.callout-note}\n",
    "::: {#def-gradient}\n",
    "\n",
    "## Gradient of a Multivariate Function\n",
    "\n",
    "Let $g:\\mathbb{R}^p \\rightarrow \\mathbb{R}$ be a *multivariate differentiable* function. The *gradient* of $g$ evaluated at point $\\mathbf{w}\\in \\mathbb{R}^p$ is written $\\nabla g(\\mathbf{w})$, and has value \n",
    "\n",
    "$$\n",
    "\\nabla g(\\mathbf{w}) \\triangleq \n",
    "\\left(\\begin{matrix}\n",
    "    \\frac{\\partial g(\\mathbf{w})}{\\partial w_1} \\\\ \n",
    "    \\frac{\\partial g(\\mathbf{w})}{\\partial w_2} \\\\ \n",
    "    \\cdots \\\\ \n",
    "    \\frac{\\partial g(\\mathbf{w})}{\\partial w_p} \\\\ \n",
    "\\end{matrix}\\right) \\in \\mathbb{R}^p\\;.\n",
    "$$\n",
    "\n",
    "Here, $\\frac{\\partial g(\\mathbf{w})}{\\partial w_1}$ is the *partial derivative of $f$ with respect to $z_1$, evaluated at $\\mathbf{w}$*. To compute it: \n",
    "\n",
    "> Take the derivative of $f$ *with respect to variable $z_1$, holding all other variables constant, and then evaluate the result at $\\mathbf{w}$. \n",
    "\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {.callout-tip}\n",
    "\n",
    "## Example\n",
    "\n",
    "Let $p = 3$. Let $g(\\mathbf{w}) = w_2\\sin w_1  + w_1e^{2w_3}$. The partial derivatives we need are \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial g(\\mathbf{w})}{\\partial w_1} &= w_2 \\cos w_1 + e^{2w_3}\\\\ \n",
    "\\frac{\\partial g(\\mathbf{w})}{\\partial w_2} &= \\sin w_1\\\\ \n",
    "\\frac{\\partial g(\\mathbf{w})}{\\partial w_3} &= 2w_1 e^{2w_3}\\;. \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So, the gradient of $g$ evaluated at a point $\\mathbf{w}$ is \n",
    "\n",
    "$$\n",
    "\\nabla g(\\mathbf{w}) =\n",
    "\\left(\\begin{matrix}\n",
    "    \\frac{\\partial g(\\mathbf{w})}{\\partial w_1} \\\\ \n",
    "    \\frac{\\partial g(\\mathbf{w})}{\\partial w_2} \\\\ \n",
    "    \\frac{\\partial g(\\mathbf{w})}{\\partial w_3} \\\\ \n",
    "\\end{matrix}\\right) = \n",
    "\\left(\\begin{matrix}\n",
    "    w_2 \\cos w_1 + e^{2w_3}\\\\\n",
    "    \\sin w_1\\\\ \n",
    "    2w_1 e^{2w_3}\n",
    "\\end{matrix}\\right) \n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "Taylor's Theorem extends smoothly to this setting. \n",
    "\n",
    "\n",
    "::: {.callout-note}\n",
    "::: {#thm-taylor}\n",
    "\n",
    "## Taylor's Theorem: Multivariate Functions\n",
    "\n",
    "Let $g:\\mathbb{R}^p\\rightarrow \\mathbb{R}$ be differentiable at point $\\mathbf{w}_0 \\in \\mathbb{R}^p$. Then, there exists $a > 0$ such that, if $\\lVert \\delta \\mathbf{w} \\rVert < a$, then [$\\lVert \\mathbf{\\delta} \\mathbf{w}\\rVert \\triangleq \\sqrt{\\sum_{i = 1}^p (\\delta w_i)^2}$]{.aside}\n",
    "\n",
    "$$\n",
    "g(\\mathbf{w}_0 + \\delta \\mathbf{w}) = g(\\mathbf{w}_0) + \\langle \\nabla g(\\mathbf{w}_0), \\delta \\mathbf{w} \\rangle + o(\\lVert \\delta \\mathbf{w}\\rVert)\\;.\n",
    "$$\n",
    "\n",
    ":::\n",
    ":::\n",
    "\n",
    "The vector $\\nabla g(\\mathbf{w}_0)$ plays the role of the single-variable derivative $\\frac{d g(w_0)}{dw}$. \n",
    "\n",
    "### Multivariate Gradient Descent \n",
    "\n",
    "In multiple dimensions, the gradient descent update is: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\hat{\\mathbf{w}} \\gets \\hat{\\mathbf{w}} - \\alpha \\nabla g(\\hat{\\mathbf{w}})\\;.\n",
    "\\end{aligned}\n",
    "$${eq-w-update-multi}\n",
    "\n",
    "Let's check that a single update of gradient descent will reduce the value of $g$ provided that $\\alpha$ is small enough. Here, $\\delta \\hat{\\mathbf{w}} = -\\alpha \\nabla g(\\hat{\\mathbf{w}})$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    g(\\hat{\\mathbf{w}} - \\delta \\hat{\\mathbf{w}}) &\\approx g(\\hat{\\mathbf{w}}) + \\langle \\nabla g(\\mathbf{w}_0), \\delta \\mathbf{w} \\rangle \\\\ \n",
    "    &= g(\\hat{\\mathbf{w}}) + \\langle \\nabla g(\\hat{\\mathbf{w}}), -\\alpha \\nabla g(\\hat{\\mathbf{w}}) \\rangle \\\\ \n",
    "    &= g(\\hat{\\mathbf{w}}) - \\alpha \\langle \\nabla g(\\hat{\\mathbf{w}}),  \\nabla g(\\hat{\\mathbf{w}}) \\rangle \\\\ \n",
    "    &= g(\\hat{\\mathbf{w}}) - \\alpha \\lVert\\nabla g(\\hat{\\mathbf{w}}) \\rVert^2\\;. \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $\\lVert\\nabla g(\\hat{\\mathbf{w}}) \\rVert^2 > 0$ whenever $\\nabla g(\\hat{\\mathbf{w}}) \\neq\\mathbf{0}$, we conclude that, unless $\\hat{w}$ is a critical point (where the gradient is zero), then \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    g(\\hat{\\mathbf{w}} - \\alpha \\nabla g(\\hat{\\mathbf{w}})) < g(\\hat{\\mathbf{w}})\\;. \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In other words, provided that $\\alpha$ is small enough for the Taylor approximation to be a good one, multivariate gradient descent also always reduces the value of the objective function. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Gradient of the Empirical Risk\n",
    "\n",
    "Remember that our big objective here was to solve @eq-erm using gradient descent. To do this, we need to be able to calculate $\\nabla L(\\mathbf{w})$, where the gradient is with respect to the entries of $\\mathf{w}$. Fortunately, the specific *linear* structure of the score function $s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle$ makes this relatively simple: indeed, we actually only need to worry about the *single* variable derivatives of the per-observation loss $\\ell$. To see this, we can compute\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla L(\\mathbf{w}) &= \\nabla \\left(\\frac{1}{n} \\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle , y_i)\\right) \\\\ \n",
    "              &= \\frac{1}{n} \\sum_{i = 1}^n \\nabla \\ell(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle , y_i) \\\\ \n",
    "              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(s_i, y_i)}{ds} \\nabla \\langle \\mathbf{w}, \\mathbf{x}_i\\rangle  \\tag{multivariate chain rule} \\\\ \n",
    "              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(s_i, y_i)}{ds}  \\mathbf{x}_i \\tag{gradient of a linear function} \\\\ \n",
    "              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(s_i, y_i)}{ds} \\mathbf{x}_i \\tag{$s_i = \\bracket{\\mathbf{w}, \\mathbf{x}_i}$} \\\\ \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The good news here is that for linear models, we don't actually need to be able to compute more gradients: we just need to be able to compute derivatives of the form $\\frac{d\\ell(s_i, y_i)}{ds}$ and then plug in $s_i = \\bracket{\\mathbf{w}, \\mathbf{x}_i}$. \n",
    "\n",
    "Let's do an example with the logistic loss: \n",
    "\n",
    "$$\\ell(s, y) = -y \\log \\sigma(s) - (1-y)\\log (1-\\sigma(s))\\;.$$\n",
    "\n",
    "A useful fact to know about the logistic sigmoid function $\\sigma$ is that $\\frac{d\\sigma(s) }{ds} = \\sigma(s) (1 - \\sigma(s))$. So, using that and the chain rule, the derivative we need is \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d\\ell(s, y)}{ds} &= -y \\frac{1}{\\sigma(s)}\\frac{d\\sigma(s) }{ds} - (1-y)\\frac{1}{1-\\sigma(s)}\\left(- \\frac{d\\sigma(s) }{ds}\\right) \\\\ \n",
    "&= -y \\frac{1}{\\sigma(s)}\\sigma(s) (1 - \\sigma(s)) - (1-y)\\frac{1}{1-\\sigma(s)}\\left(- \\sigma(s) (1 - \\sigma(s))\\right) \\\\ \n",
    "&= -y (1 - \\sigma(s)) + (1-y)\\sigma(s) \\\\ \n",
    "&= \\sigma(s) - y\\;.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally, we need to plug this back in to our empirical risk, obtaining the gradient of the empirical risk for logistic regression: \n",
    "\n",
    "[An important note about this formula that can easily trip one up: this looks a bit like a matrix multiplication or dot product, but it isn't! ]{.aside}\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla L(\\mathbf{w}) &= \\frac{1}{n} \\sum_{i = 1}^n (\\sigma(s_i) - y_i)\\mathbf{x}_i \\\\ \n",
    "              &=\\frac{1}{n} \\sum_{i = 1}^n (\\sigma(\\bracket{\\mathbf{w}, \\mathbf{x}_i}) - y_i)\\mathbf{x}_i\\;.\n",
    "\\end{align}\n",
    "$${#eq-logistic-gradient}\n",
    "\n",
    "This gives us all the math that we need in order to learn logistic regression by choosing a learning rate and iterating the update $\\mathbf{w}^{(t+1)} \\gets \\mathbf{w}^{(t)} - \\alpha \\nabla L(\\mathbf{w}^{(t)})$ until convergence. \n",
    "\n",
    "## Example: Logistic Regression\n",
    "\n",
    "\n",
    "This model *also* inherits from the `LinearModel` class that you previously started implementing. The training loop is also *very* similar to our training loop for the perceptron. The main difference is that the loss is calculated using the `binary_cross_entropy` function above, and the `step` function of the `GradientDescentOptimizer` works differently in a way that we will discuss in the following section. \n",
    "\n",
    "*Starting with the code block below, you won't be able to follow along in coding these notes unless you have sneakily implemented logistic regression in a `hidden` module.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n",
    "    \n",
    "    y = torch.arange(n_points) >= int(n_points/2)\n",
    "    y = 1.0*y\n",
    "    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n",
    "    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = classification_data(noise = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The logistic regression training loop relies on a new implementation of `opt.step`. For gradient descent, here's the complete implementation: just a quick Python version of the gradient descent update @eq-gradient-descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, X, y, lr = 0.01):\n",
    "        self.model.w -= lr*self.model.grad(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The method `model.grad()` is the challenging part of the implementation: this is where we actually need to turn @eq-logistic-gradient into code. \n",
    "\n",
    "Here's the complete training loop. This loop is *very* similar to our perceptron training loop -- we're just using a different loss and a different implementation of `grad`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: 'Evolution of the binary cross entropy loss function in the logistic regression training loop.'\n",
    "#| fig-cap-location: margin\n",
    "#| label: fig-LR-loss-iterations\n",
    "\n",
    "from hidden.logistic import LogisticRegression, GradientDescentOptimizer\n",
    "\n",
    "# instantiate a model and an optimizer\n",
    "LR = LogisticRegression() \n",
    "opt = GradientDescentOptimizer(LR)\n",
    "\n",
    "# for keeping track of loss values\n",
    "loss_vec = []\n",
    "\n",
    "for _ in range(100):\n",
    "\n",
    "    # not part of the update: just for tracking our progress    \n",
    "    loss = LR.loss(X, y) \n",
    "    loss_vec.append(loss)\n",
    "\n",
    "    # only this line actually changes the parameter value\n",
    "    # The whole definition is: \n",
    "    # self.model.w -= lr*self.model.grad(X, y)\n",
    "\n",
    "    opt.step(X, y, lr = 0.02)\n",
    "\n",
    "plt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\n",
    "plt.semilogx()\n",
    "labs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The loss quickly levels out to a constant value (which is the same as we learned with `scipy.optimize.minimize`). Because our theory tells us that the loss function is convex, we know that the value of $\\mathbf{w}$ we have found is the best possible, in the sense of minimizing the loss. \n",
    "\n",
    "Although our data is not linearly separable, the separating line we have learned appears to do a reasonable job of separating the points from each other. Let's check our accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1.0*(LR.predict(X) == y)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Not too bad! \n",
    "\n",
    "## Recap \n",
    "\n",
    "In these lecture notes, we introduced *gradient descent* as a method for minimizing functions, and showed an application of . Gradient descent is especially "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/philchodrow/My Drive (pchodrow@middlebury.edu)/teaching/ml-notes/env/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
